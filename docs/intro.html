<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Inferencia</title>
  <meta name="description" content="Notas para un curso de inferencia estadística" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Inferencia" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Notas para un curso de inferencia estadística" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Inferencia" />
  
  <meta name="twitter:description" content="Notas para un curso de inferencia estadística" />
  

<meta name="author" content="Rodrigo Zepeda-Tello y Luis Carlos Bernal" />


<meta name="date" content="2021-01-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  

<link rel="next" href="R.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Notas a un curso de inferencia estadística</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introducción</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#estadística-y-muestras"><i class="fa fa-check"></i><b>1.1</b> Estadística y muestras</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#modelos"><i class="fa fa-check"></i><b>1.2</b> Modelos</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#inferencia-estadística-ejemplo."><i class="fa fa-check"></i><b>1.3</b> Inferencia estadística: ejemplo.</a><ul>
<li class="chapter" data-level="1.3.1" data-path="intro.html"><a href="intro.html#estimador-de-momentos"><i class="fa fa-check"></i><b>1.3.1</b> Estimador de momentos</a></li>
<li class="chapter" data-level="1.3.2" data-path="intro.html"><a href="intro.html#estimador-de-máxima-verosimilitud-maximización-discreta"><i class="fa fa-check"></i><b>1.3.2</b> Estimador de máxima verosimilitud (maximización discreta)</a></li>
<li class="chapter" data-level="1.3.3" data-path="intro.html"><a href="intro.html#ejemplo-5-muestreo-de-urna-con-dos-clases-con-orden-con-reemplazo"><i class="fa fa-check"></i><b>1.3.3</b> Ejemplo 5: Muestreo de urna con dos clases, con orden, con reemplazo</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#resumen-de-la-sección"><i class="fa fa-check"></i><b>1.4</b> Resumen de la sección</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="R.html"><a href="R.html"><i class="fa fa-check"></i><b>2</b> R</a><ul>
<li class="chapter" data-level="2.1" data-path="R.html"><a href="R.html#programación-en-r"><i class="fa fa-check"></i><b>2.1</b> Programación en <code>R</code></a><ul>
<li class="chapter" data-level="2.1.1" data-path="R.html"><a href="R.html#puntos-a-favor-de-r"><i class="fa fa-check"></i><b>2.1.1</b> Puntos a favor de <code>R</code></a></li>
<li class="chapter" data-level="2.1.2" data-path="R.html"><a href="R.html#bienvenidx-a-r-bunny-wunnies-freak-out-sí-así-se-llama-esta-versión"><i class="fa fa-check"></i><b>2.1.2</b> Bienvenidx a <code>R</code>, Bunny-Wunnies Freak Out (sí, así se llama esta versión)</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="R.html"><a href="R.html#instalando-cosas"><i class="fa fa-check"></i><b>2.2</b> Instalando cosas</a><ul>
<li class="chapter" data-level="2.2.1" data-path="R.html"><a href="R.html#instalación-de-r"><i class="fa fa-check"></i><b>2.2.1</b> Instalación de <code>R</code></a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="R.html"><a href="R.html#instalación-de-rstudio"><i class="fa fa-check"></i><b>2.3</b> Instalación de <code>RStudio</code></a></li>
<li class="chapter" data-level="2.4" data-path="R.html"><a href="R.html#primeros-pasos-en-r-usando-rstudio"><i class="fa fa-check"></i><b>2.4</b> Primeros pasos en <code>R</code> usando <code>RStudio</code></a></li>
<li class="chapter" data-level="2.5" data-path="R.html"><a href="R.html#cálculos-numéricos"><i class="fa fa-check"></i><b>2.5</b> Cálculos numéricos</a><ul>
<li class="chapter" data-level="2.5.1" data-path="R.html"><a href="R.html#ejercicio"><i class="fa fa-check"></i><b>2.5.1</b> Ejercicio</a></li>
<li class="chapter" data-level="2.5.2" data-path="R.html"><a href="R.html#ejercicio-1"><i class="fa fa-check"></i><b>2.5.2</b> Ejercicio</a></li>
<li class="chapter" data-level="2.5.3" data-path="R.html"><a href="R.html#respuestas"><i class="fa fa-check"></i><b>2.5.3</b> Respuestas</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="R.html"><a href="R.html#variables"><i class="fa fa-check"></i><b>2.6</b> Variables</a><ul>
<li class="chapter" data-level="2.6.1" data-path="R.html"><a href="R.html#ejercicios"><i class="fa fa-check"></i><b>2.6.1</b> Ejercicios</a></li>
<li class="chapter" data-level="2.6.2" data-path="R.html"><a href="R.html#nivel-3"><i class="fa fa-check"></i><b>2.6.2</b> NIVEL 3</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="R.html"><a href="R.html#observaciones-sobre-la-aritmética-de-punto-flotante"><i class="fa fa-check"></i><b>2.7</b> Observaciones sobre la aritmética de punto flotante</a><ul>
<li class="chapter" data-level="2.7.1" data-path="R.html"><a href="R.html#cómo-checar-un-if"><i class="fa fa-check"></i><b>2.7.1</b> ¿Cómo checar un if?</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="R.html"><a href="R.html#leer-y-almacenar-variables-en-r"><i class="fa fa-check"></i><b>2.8</b> Leer y almacenar variables en <code>R</code></a><ul>
<li class="chapter" data-level="2.8.1" data-path="R.html"><a href="R.html#ejercicio-2"><i class="fa fa-check"></i><b>2.8.1</b> Ejercicio</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="R.html"><a href="R.html#instalación-de-paquetes"><i class="fa fa-check"></i><b>2.9</b> Instalación de paquetes</a><ul>
<li class="chapter" data-level="2.9.1" data-path="R.html"><a href="R.html#ejercicios-1"><i class="fa fa-check"></i><b>2.9.1</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="R.html"><a href="R.html#comentarios-adicionales-sobre-el-formato"><i class="fa fa-check"></i><b>2.10</b> Comentarios adicionales sobre el formato</a></li>
<li class="chapter" data-level="2.11" data-path="R.html"><a href="R.html#loops"><i class="fa fa-check"></i><b>2.11</b> Loops</a></li>
<li class="chapter" data-level="2.12" data-path="R.html"><a href="R.html#for"><i class="fa fa-check"></i><b>2.12</b> For</a></li>
<li class="chapter" data-level="2.13" data-path="R.html"><a href="R.html#while"><i class="fa fa-check"></i><b>2.13</b> While</a></li>
<li class="chapter" data-level="2.14" data-path="R.html"><a href="R.html#if-else-condicionales"><i class="fa fa-check"></i><b>2.14</b> If-else (condicionales)</a></li>
<li class="chapter" data-level="2.15" data-path="R.html"><a href="R.html#and-or-operadores-lógicos"><i class="fa fa-check"></i><b>2.15</b> And-or (Operadores lógicos)</a><ul>
<li class="chapter" data-level="2.15.1" data-path="R.html"><a href="R.html#and"><i class="fa fa-check"></i><b>2.15.1</b> And</a></li>
</ul></li>
<li class="chapter" data-level="2.16" data-path="R.html"><a href="R.html#or"><i class="fa fa-check"></i><b>2.16</b> Or</a></li>
<li class="chapter" data-level="2.17" data-path="R.html"><a href="R.html#ejercicio-1-1"><i class="fa fa-check"></i><b>2.17</b> Ejercicio 1</a></li>
<li class="chapter" data-level="2.18" data-path="R.html"><a href="R.html#media"><i class="fa fa-check"></i><b>2.18</b> Media</a></li>
<li class="chapter" data-level="2.19" data-path="R.html"><a href="R.html#desviación-estándar"><i class="fa fa-check"></i><b>2.19</b> Desviación estándar</a></li>
<li class="chapter" data-level="2.20" data-path="R.html"><a href="R.html#ejercicio-2-1"><i class="fa fa-check"></i><b>2.20</b> Ejercicio 2</a></li>
<li class="chapter" data-level="2.21" data-path="R.html"><a href="R.html#ejercicio-3"><i class="fa fa-check"></i><b>2.21</b> Ejercicio 3</a></li>
<li class="chapter" data-level="2.22" data-path="R.html"><a href="R.html#advertencias-y-otras-cosas-poco-intuitivas"><i class="fa fa-check"></i><b>2.22</b> Advertencias y otras cosas poco intuitivas</a></li>
<li class="chapter" data-level="2.23" data-path="R.html"><a href="R.html#donde-fallan-estas-cosas"><i class="fa fa-check"></i><b>2.23</b> Donde fallan estas cosas</a></li>
<li class="chapter" data-level="2.24" data-path="R.html"><a href="R.html#números-pseudoaleatorios"><i class="fa fa-check"></i><b>2.24</b> Números pseudoaleatorios</a></li>
<li class="chapter" data-level="2.25" data-path="R.html"><a href="R.html#ejercicio-4"><i class="fa fa-check"></i><b>2.25</b> Ejercicio 4</a></li>
<li class="chapter" data-level="2.26" data-path="R.html"><a href="R.html#aleatoreidad-en-r"><i class="fa fa-check"></i><b>2.26</b> Aleatoreidad en R</a></li>
<li class="chapter" data-level="2.27" data-path="R.html"><a href="R.html#las-semillas"><i class="fa fa-check"></i><b>2.27</b> Las semillas</a></li>
<li class="chapter" data-level="2.28" data-path="R.html"><a href="R.html#ejercicio-5"><i class="fa fa-check"></i><b>2.28</b> Ejercicio 5</a></li>
<li class="chapter" data-level="2.29" data-path="R.html"><a href="R.html#ejercicio-6"><i class="fa fa-check"></i><b>2.29</b> Ejercicio 6</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="análisis-exploratorio-de-datos.html"><a href="análisis-exploratorio-de-datos.html"><i class="fa fa-check"></i><b>3</b> Análisis Exploratorio de Datos</a><ul>
<li class="chapter" data-level="3.1" data-path="análisis-exploratorio-de-datos.html"><a href="análisis-exploratorio-de-datos.html#inicio"><i class="fa fa-check"></i><b>3.1</b> Inicio</a></li>
<li class="chapter" data-level="3.2" data-path="análisis-exploratorio-de-datos.html"><a href="análisis-exploratorio-de-datos.html#librerías"><i class="fa fa-check"></i><b>3.2</b> Librerías</a></li>
<li class="chapter" data-level="3.3" data-path="análisis-exploratorio-de-datos.html"><a href="análisis-exploratorio-de-datos.html#base-a-analizar"><i class="fa fa-check"></i><b>3.3</b> Base a analizar</a></li>
<li class="chapter" data-level="3.4" data-path="análisis-exploratorio-de-datos.html"><a href="análisis-exploratorio-de-datos.html#definiciones-y-notación"><i class="fa fa-check"></i><b>3.4</b> Definiciones y notación</a></li>
<li class="chapter" data-level="3.5" data-path="análisis-exploratorio-de-datos.html"><a href="análisis-exploratorio-de-datos.html#estadísticos-univariados"><i class="fa fa-check"></i><b>3.5</b> Estadísticos univariados</a><ul>
<li class="chapter" data-level="3.5.1" data-path="análisis-exploratorio-de-datos.html"><a href="análisis-exploratorio-de-datos.html#definición-estadístico-observado"><i class="fa fa-check"></i><b>3.5.1</b> Definición [Estadístico observado]</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="análisis-exploratorio-de-datos.html"><a href="análisis-exploratorio-de-datos.html#ejercicios-2"><i class="fa fa-check"></i><b>3.6</b> Ejercicios</a></li>
<li class="chapter" data-level="3.7" data-path="análisis-exploratorio-de-datos.html"><a href="análisis-exploratorio-de-datos.html#gráficas-univariadas"><i class="fa fa-check"></i><b>3.7</b> Gráficas univariadas</a></li>
<li class="chapter" data-level="3.8" data-path="análisis-exploratorio-de-datos.html"><a href="análisis-exploratorio-de-datos.html#gráficas-bivariadas"><i class="fa fa-check"></i><b>3.8</b> Gráficas bivariadas</a><ul>
<li class="chapter" data-level="3.8.1" data-path="análisis-exploratorio-de-datos.html"><a href="análisis-exploratorio-de-datos.html#ejercicio-7"><i class="fa fa-check"></i><b>3.8.1</b> Ejercicio</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="análisis-exploratorio-de-datos.html"><a href="análisis-exploratorio-de-datos.html#estadísticos-bivariados"><i class="fa fa-check"></i><b>3.9</b> Estadísticos bivariados</a><ul>
<li class="chapter" data-level="3.9.1" data-path="análisis-exploratorio-de-datos.html"><a href="análisis-exploratorio-de-datos.html#ejercicio-8"><i class="fa fa-check"></i><b>3.9.1</b> Ejercicio</a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="análisis-exploratorio-de-datos.html"><a href="análisis-exploratorio-de-datos.html#ejercicio-9"><i class="fa fa-check"></i><b>3.10</b> Ejercicio</a></li>
<li class="chapter" data-level="3.11" data-path="análisis-exploratorio-de-datos.html"><a href="análisis-exploratorio-de-datos.html#ajuste-funcional"><i class="fa fa-check"></i><b>3.11</b> Ajuste funcional</a><ul>
<li class="chapter" data-level="3.11.1" data-path="análisis-exploratorio-de-datos.html"><a href="análisis-exploratorio-de-datos.html#ejercicio-10"><i class="fa fa-check"></i><b>3.11.1</b> Ejercicio</a></li>
<li class="chapter" data-level="3.11.2" data-path="análisis-exploratorio-de-datos.html"><a href="análisis-exploratorio-de-datos.html#ejercicio-sugerido"><i class="fa fa-check"></i><b>3.11.2</b> Ejercicio sugerido</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="referencias.html"><a href="referencias.html"><i class="fa fa-check"></i>Referencias</a></li>
<li class="divider"></li>
<li><a href="https://github.com/RodrigoZepeda" target="blank">Elaborado por Rodrigo Zepeda y Luis Carlos Bernal</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Inferencia</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="header">
<h1 class="title">Inferencia</h1>
<p class="author"><em>Rodrigo Zepeda-Tello y Luis Carlos Bernal</em></p>
<p class="date"><em>2021-01-14</em></p>
</div>
<div id="intro" class="section level1">
<h1><span class="header-section-number">Capítulo 1</span> Introducción</h1>
<div id="estadística-y-muestras" class="section level2">
<h2><span class="header-section-number">1.1</span> Estadística y muestras</h2>
<p>La <a href="https://plato.stanford.edu/entries/statistics/#StaInd">enciclopedia Stanford de filosofía</a> establece la siguiente definición de estadística<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>.</p>
<div class="Definicion">
<p>
<strong>Estadística</strong> La estadística es una disciplina matemática y conceptual que se enfoca en la relación entre datos e hipótesis. Los datos son registros de observaciones o eventos en un estudio científico, por ejemplo, un conjunto de mediciones de individuos de una población. Los datos que son obtenidos se conoce como la muestra, datos muestrales, o simplemente los datos, y todas las posibles muestras posibles en un estudio forman una colección llamada el espacio muestra. Las hipótesis, por su parte, son enunciados generales sobre el sistema objetivo de la investigación científica, por ejemplo, expresar un hecho general sobre todos los individuos en la población. Una hipótesis estadística es un enunciado general que puede ser expresada como una distribución de probabilidad sobre el espacio muestral, es decir, ésta determina una probabilidad para cada una de las posibles muestras.
</p>
</div>
<p>De manera breve, la estadística es una disciplina que se encarga de, a través de muestras (cuantificadas como datos), describir el mundo. Y hay muchas cosas por describir: asociaciones, causalidad, realizar predicciones, establecer mecanismos de funcionamiento de objetos, etc. Es así como se establece su objetivo el cual de acuerdo con <span class="citation">Wackerly, Mendenhall, and Scheaffer (<a href="#ref-wackerly" role="doc-biblioref">2014</a>)</span> es:</p>
<blockquote>
<p>realizar una inferencia sobre la población con base en la información contenida en una muestra de dicha población y proveer una medida asociada de qué tan buena es la inferencia.</p>
</blockquote>
<p>Dentro de la definición previa y objetivos hay que destacar varios términos que son de importancia. La primera es la <strong>población</strong>, cualquier conjunto (no vacío) de objetos. Una <strong>población</strong> es lo más general posible, no necesariamente involucra personas o seres vivos. Algunos ejemplos de poblaciones incluyen: las personas que viven en Guatemala (si me interesa saber algo de los guatemaltecos en general), los árboles del Amazonas (si quiero saber cosas de ecología), los perros callejeros en Ciudad de México, los consumidores de una marca de cereal, los coches que transitan por Dubai, los granos de arena en una playa específica de Cancún, las células T dentro de los seres humanos o los metales pesados.</p>
<p>Más relevante que la población (para nuestros propósitos) es la <strong>población objetivo</strong> El conjunto de elementos que formarán parte del estudio. Definir la <strong>población objetivo</strong> es complicado en algunas situaciones; por ejemplo, si se desea saber si <em>los mexicanos</em> están a favor o en contra de legalizar la marihuana hay que establecer quiénes son <em>los mexicanos</em>. ¿Cuentan las personas con nacionalidad mexicana que residen en el extranjero? ¿Cuentan los menores de edad? ¿Qué pasa con los extranjeros que son residentes? De nuevo, la población objetivo no necesariamente son personas, es sólo aquello que nos interesa medir.</p>
<p>Idealmente el estudio estadístico sería sobre la población objetivo. Por ejemplo, si nos interesa estudiar la evolución de los enfermos de VIH, la <strong>población objetivo</strong> serían los enfermos. Sin embargo, en el mundo real es imposible conseguir a toda la población objetivo (dentro de los enfermos, por ejemplo, están aquellos que aún no saben que tienen la enfermedad y no acudirían a nuestro estudio). La <strong>población muestreada</strong> resulta de esta dificultad. La <strong>población muestreada</strong> es el conjunto de elementos sobre los cuales se construyó la muestra para el análisis estadístico. En el caso de los enfermos de VIH la <strong>población muestreada</strong> podrían ser las personas que para una fecha específica habían sido diagnosticadas (y nos olvidamos de quienes desconocen su diagnóstico) o toda la población mexicana (y llevamos kits de diagnóstico con nosotros cuando diagnostiquemos). En encuestas de consumo, por ejemplo, usualmente no se muestrean zonas remotas o de muy bajos recursos por lo que la <strong>población muestreada</strong> no coincide con la <strong>población objetivo</strong> (todos los consumidores) sino que son sólo los consumidores de mayor poder adquisitivo. En encuestas de elecciones si bien la población objetivo son <em>todas las personas que voten el día de la elección</em>, como la mayoría se hacen <em>antes</em> de la elección (exceptuando las de salida) entonces se aproxima la definición de <em>votante</em> buscando incluir sólo aquellos que estén registrados en el padrón electoral o bien aquellos que al ser encuestados digan que <em>sí</em> van a votar. Aquí la <strong>población muestreada</strong> tampoco coincide con la objetivo.</p>
<p>Una <strong>muestra</strong> es un subconjunto de la población muestreada. Si la muestra coincide con la población muestreada (es decir, muestreaste a todo el mundo) se dice que es un <strong>censo</strong>. Si se tiene un censo se conoce TODA la población por lo que no es necesario hacer ningún análisis de inferencia (ya sabes todo de todos). Puedes realizar predicciones o descripciones. Ejemplos de censos son las encuestas de fin de cursos, las calificaciones de todo un grupo o el registro de todas las compras de todas las personas en una tienda en línea.</p>
<blockquote>
<p><strong>Ojo</strong> No hay que confundir la definición de <strong>muestra</strong> con la definición estadística de <strong>muestra aleatoria</strong> (ver más adelante) la cual es un tipo muy específico de muestra obtenida bajo reglas restrictivas.</p>
</blockquote>
<p>Finalmente hay que definir <strong>inferencia</strong>, el propósito de estas notas. Para ello usaremos el ejemplo y una versión adaptada de la definición de <span class="citation">Boghossian (<a href="#ref-boghossian2014inference" role="doc-biblioref">2014</a>)</span>. Considera que sabes dos verdades:</p>
<ol style="list-style-type: decimal">
<li><p>Llovió anoche</p></li>
<li><p>Cuando llueve el suelo se moja</p></li>
</ol>
<p>por lo que esta mañana <em>infieres</em> que el suelo estará mojado y sales de tu casa con botas y no con chanclas. El proceso de <em>inferir</em> parece una consecuencia lógica de las premisas 1 y 2 pero no lo es exactamente: hoy es otro día y si hizo suficiente calor en la noche el agua pudo haberse evaporado del suelo. De ahí que definamos inferencia como:</p>
<blockquote>
<p>Realizar un juicio el cual se explica a partir de premisas que suponemos verdaderas.</p>
</blockquote>
<p>En particular <strong>la inferencia estadística</strong> será la rama de la estadística cuyo propósito es</p>
<blockquote>
<p>Realizar juicios probabilísticos a partir de datos que suponemos verdaderos.</p>
</blockquote>
<p>Aquí es necesario desglosar un poco la definición:</p>
<ul>
<li><p>Se habla de <strong>juicios probabilísticos</strong> pues nuestros juicios nunca van a ser tan certeros como <code>el suelo està mojado</code>. Más bien van a ser del estilo <code>hay una probabilidad muy alta de que el suelo esté mojado</code> o <code>nueve de cada diez veces el suelo estará mojado</code>.</p></li>
<li><p>La <strong>suposición de verdad</strong> de los datos es muy relevante. Imagina el siguiente experimento: tu amiga borracha durante una fiesta se le ocurre que, de la nada, desarrolló poderes de psíquica y puede adivinar el futuro resultado de una moneda (cara o cruz). Tiras una moneda diez veces y todas las veces tu amiga hace una predicción correcta. <em>Considerando los datos como verdad concluirías que tu amiga es psíquica</em>. Una observación a profundidad de la moneda quizá te revele que es una moneda truqueada que siempre cae en cara. En ese momento cambiarías la <strong>suposición de verdad</strong> de los datos y la inferencia de que tu amiga es psíquica.</p></li>
</ul>
<p>A lo largo de este libro aprenderemos lo básico para realizar inferencias estadísticas: observar datos y suponer verdades a partir de ellos. Tristemente la estadística nunca nos va a poder dar la verdad absoluta pero, si lo hacemos bien, es quizá lo más cerca que podamos estar de ella.</p>
</div>
<div id="modelos" class="section level2">
<h2><span class="header-section-number">1.2</span> Modelos</h2>
<p>La estadística funciona a partir de la construcción de <strong>modelos</strong>. Estos pretenden ser una forma de describir el mundo mediante teoría de la probabilidad y lo que se busca es utilizar dicha teoría para realizar inferencias. Estos modelos teóricos representan la forma en la que suponemos funciona la población. Para propósitos de estas notas diremos que los modelos viven <em>en el mundo de los modelos</em> o <em>mundo de las ideas</em>. Los datos observados, para poder distinguirlos, viven en <em>el mundo real</em>. Muchos de los modelos (no todos) se componen de <strong>parámetros</strong> que requieren para poder funcionar los cuales son estimados mediante <strong>estadísticos</strong> que se construyen a partir de los datos.</p>
<p>Para nuestros propósitos, los modelos que usaremos siempre construirán una población de la siguiente forma:</p>
<div class="Definicion">
<h3 id="población">
Población
</h3>
<p>
Una población es un conjunto no vacío de variables (o vectores) aleatorias. <span class="math display"><span class="math display">\[
\mathcal{X} = \{ X_1, X_2, \dots \}
\]</span></span>
</p>
</div>
<p>Una población no necesariamente es finita. Por ejemplo, si nos interesa saber el tiempo que tarda un cliente en ser atendido en una llamada telefónica al banco quizá podemos suponer que la llamada telefónica tiene una duración descrita por un modelo <span class="math inline">\(\text{Exponencial}(\lambda)\)</span>. La población sería el conjunto infinito de todas las posibles llamadas telefónicas que se pueden realizar bajo este modelo. Por otro lado, un ejemplo finito de una población, son las caras de una moneda en un experimento donde busquemos, para una moneda específica, si caen más caras que cruces (cae más de un lado que del otro).</p>
<p>Una <strong>muestra</strong> es cualquier subconjunto (posiblemente infinito también) de la población.</p>
<div class="Definicion">
<h3 id="muestra">
Muestra
</h3>
<p>
Una muestra <span class="math inline"><span class="math inline">\(\mathcal{M}\)</span></span> de una población <span class="math inline"><span class="math inline">\(\mathcal{X}\)</span></span> es cualquier subconjunto no vacío de <span class="math inline"><span class="math inline">\(\mathcal{X}\)</span></span>. Es decir, <span class="math inline"><span class="math inline">\(\mathcal{M}\)</span></span> es una muestra de <span class="math inline"><span class="math inline">\(\mathcal{X}\)</span></span> si: <span class="math display"><span class="math display">\[
 \mathcal{M} \subseteq \mathcal{X}
\]</span></span>
</p>
</div>
<p>Pocas veces hablaremos de <em>muestras</em> de manera general y nos enfocaremos, sobre todo, en <strong>muestras aleatorias</strong>:</p>
<div class="Definicion">
<h3 id="muestra-aleatoria">
Muestra aleatoria
</h3>
<p>
Una muestra aleatoria de tamaño <span class="math inline"><span class="math inline">\(n\)</span></span>, <span class="math inline"><span class="math inline">\(\mathcal{X}_{(n)}\)</span></span>, de una población <span class="math inline"><span class="math inline">\(\mathcal{X}\)</span></span> es un subconjunto finito (de tamaño <span class="math inline"><span class="math inline">\(n\)</span></span>), no vacío de <span class="math inline"><span class="math inline">\(\mathcal{X}\)</span></span> donde sus elementos son <strong>variables aleatorias independientes idénticamente distribuidas</strong>. Es decir, <span class="math inline"><span class="math inline">\(\mathcal{X}_{(n)}\)</span></span> es una muestra de <span class="math inline"><span class="math inline">\(\mathcal{X}\)</span></span> si:
</p>
<ol style="list-style-type: decimal">
<li>
<p>
<strong>Es una muestra</strong>: <span class="math inline"><span class="math inline">\(\mathcal{X}_{(n)} \subseteq \mathcal{X}\)</span></span>,
</p>
</li>
<li>
<p>
<strong>de tamaño <span class="math inline"><span class="math inline">\(n\)</span></span></strong>: <span class="math inline"><span class="math inline">\(\textrm{Cardinalidad}\Big( \mathcal{X}_{(n)} \Big) = n\)</span></span>,
</p>
</li>
<li>
<p>
<strong>con variables independientes</strong>: si <span class="math inline"><span class="math inline">\(X_i, X_j \in \mathcal{X}_{(n)}\)</span></span> entonces <span class="math inline"><span class="math inline">\(\mathbb{P}(X_i \in A , X_j \in B) = \mathbb{P}(X_i \in A)\cdot\mathbb{P}(X_j \in B)\)</span></span> para <span class="math inline"><span class="math inline">\(A,B\)</span></span> conjuntos <em>medibles</em>,
</p>
</li>
<li>
<p>
<strong>idénticamente distribuidas</strong>: para todo <span class="math inline"><span class="math inline">\(i = 1,2,\dots, n\)</span></span> se tiene que <span class="math inline"><span class="math inline">\(X_i\)</span></span> tiene función de distribución acumulada <span class="math inline"><span class="math inline">\(F_X\)</span></span>.
</p>
</li>
</ol>
</div>
<p>El punto 4. de la definición pide que todas las variables descritas tengan la misma distribución. Por ejemplo, podemos pedir que todas sean exponenciales con el mismo parámetro o todas sean gamma con los mismos parámetros. El punto es que todas las variables aleatorias estén descritas con el mismo modelo y sean independientes entre sí.</p>
<p>El punto 3. de la definición puede escribirse de otras formas más amigables, por ejemplo, si suponemos que las variables aleatorias son continuas y tienen densidad <span class="math inline">\(f_X\)</span> entonces la independencia puede escribirse como:
<span class="math display">\[
f_X(x_i, x_j) = f_X(x_i) \cdot f_X(x_j)
\]</span>
mientras que si son discretas con función de masa de probabilidad <span class="math inline">\(p_X\)</span> tenemos:
<span class="math display">\[
p_X(x_i, x_j) = p_X(x_i) \cdot p_X(x_j)
\]</span></p>
<p>La <strong>muestra observada</strong> así como <strong>la muestra aleatoria observada</strong> es el conjunto de <em>datos</em> que realmente viste. Mientras que la <strong>muestra</strong> y la <strong>muestra aleatoria</strong> viven <em>en el mundo de los modelos</em> y son variables aleatorias (constructos teóricos, como sabes, bastante complejos), la <strong>muestra observada</strong> es lo que se midió. Antes de dar la definición veamos un ejemplo con un dado.</p>
<div class="Ejemplo">
<h3 id="tiro-de-un-dado">
Tiro de un dado
</h3>
<p>
Se realiza un experimento para saber si un dado es justo (todos los lados tienen la misma probabilidad). Para ello se tira el dado <span class="math inline"><span class="math inline">\(n = 10\)</span></span> veces y se registran los tiros: <span class="math inline"><span class="math inline">\(2,6,1,3,3,3,5,1,3,2\)</span></span>.
</p>
<p>
<em>Mundo del modelo</em>
</p>
<p>
La población en este caso es el conjunto infinito de todos los posibles tiros del dado. De ese conjunto obtenemos una muestra aleatoria de tamaño <span class="math inline"><span class="math inline">\(n = 10\)</span></span> (suponemos que los tiros son independientes entre sí) dada por: <span class="math display"><span class="math display">\[
X_{(n)} = \{ X_1, X_2, X_3, \dots, X_{10}\}
\]</span></span> donde <span class="math inline"><span class="math inline">\(X_i\)</span></span> tiene la siguiente distribución: <span class="math display"><span class="math display">\[
\mathbb{P}(X_i = z) = 
\begin{cases}
p_1  \text{ si } z = 1 \\
p_2  \text{ si } z = 2 \\
p_3  \text{ si } z = 3 \\
p_4  \text{ si } z = 4 \\
p_5  \text{ si } z = 5 \\
p_6  \text{ si } z = 6 \\
0  \text{ en otro caso.}
\end{cases}
\]</span></span> donde <span class="math inline"><span class="math inline">\(\sum_{k = 1}^n p_k = 1\)</span></span> y <span class="math inline"><span class="math inline">\(p_{k} \geq 0\)</span></span> para todo <span class="math inline"><span class="math inline">\(k\)</span></span>. Lo que interesa en este estudio es <em>inferir</em> quiénes son las <span class="math inline"><span class="math inline">\(p_k\)</span></span> para determinar si es más probable que caiga en un lado que en otro. Las <span class="math inline"><span class="math inline">\(p_k\)</span></span> se conocen como parámetros.
</p>
<p>
<em>Mundo real</em>
</p>
<p>
Ya en la realidad en esos <span class="math inline"><span class="math inline">\(10\)</span></span> tiros no observamos cualquier cosa, observamos valores específicos que hacen que <strong>la muestra aleatoria observada</strong> sea: <span class="math display"><span class="math display">\[
s_n = \{x_1, x_2, \dots, x_{10} \} = \{2,6,1,3,3,3,5,1,3,2\}.
\]</span></span> Por supuesto que repitiendo el experimento (volviendo a tirar el dado 10 veces) lo más probable es que la <strong>muestra aleatoria observada</strong> cambie (y veamos otros números) pero el modelo, reflejado en la <strong>muestra aleatoria</strong> (teórica), permanezca inmutable. Una forma de estimar las probabilidades podría ser mediante proporciones y calcular, por ejemplo, la probabilidad de que aparezca <span class="math inline"><span class="math inline">\(1\)</span></span> como: <span class="math display"><span class="math display">\[
\hat{p}_1 = \frac{\text{Veces que aparece 1}}{n} = \frac{2}{10}
\]</span></span> En este caso, <span class="math inline"><span class="math inline">\(\hat{p}_1\)</span></span> dado por <span class="math inline"><span class="math inline">\(\frac{2}{10}\)</span></span> es un <strong>estimador observado</strong> de la verdadera probabilidad <span class="math inline"><span class="math inline">\(p_1\)</span></span> que vive en el mundo de los modelos (y jamás podremos conocer)
</p>
</div>
<p>Armados con el ejemplo anterior realicemos la definición de las muestras observadas:</p>
<div class="Definicion">
<h3 id="muestra-observada">
Muestra observada
</h3>
<p>
Una muestra observada es una colección no vacía de valores codificados como números reales los cuales corresponden a realizaciones de una muestra <span class="math inline"><span class="math inline">\(\mathcal{X}\)</span></span>. Usualmente la denotamos: <span class="math display"><span class="math display">\[
s = \{ x_1, x_2, \dots\}
\]</span></span> donde las <span class="math inline"><span class="math inline">\(x_i\)</span></span> <strong>NO SON VARIABLES ALEATORIAS</strong> sino que son datos <strong>fijos</strong> ya observados.
</p>
</div>
<div class="Definicion">
<h3 id="muestra-aleatoria-observada">
Muestra aleatoria observada
</h3>
<p>
Una muestra aleatoria observada es una colección no vacía de tamaño <span class="math inline"><span class="math inline">\(n\)</span></span> de valores codificados como números reales los cuales corresponden a realizaciones de una muestra aleatoria <span class="math inline"><span class="math inline">\(\mathcal{X}_{(n)}\)</span></span>. En particular suponemos que <span class="math inline"><span class="math inline">\(x_1\)</span></span> es el valor observado de la variable aleatoria <span class="math inline"><span class="math inline">\(X_1\)</span></span>, <span class="math inline"><span class="math inline">\(x_2\)</span></span> es el valor observado de la variable aleatoria <span class="math inline"><span class="math inline">\(X_2\)</span></span> y así sucesivamente. Generalmente la denotamos por: <span class="math display"><span class="math display">\[
s_{(n)} = \{ x_1, x_2, \dots, x_n\}
\]</span></span> donde las <span class="math inline"><span class="math inline">\(x_i\)</span></span> <strong>NO SON VARIABLES ALEATORIAS</strong> sino que son datos <strong>fijos</strong> ya observados.
</p>
</div>
<p>Veamos un segundo ejemplo:</p>
<div class="Ejemplo">
<h3 id="cantidad-de-personas-que-llegan-a-una-tienda">
Cantidad de personas que llegan a una tienda
</h3>
<p>
En muchos casos la llegada de personas se supone que sigue una distribución Poisson. En este caso nos interesa estimar el número promedio de personas por día que hay en una tienda de la cual se han medido las siguientes cantidades (por día). Suponemos que las llegadas son independientes entre sí (la cantidad de gente que llegó un día no influye en la cantidad que llegó el otro).
</p>
<table>
<thead>
<tr class="header">
<th>
<strong>Día</strong>
</th>
<th>
<strong>Número de personas</strong>
</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>
1
</td>
<td>
50
</td>
</tr>
<tr class="even">
<td>
2
</td>
<td>
45
</td>
</tr>
<tr class="odd">
<td>
3
</td>
<td>
60
</td>
</tr>
<tr class="even">
<td>
4
</td>
<td>
65
</td>
</tr>
<tr class="odd">
<td>
5
</td>
<td>
55
</td>
</tr>
<tr class="even">
<td>
6
</td>
<td>
40
</td>
</tr>
</tbody>
</table>
<p>
<em>Mundo del modelo</em>
</p>
<p>
La población en este caso es el conjunto infinito de todas las posibles formas en que en un día pueden llegar personas. De ese conjunto obtenemos una muestra aleatoria de tamaño <span class="math inline"><span class="math inline">\(n = 6\)</span></span> (suponemos que las observaciones son independientes entre sí) dada por: <span class="math display"><span class="math display">\[
X_{(n)} = \{ X_1, X_2, X_3, X_4, X_5, X_6\}
\]</span></span> donde las <span class="math inline"><span class="math inline">\(X_i \sim \text{Poisson}(\lambda)\)</span></span> (todas con el mismo <span class="math inline"><span class="math inline">\(\lambda\)</span></span>). Recordamos que la media de una Poisson es <span class="math inline"><span class="math inline">\(\lambda\)</span></span> por lo que el <strong>parámetro</strong> que nos interesa estimar es <span class="math inline"><span class="math inline">\(\lambda\)</span></span>.
</p>
<p>
<em>Mundo real</em>
</p>
<p>
A partir de las <span class="math inline"><span class="math inline">\(6\)</span></span> llegadas observadas construimos <strong>la muestra aleatoria observada</strong>: <span class="math display"><span class="math display">\[
s_n = \{x_1, x_2, x_3, x_4, x_5, x_6 \} = \{50, 45, 60, 65, 55, 40\}.
\]</span></span> Una forma de estimar la media <span class="math inline"><span class="math inline">\(\lambda\)</span></span> es mediante el siguiente <strong>estimador observado</strong>: <span class="math display"><span class="math display">\[
\hat{\lambda} = \frac{1}{6} \sum_{i = 1}^6 x_i = 52.5
\]</span></span> Ojo, esto no significa que <span class="math inline"><span class="math inline">\(\lambda\)</span></span> <em>sea</em> <span class="math inline"><span class="math inline">\(52.5\)</span></span>. Significa que nuestra hipótesis de quién es <span class="math inline"><span class="math inline">\(\lambda\)</span></span> es <span class="math inline"><span class="math inline">\(52.5\)</span></span> y que esperaríamos la próxima vez en la tienda <span class="math inline"><span class="math inline">\(52\)</span></span> ó <span class="math inline"><span class="math inline">\(53\)</span></span> personas. En el mundo real <em>quién sabe cuánto vale <span class="math inline"><span class="math inline">\(\lambda\)</span></span></em> , nuestra hipótesis es que vale <span class="math inline"><span class="math inline">\(52.5\)</span></span> pero eso no necesarimente es la realidad.
</p>
</div>
<p>Como ya establecimos, muchas veces el modelo utiliza un <strong>parámetro</strong> el cual es desconocido. A partir de los datos construimos un <strong>estimador observado</strong> el cual es nuestra hipótesis del verdadero valor del parámetro. En general va a ser imposible que le atinemos al <em>verdadero</em> valor del parámetro pero la idea es que el <strong>estimador observado</strong> esté lo suficientemente cerca. En el ejemplo anterior nos gustaría, por ejemplo, que el verdadero parámetro quizá fuera <span class="math inline">\(\lambda = 52\)</span> ó <span class="math inline">\(\lambda = 54\)</span> pero nos sacaría mucho de onda que el parámetro real fuera <span class="math inline">\(\lambda = 1000000\)</span>.</p>
<div class="Definicion">
<h3 id="distirbución-paramétrica">
Distirbución paramétrica
</h3>
<p>
Una función de distirbución acumulada es una <strong>distribución paramétrica</strong> con parámetro <span class="math inline"><span class="math inline">\(\vec{\theta}\)</span></span> si dada una colección de distribuciones <span class="math display"><span class="math display">\[
\{ F_{\vec{\theta}} | \theta \in \Theta \}
\]</span></span> determinar <span class="math inline"><span class="math inline">\(\vec{\theta}\)</span></span> determina la distribución. Es decir, la familia de distribuciones está indizada por <span class="math inline"><span class="math inline">\(\vec{\theta}\)</span></span>. A <span class="math inline"><span class="math inline">\(\vec{\theta}\)</span></span> se le conoce como el <strong>parámetro</strong> o <strong>vector de parámetros</strong>.
</p>
</div>
<p>La definición anterior suena muy compleja sin embargo los ejemplos ya los conocemos.</p>
<div class="Ejemplo">
<h3 id="la-normal">
La normal
</h3>
<p>
La distribución normal es una distribución paramétrica con <span class="math display"><span class="math display">\[
\vec{\theta} = (\mu, \sigma^2)^T
\]</span></span> el vector de parámetros dado por la media y la varianza.
</p>
</div>
<div class="Ejemplo">
<h3 id="la-exponencial">
La exponencial
</h3>
<p>
La distribución exponencial es una distribución paramétrica con <span class="math display"><span class="math display">\[
\theta = \lambda
\]</span></span> el parámetro que establece la tasa de la exponencial.
</p>
</div>
<div class="Ejemplo">
<h3 id="la-normal-con-varianza-1">
La normal con varianza 1
</h3>
<p>
La distribución normal con varianza 1 es una distribución paramétrica con <span class="math display"><span class="math display">\[
\theta = \mu
\]</span></span> En este caso la varianza es conocida (<span class="math inline"><span class="math inline">\(\sigma^2 = 1\)</span></span>) pero la media no por eso sólo la media es el parámetro.
</p>
</div>
<p>Podemos entonces definir un <strong>estimador</strong>:</p>
<div class="Definicion">
<h3 id="estimador">
Estimador
</h3>
<p>
Dada una distribución paramétrica <span class="math inline"><span class="math inline">\(F_{\theta}\)</span></span> con parámetro <span class="math inline"><span class="math inline">\(\theta\)</span></span> un estimador <span class="math inline"><span class="math inline">\(\hat{\theta}\)</span></span> de <span class="math inline"><span class="math inline">\(\theta\)</span></span> es una variable aleatoria que se construye como función de la muestra aleatoria: <span class="math display"><span class="math display">\[
\hat{\theta}: \mathcal{X}_{(n)} \to \Theta
\]</span></span> Como <span class="math inline"><span class="math inline">\(\hat{\theta}\)</span></span> es una función de la muestra aleatoria entonces puede representarse como: <span class="math display"><span class="math display">\[
\hat{\theta} = \hat{\theta}(X_1, X_2, \dots, X_n)
\]</span></span>
</p>
</div>
<p>Dado un conjunto de datos, el <strong>estimador observado de <span class="math inline">\(\theta\)</span></strong> es el estimador <span class="math inline">\(\hat{\theta}\)</span> de <span class="math inline">\(\theta\)</span> evaluado en los datos.</p>
<div class="Definicion">
<h3 id="estimador-observado">
Estimador observado
</h3>
<p>
Dada una distribución paramétrica <span class="math inline"><span class="math inline">\(F_{\theta}\)</span></span> con parámetro <span class="math inline"><span class="math inline">\(\theta\)</span></span> con estimador <span class="math inline"><span class="math inline">\(\hat{\theta}\)</span></span> y datos observados <span class="math inline"><span class="math inline">\(s_{(n)} = \{x_1, x_2, \dots, x_n\}\)</span></span> el <strong>estimador observado</strong> corresponde a la evaluación de <span class="math inline"><span class="math inline">\(\hat{\theta}\)</span></span> en <span class="math inline"><span class="math inline">\(s_{(n)}\)</span></span>; es decir: <span class="math display"><span class="math display">\[
\hat{\theta}(x_1, x_2, \dots, x_n)
\]</span></span>
</p>
</div>
<p>Veamos ejemplos para entender mejor cómo funciona esto.</p>
<div class="Ejemplo">
<h3 id="tiros-de-una-moneda">
Tiros de una moneda
</h3>
<p>
Se tiene una moneda que cae más de un lado que del otro. Interesa estimar <span class="math inline"><span class="math inline">\(p\)</span></span> la probabilidad de que caiga cruz. Para ello se toma una <strong>muestra aleatoria</strong> de <span class="math inline"><span class="math inline">\(5\)</span></span> tiros de la moneda: <span class="math display"><span class="math display">\[
X_{(n)} = \{X_1, X_2, \dots, X_{5} \}
\]</span></span> Suponemos que los tiros son independientes. El modelo entonces implicaría que <span class="math display"><span class="math display">\[
X_i \sim \text{Bernoulli}(p)
\]</span></span> para cada <span class="math inline"><span class="math inline">\(i = 1, 2, \dots, 5\)</span></span>. Si codificamos cruz como <span class="math inline"><span class="math inline">\(1\)</span></span> y cara como <span class="math inline"><span class="math inline">\(0\)</span></span>, la <strong>muestra aleatoria observada</strong> es: <span class="math display"><span class="math display">\[
s_{(n)} = \{1,1,1,0,1\} = \{x_1, x_2, \dots, x_5\}
\]</span></span> donde tuvimos tres cruces continuas, luego una cara y finalmente una cruz. Una opción de estimador observado sería contar la proporción de cruces haciendo: <span class="math display"><span class="math display">\[
\hat{\theta}(x_1, \dots, x_5) = \frac{1}{n} \sum_{k = 1}^n x_i = \frac{4}{5}
\]</span></span> de donde diríamos que nuestra hipótesis de cuánto vale el parámetro <span class="math inline"><span class="math inline">\(p\)</span></span> es <span class="math inline"><span class="math inline">\(4/5\)</span></span>. Por otro lado, el <strong>estimador</strong> teórico es: <span class="math display"><span class="math display">\[
\hat{\theta}(X_1, \dots, X_5) = \frac{1}{n} \sum_{k = 1}^n X_i 
\]</span></span> el cual tiene una distribución de probabilidad sencilla pues <span class="math inline"><span class="math inline">\(\sum_{k = 1}^n X_i \sim \textrm{Binomial}(n,p)\)</span></span>. Particularmente podemos calcular su valor esperado, por ejemplo, <span class="math display"><span class="math display">\[
\mathbb{E}\Big[  \hat{\theta}(X_1, \dots, X_5) \Big]  =  \mathbb{E}\Big[  \frac{1}{n} \sum_{k = 1}^n X_i  \Big] = \frac{1}{n}\sum_{k = 1}^n \mathbb{E}\Big[X_i \Big] = \frac{1}{n}\sum_{k = 1}^n  p = \frac{1}{n} np = p
\]</span></span> lo cual implica que el estimador, en promedio, devolvería el parámetro que nos interesa (esta propiedad se conoce como <em>ser insesgado</em> y lo veremos más adelante).
</p>
</div>
<div class="Ejemplo">
<h3 id="error-de-medición-de-una-app">
Error de medición de una app
</h3>
<p>
Una app que se dedica a medir la altura de edificios mediante la toma de videos tiene un error de medición con distribución normal y cuya varianza es <span class="math inline"><span class="math inline">\(1\)</span></span>. Interesa determinar el error de medición promedio, el parámetro <span class="math inline"><span class="math inline">\(\mu\)</span></span>. Para ello se toman videos y se miden edificios para obtener una colección de 7 errores de medición independientes en la siguiente <strong>muestra aleatoria</strong>: <span class="math display"><span class="math display">\[
X_{(n)} = \{X_1, X_2, \dots, X_{5} \}
\]</span></span> El modelo es <span class="math display"><span class="math display">\[
X_i \sim \text{Normal}(\mu, 1)
\]</span></span> para cada <span class="math inline"><span class="math inline">\(i = 1, 2, \dots, 7\)</span></span>. Si los datos fueron:
</p>
<table>
<thead>
<tr class="header">
<th>
<strong>Edificio</strong>
</th>
<th>
<strong>Error de medición</strong>
</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>
Bellas Artes
</td>
<td>
12.11
</td>
</tr>
<tr class="even">
<td>
Torre Latinoamericana
</td>
<td>
40.54
</td>
</tr>
<tr class="odd">
<td>
Catedral Metropolitana
</td>
<td>
22.07
</td>
</tr>
<tr class="even">
<td>
Palacio Nacional
</td>
<td>
15.22
</td>
</tr>
<tr class="odd">
<td>
Rectoría de la UNAM
</td>
<td>
45.18
</td>
</tr>
<tr class="even">
<td>
Guerrero Chimalli
</td>
<td>
33.39
</td>
</tr>
<tr class="odd">
<td>
Estadio Azteca
</td>
<td>
41.76
</td>
</tr>
</tbody>
</table>
<p>
la <strong>muestra aleatoria observada</strong> en este caso correspondió : <span class="math display"><span class="math display">\[
s_{(n)} = \{12.11,40.54,22.07,15.22,45.18, 33.39, 41.76\} = \{x_1, x_2, \dots, x_7\}
\]</span></span> Una opción de estimador observado sería calcular la media muestral haciendo: <span class="math display"><span class="math display">\[
\hat{\theta}(x_1, \dots, x_7) = \frac{1}{n} \sum_{k = 1}^n x_i = 30.03857
\]</span></span> de donde diríamos que nuestra hipótesis de cuánto vale el parámetro <span class="math inline"><span class="math inline">\(\mu\)</span></span> es <span class="math inline"><span class="math inline">\(30.03857\)</span></span>. Por otro lado, el <strong>estimador</strong> teórico es: <span class="math display"><span class="math display">\[
\hat{\theta}(X_1, \dots, X_7) = \frac{1}{n} \sum_{k = 1}^n X_i 
\]</span></span> tiene una distribución de probabilidad sencilla pues sabemos que <span class="math inline"><span class="math inline">\(\sum_{k = 1}^n X_i \sim \textrm{Normal}(\mu,\sigma^2)\)</span></span>. Particularmente podemos calcular su valor esperado, por ejemplo, <span class="math display"><span class="math display">\[
\mathbb{E}\Big[  \hat{\theta}(X_1, \dots, X_7) \Big]  =  \mathbb{E}\Big[  \frac{1}{n} \sum_{k = 1}^n X_i  \Big] = \frac{1}{n}\sum_{k = 1}^n \mathbb{E}\Big[X_i \Big] = \frac{1}{n}\sum_{k = 1}^n  \mu = \frac{1}{n} n\mu = \mu
\]</span></span> lo cual implica que el estimador, en promedio, devolvería el parámetro que nos interesa (este también es <em>insesgado</em>).
</p>
</div>
<p>A modo de resumen y para concluir este capítulo introductorio, veamos un ejemplo más desarrollado de inferencia estadística.</p>
</div>
<div id="inferencia-estadística-ejemplo." class="section level2">
<h2><span class="header-section-number">1.3</span> Inferencia estadística: ejemplo.</h2>
<p>Se tiene una caja con cinco pelotas de colores rojo <span class="math inline">\(R\)</span> y azul <span class="math inline">\(A\)</span>. Las pelotas son indistinguibles<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> entre sí salvo por el color. Se desconoce exactamente la proporción de colores de la caja (es decir no se sabe cuál de las siguientes opciones es: <span class="math inline">\(\{ R, R, R, R, R\}\)</span>, <span class="math inline">\(\{ R, R, R, R, A\}\)</span>, <span class="math inline">\(\{ R, R, R, A, A\}\)</span>, <span class="math inline">\(\{ R, R, A, A, A\}\)</span>, <span class="math inline">\(\{ R, A, A, A, A\}\)</span>, <span class="math inline">\(\{ A, A, A, A, A\}\)</span>) y eso es lo que se desea determinar. Para ello se extrae una bola, se anota que su color fue rojo, <span class="math inline">\(R\)</span>, y se devuelve a la caja. Se extrae otra bola (que, pudo haber sido la misma que la inicial, recuerda que las bolas son indistinguibles y que la anterior se devolvió a la caja), se anota que su color fue rojo <span class="math inline">\(R\)</span> y se devuelve a la caja. Finalmente en la tercera extracción sale una pelota azul <span class="math inline">\(A\)</span>. Los datos observados (y ordenados) son los siguientes <span class="math inline">\(( R, R, A)\)</span>. Hay dos estimadores posibles de la probabilidad de que salga rojo <span class="math inline">\(p\)</span> que podemos calcular a partir de la <strong>muestra aleatoria</strong> ordenada <span class="math inline">\(( R, R, A)\)</span></p>
<div id="estimador-de-momentos" class="section level3">
<h3><span class="header-section-number">1.3.1</span> Estimador de momentos</h3>
<p>Una opción es estimar la probabilidad de que salga rojo, <span class="math inline">\(p\)</span>, mediante el conteo de cuántos rojos salieron divididos entre el total de extracciones. En este caso tendríamos el estimador evaluado en la muestra:
<span class="math display">\[
\hat{p}_M = \frac{2}{3}
\]</span>
Aquí una nota bien importante: es imposible <em>(¿por qué?)</em> que en la vida real la probabilidad <span class="math inline">\(p\)</span> de que salga rojo sea <span class="math inline">\(2/3\)</span>. El <span class="math inline">\(\hat{p}\)</span> es un estimador pero que jamás va a coincidir con el valor de verdad.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>. Sin embargo este estimador <span class="math inline">\(\hat{p}\)</span> tiene características interesantes. Para verlas, definamos primero una <strong>muestra aleatoria</strong> de tamaño <span class="math inline">\(3\)</span> de las pelotas en la urna:
<span class="math display">\[
X_{(n)} = \{ X_1, X_2, X_3\}
\]</span>
donde marcaremos <span class="math inline">\(X_i = 1\)</span> si salió rojo y <span class="math inline">\(X_i = 0\)</span> si salió azul. Preguntarnos por la probabilidad de rojo es lo mismo que preguntarnos por la probabilidad de que <span class="math inline">\(X_i = 1\)</span>. El estimador <span class="math inline">\(\hat{p}\)</span> evaluado en la muestra, la suma ponderada de todos, (los rojos aportan <span class="math inline">\(1\)</span> y los azules nada) está dado por:</p>
<p><span class="math display">\[
\hat{p}(X_1, X_2, X_3) = \frac{1}{3}(X_1 + X_2 + X_3)
\]</span></p>
<p>Notamos que para este caso los datos (muestra aleatoria observada) son <span class="math inline">\(x_1 = 1\)</span>, <span class="math inline">\(x_2 = 1\)</span> y <span class="math inline">\(x_3 = 0\)</span>. Por lo que el estimador observado es:</p>
<p><span class="math display">\[
\hat{p}(x_1, x_2, x_3) = \frac{1}{3}(x_1 + x_2 + x_3) = \frac{2}{3}
\]</span></p>
<p>Notamos que el estimador <span class="math inline">\(\hat{p}\)</span> es una variable aleatoria que depende de la muestra (aleatoria). Una vez que se tiene la muestra el estimador <span class="math inline">\(\hat{p}\)</span> colapsa en un único número real definido por la tabla. Pero antes de hacer el experimento (o bien si repetimos el experimento) el <span class="math inline">\(\hat{p}\)</span> es una variable aleatoria que puede obtener múltiples valores distintos. Como es una variable aleatoria podemos entonces calcular su varianza, por ejemplo, así como su media:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}\Big[ \hat{p} \Big] &amp; =  \frac{1}{3}\mathbb{E}\big[ X_1 + X_2 + X_3\big]
\\ &amp; = \frac{1}{3}\big( \mathbb{E}[ X_1] + \mathbb{E}[ X_2] + \mathbb{E}[ X_3]\big)
\\ &amp; = \frac{1}{3} 3p 
\\ &amp; = p
\end{aligned}
\]</span></p>
<p>por lo que si hiciéramos el ejercicio de muestreo múltiples veces los estimadores <span class="math inline">\(\hat{p}\)</span> que obtuviéramos le atinarían en promedio a <span class="math inline">\(p\)</span>. Por otro lado la varianza está dada por:</p>
<p><span class="math display">\[
\begin{aligned}
\textrm{Var}Big[ \hat{p} \Big] &amp; =  \frac{1}{9}\textrm{Var}\big[ X_1 + X_2 + X_3\big]
\\ &amp; = \frac{1}{9}\big( \textrm{Var}[ X_1] + \textrm{Var}[ X_2] + \textrm{Var}[ X_3]\big)
\\ &amp; = \frac{1}{9} 3p (1 - p)
\\ &amp; = \frac{1}{3} p (1 - p)
\end{aligned}
\]</span></p>
<p>Podemos calcular más propiedades probabilísticas de <span class="math inline">\(\hat{p}\)</span> pero el punto importante es que el <span class="math inline">\(\hat{p}\)</span> tiene su propia distribución.</p>
<p>En <code>R</code> podemos simular este proceso de extracción de los <span class="math inline">\(\hat{p}\)</span>:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="intro.html#cb1-1"></a><span class="kw">library</span>(ggplot2)</span>
<span id="cb1-2"><a href="intro.html#cb1-2"></a></span>
<span id="cb1-3"><a href="intro.html#cb1-3"></a><span class="co"># Número de veces que haremos el experimento de extraer 3 pelotas</span></span>
<span id="cb1-4"><a href="intro.html#cb1-4"></a>nsim           &lt;-<span class="st"> </span><span class="dv">100</span></span>
<span id="cb1-5"><a href="intro.html#cb1-5"></a>tamaño.muestra &lt;-<span class="st"> </span><span class="dv">3</span></span>
<span id="cb1-6"><a href="intro.html#cb1-6"></a></span>
<span id="cb1-7"><a href="intro.html#cb1-7"></a><span class="co">#La verdadera población</span></span>
<span id="cb1-8"><a href="intro.html#cb1-8"></a>poblacion &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;R&quot;</span>,<span class="st">&quot;R&quot;</span>,<span class="st">&quot;R&quot;</span>,<span class="st">&quot;A&quot;</span>,<span class="st">&quot;A&quot;</span>)</span>
<span id="cb1-9"><a href="intro.html#cb1-9"></a></span>
<span id="cb1-10"><a href="intro.html#cb1-10"></a><span class="co">#Aquí guardaremos los valores de pgorro</span></span>
<span id="cb1-11"><a href="intro.html#cb1-11"></a>pgorro    &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, nsim)</span>
<span id="cb1-12"><a href="intro.html#cb1-12"></a></span>
<span id="cb1-13"><a href="intro.html#cb1-13"></a><span class="co">#Repetimos el proceso de muestreo n veces</span></span>
<span id="cb1-14"><a href="intro.html#cb1-14"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>nsim){</span>
<span id="cb1-15"><a href="intro.html#cb1-15"></a>  muestra      &lt;-<span class="st"> </span><span class="kw">sample</span>(poblacion, tamaño.muestra, <span class="dt">replace =</span> T)</span>
<span id="cb1-16"><a href="intro.html#cb1-16"></a>  conteo_rojos &lt;-<span class="st"> </span><span class="kw">length</span>(<span class="kw">which</span>(muestra <span class="op">==</span><span class="st"> &quot;R&quot;</span>))</span>
<span id="cb1-17"><a href="intro.html#cb1-17"></a>  pgorro[i]    &lt;-<span class="st"> </span>conteo_rojos<span class="op">/</span>tamaño.muestra</span>
<span id="cb1-18"><a href="intro.html#cb1-18"></a>}</span>
<span id="cb1-19"><a href="intro.html#cb1-19"></a></span>
<span id="cb1-20"><a href="intro.html#cb1-20"></a><span class="kw">ggplot</span>() <span class="op">+</span></span>
<span id="cb1-21"><a href="intro.html#cb1-21"></a><span class="st">  </span><span class="kw">geom_histogram</span>(<span class="kw">aes</span>(<span class="dt">x =</span> pgorro, <span class="dt">y =</span> ..count..<span class="op">/</span><span class="dv">100</span>), </span>
<span id="cb1-22"><a href="intro.html#cb1-22"></a>                 <span class="dt">bins =</span> <span class="dv">10</span>, <span class="dt">fill =</span> <span class="st">&quot;purple&quot;</span>, <span class="dt">color =</span> <span class="st">&quot;white&quot;</span>) <span class="op">+</span></span>
<span id="cb1-23"><a href="intro.html#cb1-23"></a><span class="st">  </span><span class="kw">theme_classic</span>() <span class="op">+</span></span>
<span id="cb1-24"><a href="intro.html#cb1-24"></a><span class="st">  </span><span class="kw">labs</span>(</span>
<span id="cb1-25"><a href="intro.html#cb1-25"></a>    <span class="dt">x =</span> <span class="st">&quot;Valores de pgorro&quot;</span>,</span>
<span id="cb1-26"><a href="intro.html#cb1-26"></a>    <span class="dt">y =</span> <span class="st">&quot;Masa de probabilidad de pgorro&quot;</span>,</span>
<span id="cb1-27"><a href="intro.html#cb1-27"></a>    <span class="dt">title =</span> <span class="st">&quot;Función de masa de probabilidad de pgorro&quot;</span>,</span>
<span id="cb1-28"><a href="intro.html#cb1-28"></a>    <span class="dt">subtitle =</span> <span class="st">&quot;Aproximación por simulaciones&quot;</span></span>
<span id="cb1-29"><a href="intro.html#cb1-29"></a>  )</span></code></pre></div>
<p><img src="InferenciaEstadistica_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>Esto implica que aunque tengamos datos fijos <span class="math inline">\(\{ R,R,R A, A\}\)</span> como la extracción de la muestra es aleatoria el valor de <span class="math inline">\(\hat{p}\)</span> va a variar por el simple proceso de selección aleatoria de la muestra. Darnos cuenta de qué tanto varía nuestro valor a estimar y cómo se aleja (o no) de la verdad va a ser un punto muy importante (en general queremos estimadores <span class="math inline">\(\hat{p}\)</span> que no se alejen de los valores verdaderos).</p>
<p>Ahora, los estimadores <span class="math inline">\(\hat{p}\)</span> no son únicos y se nos puede ocurrir otro estimador como ejemplo:</p>
</div>
<div id="estimador-de-máxima-verosimilitud-maximización-discreta" class="section level3">
<h3><span class="header-section-number">1.3.2</span> Estimador de máxima verosimilitud (maximización discreta)</h3>
<p>Una segunda opción es buscar bajo qué valor de <span class="math inline">\(p\)</span> (de muchos posibles) son más probables los datos observados. Este criterio se conoce como el criterio de <strong>máxima verosimilitud</strong>. La idea es ver, bajo cada una de las construcciones posibles de la caja (<em>i.e.</em> <span class="math inline">\(\{ R, R, R, R, R\}\)</span>, <span class="math inline">\(\{ R, R, R, R, A\}\)</span>, <span class="math inline">\(\{ R, R, R, A, A\}\)</span>, <span class="math inline">\(\{ R, R, A, A, A\}\)</span>, <span class="math inline">\(\{ R, A, A, A, A\}\)</span>, <span class="math inline">\(\{ A, A, A, A, A\}\)</span>) son más probables los datos observados <span class="math inline">\(( R, R, A)\)</span> y elegir esa caja.
Vamos a resolver este problema calculando bajo cada escenario de caja las probabilidades de obtener la combinación observada <span class="math inline">\(( R, R, A)\)</span>.</p>
<p>La pregunta, antes de resolver el problema, es <em>¿y esto para qué sirve?</em> . Si bien los ejercicios de pelotas de colores y cajas son divertidos, estos por sí mismos no llegan muy lejos. Lo importante es que los ejercicios de urnas <em>son abstracciones</em> de problemas reales. Por ejemplo, el problema de urnas ocurre de manera poblacional cuando nos interesa estimar una proporción. En un país hay personas que padecen diabetes R y sin diabetes A. Se sabe que en el país hay 100 millones de personas. La proporción exacta (dentro del país) es desconocida. Sin embargo podemos hacer una encuesta y obtener una muestra de 100 personas dentro de las cuáles 20 padecieron diabetes y 80 no (<span class="math inline">\(\{20 R, 80 A\}\)</span>). De aquí, usando el mismo razonamiento que usaremos con la caja de pelotas podemos buscar la combinación de personas con diabetes y sin diabetes en el país donde la combinación de <span class="math inline">\(\{20 R, 80 A\}\)</span> es la más probable.</p>
<p>Este mismo razonamiento puede cambiarse de múltiples formas. En una encuesta podemos tener más de dos opciones. Por ejemplo, si interesa determinar la cantidad de personas que votarían por el partido rojo <span class="math inline">\(R\)</span>, por el azul <span class="math inline">\(A\)</span> o por el negro <span class="math inline">\(B\)</span> el modelo de pelotas en una caja ahora tiene tres tipos de pelota (y si hay <span class="math inline">\(n\)</span> partidos habría <span class="math inline">\(n\)</span> colores). Puede que los colores estén relacionados entre sí y extraer uno garantice la extracción de otro (por ejemplo si se entrevista gente en casas es muy probable que si se vive un niño en una casa <em>a fuerza</em> viva un adulto en ella mientras que la relación inversa no funciona: que un adulto habite una casa no determina que viva un niño en la misma). Otros cambios posibles son en el mecanismo de selección. Pudiera ser que las pelotas rojas <span class="math inline">\(R\)</span> fueran más grandes que las azules <span class="math inline">\(A\)</span> de tal forma que cuando se extrajeran hubiera mayor probabilidad de tener rojas en la muestra. Esto pasa, por ejemplo, cuando se hacen encuestas de productos. Generalmente sólo aquellas personas que tienen un sentimiento muy fuerte hacia un producto contestan la encuesta. De ahí que haya muchísimas reseñas diciendo que los productos son malísimos o buenísimos y nada intermedio: la gente que reseña algo con 3 estrellas son las pelotas azules que son más difíciles de extraer. Poco a poco veremos otros problemas con pelotas y urnas con sus análogos al mundo real. Por ahora resolvamos el que se especifica más arriba.</p>
</div>
<div id="ejemplo-5-muestreo-de-urna-con-dos-clases-con-orden-con-reemplazo" class="section level3">
<h3><span class="header-section-number">1.3.3</span> Ejemplo 5: Muestreo de urna con dos clases, con orden, con reemplazo</h3>
<p>Considera una urna con cinco pelotas de colores rojo <span class="math inline">\(R\)</span> y azul <span class="math inline">\(A\)</span>. Se desconoce la proporción de pelotas en la urna. Las pelotas son indistinguibles entre sí salvo por el color. Se extrae de manera ordenada primero una bola roja, <span class="math inline">\(R\)</span>, y se devuelve a la urna. Luego se extrae una segunda bola roja, <span class="math inline">\(R\)</span>, y se devuelve a la urna. Finalmente se extrae una tercera bola y resulta azul: <span class="math inline">\(A\)</span>. La combinación ordenada de pelotas extraídas es: <span class="math inline">\(( R, R, A)\)</span>. Suponiendo todas las pelotas tienen la misma probabilidad de salir, cuál urna genera con mayor probabilidad los datos observados (y por tanto sería nuestra opción para decidir qué urna es la que tenemos): <span class="math inline">\(\{ R, R, R, R, R\}\)</span>, <span class="math inline">\(\{ R, R, R, R, A\}\)</span>, <span class="math inline">\(\{ R, R, R, A, A\}\)</span>, <span class="math inline">\(\{ R, R, A, A, A\}\)</span>, <span class="math inline">\(\{ R, A, A, A, A\}\)</span> ó <span class="math inline">\(\{ A, A, A, A, A\}\)</span>.</p>
<blockquote>
<h3 id="solución-5-muestreo-de-urna-con-dos-clases-con-orden-con-reemplazo"><span class="header-section-number">1.3.3</span> Solución 5: Muestreo de urna con dos clases, con orden, con reemplazo</h3>
<p>Hay dos combinaciones de posible urna que podemos descartar desde un inicio: la que sólo tiene rojos <span class="math inline">\(\{ R, R, R, R, R\}\)</span> y la que sólo tiene azules <span class="math inline">\(\{ A, A, A, A, A\}\)</span>. Esto porque los datos observados nos muestran que obtuvimos tantos rojos como azules. En estas dos descartadas la probabilidad de obtener <span class="math inline">\(( R, R, A)\)</span> es cero. Quedan como cajas posibles: la de cuatro rojas <span class="math inline">\(\{ R, R, R, R, A\}\)</span>, la de tres rojas <span class="math inline">\(\{ R, R, R, A, A\}\)</span>, la de tres azules <span class="math inline">\(\{ R, R, A, A, A\}\)</span>, la de una roja <span class="math inline">\(\{ R, A, A, A, A\}\)</span>. Comenzaré mi análisis con la primera que puse: los otros análisis son similares.</p>
<p><strong>Análisis de <span class="math inline">\(\{ R, R, R, R, A\}\)</span></strong></p>
<p>Una de las formas más posibles de enlistar todos los escenarios es con un árbol. La forma larga (e impráctica) consiste en enlistar cada una de las formas de extraer las pelotas de la urna como muestra la siguiente imagen:</p>
<div class="figure">
<img src="images/arbol_decision_1.jpeg" alt="" />
<p class="caption">Árbol de decisión para el análisis de <span class="math inline">\(\{ R, R, R, R, A\}\)</span> con reemplazo</p>
</div>
<p>Esta es una forma “segura” de resolver un problema: puede ser largo pero si no se te olvida nada ¡siempre es una posibilidad! Observa que de todas las opciones (combinadas del primer nodo,el segundo y el tercero) se tienen 16 extracciones de la forma <span class="math inline">\(( R, R, A)\)</span> de un total de 125 extracciones (<span class="math inline">\(5\)</span> opciones en el último nodo por <span class="math inline">\(5\)</span> formas de extraer el segundo por <span class="math inline">\(5\)</span> opciones primeras dan 125). La probabilidad entonces de extraer <span class="math inline">\(( R, R, A)\)</span> bajo este esquema es:
<span class="math display">\[
\textrm{Probabilidad de ( R, R, A) dada la urna \{ R, R, R, R, A\} con reemplazo} =  \dfrac{16}{125} 
\]</span></p>
<p><strong>Análisis de <span class="math inline">\(\{ R, R, R, A, A\}\)</span></strong></p>
<p>Vale la pena para este segundo análisis podar un poco el árbol. Podemos darnos cuenta que en el caso anterior todas las ramas rojas son iguales por lo que quizá no vale la pena ponerlas todas. Al calcular la probabilidad de la extracción <span class="math inline">\(( R, R, A)\)</span> dado que la urna es de la forma <span class="math inline">\(\{ R, R, R, A, A\}\)</span> trabajaremos más a fondo el árbol. El árbol inicial es como sigue:</p>
<div class="figure">
<img src="images/arbol_decision_2.jpeg" alt="" />
<p class="caption">Árbol de decisión en el caso <span class="math inline">\(\{ R, R, R, A, A\}\)</span> con reemplazo</p>
</div>
<p>En este caso notamos que todos los caminos iniciados por rojo, R, son idénticos lo mismo que los caminos iniciados por azul A por lo que podemos simplificar el árbol copiando sólo las ramas distintas y anotando cada rama a cuántas representa (las azules son <span class="math inline">\(2\)</span> de <span class="math inline">\(5\)</span> mientras que las rojas <span class="math inline">\(3\)</span> de <span class="math inline">\(5\)</span> de ahí los números <span class="math inline">\(2/5\)</span> y <span class="math inline">\(3/5\)</span>).</p>
<div class="figure">
<img src="images/arbol_decision_2_1.jpeg" alt="" />
<p class="caption">Árbol de decisión simplificado para el análisis de <span class="math inline">\(\{ R, R, R, A, A\}\)</span> con reemplazo</p>
</div>
<p>Para el caso que nos atañe en esta ocasión: obtener primero roja, luego otra roja y finalmente azul, <span class="math inline">\(( R, R, A)\)</span>, la única opción es la rama de abajo con probabilidades dadas por:
<span class="math display">\[
\textrm{Probabilidad de ( R, R, A) dada la urna \{R, R, R, A, A\} con reemplazo}  = \dfrac{3 \times 3 \times 2}{5 \times 5 \times 5} = \dfrac{18}{125}
\]</span>
Esta forma reducida es equivalente a la que hubiéramos obtenido haciendo el análisis completo del árbol (por la primera imagen donde hay <span class="math inline">\(18\)</span> opciones de <span class="math inline">\(125\)</span> resultados). El producto de arriba refiere a el número de opciones para primera roja, por el número de opciones para segunda roja <em>dado</em> que la primera fue roja y la última son las opciones para una tercera azul <em>condicional</em> en que las dos primeras fueron rojas. En el denominador sólo multiplicamos los casos totales que son <span class="math inline">\(5\)</span> ramas iniciales que ramifican en <span class="math inline">\(5\)</span> nodos secundarios y <span class="math inline">\(5\)</span> hojas finales.</p>
<p>Tómate unos minutos para intentar justificar por qué este conteo de multiplicaciones es equivalente a haber contado todas. Asegúrate de entenderlo bien antes de continuar ¡volveremos a ello más adelante!</p>
<p><strong>Análisis de <span class="math inline">\(\{ R, R, A, A, A\}\)</span></strong></p>
<p>En este análisis usaremos el mismo truco de un árbol de decisiones reducido que usamos la vez pasada. Lo construiremos paso a paso para mostrar el proceso. De manera inicial tenemos dos opciones: roja (<span class="math inline">\(2\)</span> bolas de <span class="math inline">\(5\)</span>) ó azul (<span class="math inline">\(3\)</span> bolas de <span class="math inline">\(5\)</span>) por lo que a partir de la raíz construimos el árbol con las dos opciones y sus probabilidades</p>
<div class="figure">
<img src="images/arbol_decision_3_1.jpeg" alt="" />
<p class="caption">Versión 1 del árbol de decisión simplificado para <span class="math inline">\(\{ R, R, A, A, A\}\)</span> con reemplazo</p>
</div>
<p>Dentro de la rama azul de nuevo tenemos la posibilidad de extraer rojas (<span class="math inline">\(2\)</span> de <span class="math inline">\(5\)</span>) o azules (<span class="math inline">\(3\)</span> de <span class="math inline">\(5\)</span>) por lo que se acoplan a la rama:</p>
<div class="figure">
<img src="images/arbol_decision_3_2.jpeg" alt="" />
<p class="caption">Versión 2 del árbol de decisión simplificado para <span class="math inline">\(\{ R, R, A, A, A\}\)</span> con reemplazo</p>
</div>
<p>La lógica es idéntica si salió roja: hay <span class="math inline">\(2/5\)</span> de probabilidad de extraer una roja o <span class="math inline">\(3/5\)</span> de extraer una azul</p>
<div class="figure">
<img src="images/arbol_decision_3_3.jpeg" alt="" />
<p class="caption">Versión 3 del árbol de decisión simplificado para <span class="math inline">\(\{ R, R, A, A, A\}\)</span> con reemplazo</p>
</div>
<p>Finalmente las últimas ramas del árbol se agregan de manera idéntica: <span class="math inline">\(2/5\)</span> de rojas y <span class="math inline">\(3/5\)</span> de azul en cada una de las posibles extracciones.</p>
<div class="figure">
<img src="images/arbol_decision_3_4.jpeg" alt="" />
<p class="caption">Versión final del árbol de decisión simplificado para <span class="math inline">\(\{ R, R, A, A, A\}\)</span> con reemplazo</p>
</div>
<p>Notamos entonces que podemos hacer el cálculo multiplicando (como hicimos la vez pasada) para obtener:
<span class="math display">\[
\textrm{Probabilidad de ( R, R, A) dada la urna \{R, R, A, A, A\} con reemplazo}  = \dfrac{2}{5} \cdot \dfrac{2}{5} \cdot \dfrac{3}{5} = \dfrac{12}{125}
\]</span></p>
<p><strong>Análisis de <span class="math inline">\(\{ R, A, A, A, A\}\)</span></strong></p>
<p>Ya para este último caso los árboles de decisiones te son familiares por lo que puedes verificar que en este caso el árbol es el siguiente:</p>
<div class="figure">
<img src="images/arbol_decision_4.jpeg" alt="" />
<p class="caption">Árbol de decisión simplificado para el análisis de <span class="math inline">\(\{ R, A, A, A, A\}\)</span> con reemplazo</p>
</div>
<p>donde la probabilidad de $( R, R, A) $ dado que la urna es <span class="math inline">\(\{ R, A, A, A, A\}\)</span> está dada por:
<span class="math display">\[
\textrm{Probabilidad de ( R, R, A) dada la urna \{ R, A, A, A, A\} con reemplazo} = \dfrac{1}{5} \cdot \dfrac{1}{5} \cdot \dfrac{4}{5} = \dfrac{4}{125}.
\]</span></p>
<p><strong>Conclusión</strong></p>
<p>Después de que analizamos las probabilidades bajo todas las combinaciones posibles de urna concluimos que la que tiene probabilidad más alta de generar en ese orden rojo, luego rojo y finalmente azul, <span class="math inline">\(( R, R, A)\)</span>, es la urna <span class="math inline">\(\{R, R, R, A, A\}\)</span> pues si la urna tiene esa combinación de pelotas la probabilidad de extraer <span class="math inline">\(( R, R, A)\)</span> es:
<span class="math display">\[
\textrm{Probabilidad de ( R, R, A) dada la urna \{R, R, R, A, A\} con reemplazo} = \dfrac{18}{125} = 0.144.
\]</span>
Siguiendo la idea de que la urna que tenemos es la que tiene mayor probabilidad de generar los datos observados (esto se conoce como <em>criterio de máxima verosimilitud</em> en estadística) entonces estimaríamos que la urna que tenemos es <span class="math inline">\(\{R, R, R, A, A\}\)</span> por lo que <span class="math inline">\(\hat{p} = \frac{3}{5}\)</span> es el estimador de máxima verosimilitud de <span class="math inline">\(p\)</span>.</p>
<p><strong>OJO</strong> Dadas las observaciones <span class="math inline">\(( R, R, A)\)</span> no podemos determinar a ciencia cierta cuál es la combinación de pelotas en la urna que tenemos. Por lo que siempre puede pasar que la combinación <em>real</em> sea distinta. Aquí el modelo nos dice por cuál optar (de manera lógica, aquella que genera con mayor probabilidad lo que observamos) pero la realidad ¡puede estar por otro lado!</p>
</blockquote>
<p>Una vez que obtuvimos el estimador, <span class="math inline">\(\hat{p}\)</span>, la segunda pregunta que nos interesará resolver es <em>qué tan buen estimador es <span class="math inline">\(\hat{p}\)</span></em>. Para ello también buscaríamos describir su distribución probabilística (su masa, su varianza, su valor esperado). Esto lo dejaremos para más adelante.</p>
</div>
</div>
<div id="resumen-de-la-sección" class="section level2">
<h2><span class="header-section-number">1.4</span> Resumen de la sección</h2>
<p>En esta sección aprendimos que, en el mundo de las ideas, tenemos poblaciones de las cuales obtenemos subconjuntos (muestras). Las muestras observadas corresponden a los datos mientras que las muestras (sin el “observadas”) corresponden al modelo en el mundo de las ideas. Lo que buscamos es estimar los valores que necesitamos para determinar el modelo (parámetros) y esos valores se obtienen a través de estimadores (puede haber varios para el mismo parámetro). Los estimadores viven también en el mundo de los modelos y ahí tienen su distribución de probabilidad, sus masas y densidades, sus valores esperados y sus varianzas. Describir esto nos ayuda a saber cómo se comportan los estimadores y decidir cuáles son los buenos. Profundizaremos más en la siguiente sección donde haremos un ejemplo muy específico: ¿qué pasa si mi población está compuesta de variables aleatorias normales? Jugaremos con ello y veremos por qué es normal usar la normal.</p>

</div>
</div>
<h3>Referencias</h3>
<div id="refs" class="references">
<div id="ref-boghossian2014inference">
<p>Boghossian, Paul. 2014. “What Is Inference?” <em>Philosophical Studies</em> 169 (1): 1–18.</p>
</div>
<div id="ref-wackerly">
<p>Wackerly, Dennis, William Mendenhall, and Richard L Scheaffer. 2014. <em>Mathematical Statistics with Applications</em>. Cengage Learning.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>Traducción y subrayado de Rodrigo<a href="intro.html#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>Pelotas distinguibles, por ejemplo, tendrían números distintos o serían de materiales diferentes o con marcas específicas para saber que una azul es distinta de la otra.<a href="intro.html#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>Esto porque en la caja hay cinco pelotas y el denominador de la probabilidad <span class="math inline">\(p\)</span> va a ser <span class="math inline">\(5\)</span> por construcción del modelo. No hay forma de que el <span class="math inline">\(5\)</span> se pueda simplificar en <span class="math inline">\(3\)</span> o viceversa.<a href="intro.html#fnref3" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>

<a href="R.html" class="navigation navigation-next navigation-unique" aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["InferenciaEstadistica.pdf", "InferenciaEstadistica.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
