[["intro.html", "Inferencia Capítulo 1 Introducción 1.1 Estadística y muestras 1.2 Modelos 1.3 Inferencia estadística: ejemplo. 1.4 Resumen de la sección", " Inferencia Rodrigo Zepeda-Tello y Luis Carlos Bernal 2021-01-14 Capítulo 1 Introducción 1.1 Estadística y muestras La enciclopedia Stanford de filosofía establece la siguiente definición de estadística1. Estadística La estadística es una disciplina matemática y conceptual que se enfoca en la relación entre datos e hipótesis. Los datos son registros de observaciones o eventos en un estudio científico, por ejemplo, un conjunto de mediciones de individuos de una población. Los datos que son obtenidos se conoce como la muestra, datos muestrales, o simplemente los datos, y todas las posibles muestras posibles en un estudio forman una colección llamada el espacio muestra. Las hipótesis, por su parte, son enunciados generales sobre el sistema objetivo de la investigación científica, por ejemplo, expresar un hecho general sobre todos los individuos en la población. Una hipótesis estadística es un enunciado general que puede ser expresada como una distribución de probabilidad sobre el espacio muestral, es decir, ésta determina una probabilidad para cada una de las posibles muestras. De manera breve, la estadística es una disciplina que se encarga de, a través de muestras (cuantificadas como datos), describir el mundo. Y hay muchas cosas por describir: asociaciones, causalidad, realizar predicciones, establecer mecanismos de funcionamiento de objetos, etc. Es así como se establece su objetivo el cual de acuerdo con Wackerly, Mendenhall, and Scheaffer (2014) es: realizar una inferencia sobre la población con base en la información contenida en una muestra de dicha población y proveer una medida asociada de qué tan buena es la inferencia. Dentro de la definición previa y objetivos hay que destacar varios términos que son de importancia. La primera es la población, cualquier conjunto (no vacío) de objetos. Una población es lo más general posible, no necesariamente involucra personas o seres vivos. Algunos ejemplos de poblaciones incluyen: las personas que viven en Guatemala (si me interesa saber algo de los guatemaltecos en general), los árboles del Amazonas (si quiero saber cosas de ecología), los perros callejeros en Ciudad de México, los consumidores de una marca de cereal, los coches que transitan por Dubai, los granos de arena en una playa específica de Cancún, las células T dentro de los seres humanos o los metales pesados. Más relevante que la población (para nuestros propósitos) es la población objetivo El conjunto de elementos que formarán parte del estudio. Definir la población objetivo es complicado en algunas situaciones; por ejemplo, si se desea saber si los mexicanos están a favor o en contra de legalizar la marihuana hay que establecer quiénes son los mexicanos. ¿Cuentan las personas con nacionalidad mexicana que residen en el extranjero? ¿Cuentan los menores de edad? ¿Qué pasa con los extranjeros que son residentes? De nuevo, la población objetivo no necesariamente son personas, es sólo aquello que nos interesa medir. Idealmente el estudio estadístico sería sobre la población objetivo. Por ejemplo, si nos interesa estudiar la evolución de los enfermos de VIH, la población objetivo serían los enfermos. Sin embargo, en el mundo real es imposible conseguir a toda la población objetivo (dentro de los enfermos, por ejemplo, están aquellos que aún no saben que tienen la enfermedad y no acudirían a nuestro estudio). La población muestreada resulta de esta dificultad. La población muestreada es el conjunto de elementos sobre los cuales se construyó la muestra para el análisis estadístico. En el caso de los enfermos de VIH la población muestreada podrían ser las personas que para una fecha específica habían sido diagnosticadas (y nos olvidamos de quienes desconocen su diagnóstico) o toda la población mexicana (y llevamos kits de diagnóstico con nosotros cuando diagnostiquemos). En encuestas de consumo, por ejemplo, usualmente no se muestrean zonas remotas o de muy bajos recursos por lo que la población muestreada no coincide con la población objetivo (todos los consumidores) sino que son sólo los consumidores de mayor poder adquisitivo. En encuestas de elecciones si bien la población objetivo son todas las personas que voten el día de la elección, como la mayoría se hacen antes de la elección (exceptuando las de salida) entonces se aproxima la definición de votante buscando incluir sólo aquellos que estén registrados en el padrón electoral o bien aquellos que al ser encuestados digan que sí van a votar. Aquí la población muestreada tampoco coincide con la objetivo. Una muestra es un subconjunto de la población muestreada. Si la muestra coincide con la población muestreada (es decir, muestreaste a todo el mundo) se dice que es un censo. Si se tiene un censo se conoce TODA la población por lo que no es necesario hacer ningún análisis de inferencia (ya sabes todo de todos). Puedes realizar predicciones o descripciones. Ejemplos de censos son las encuestas de fin de cursos, las calificaciones de todo un grupo o el registro de todas las compras de todas las personas en una tienda en línea. Ojo No hay que confundir la definición de muestra con la definición estadística de muestra aleatoria (ver más adelante) la cual es un tipo muy específico de muestra obtenida bajo reglas restrictivas. Finalmente hay que definir inferencia, el propósito de estas notas. Para ello usaremos el ejemplo y una versión adaptada de la definición de Boghossian (2014). Considera que sabes dos verdades: Llovió anoche Cuando llueve el suelo se moja por lo que esta mañana infieres que el suelo estará mojado y sales de tu casa con botas y no con chanclas. El proceso de inferir parece una consecuencia lógica de las premisas 1 y 2 pero no lo es exactamente: hoy es otro día y si hizo suficiente calor en la noche el agua pudo haberse evaporado del suelo. De ahí que definamos inferencia como: Realizar un juicio el cual se explica a partir de premisas que suponemos verdaderas. En particular la inferencia estadística será la rama de la estadística cuyo propósito es Realizar juicios probabilísticos a partir de datos que suponemos verdaderos. Aquí es necesario desglosar un poco la definición: Se habla de juicios probabilísticos pues nuestros juicios nunca van a ser tan certeros como el suelo està mojado. Más bien van a ser del estilo hay una probabilidad muy alta de que el suelo esté mojado o nueve de cada diez veces el suelo estará mojado. La suposición de verdad de los datos es muy relevante. Imagina el siguiente experimento: tu amiga borracha durante una fiesta se le ocurre que, de la nada, desarrolló poderes de psíquica y puede adivinar el futuro resultado de una moneda (cara o cruz). Tiras una moneda diez veces y todas las veces tu amiga hace una predicción correcta. Considerando los datos como verdad concluirías que tu amiga es psíquica. Una observación a profundidad de la moneda quizá te revele que es una moneda truqueada que siempre cae en cara. En ese momento cambiarías la suposición de verdad de los datos y la inferencia de que tu amiga es psíquica. A lo largo de este libro aprenderemos lo básico para realizar inferencias estadísticas: observar datos y suponer verdades a partir de ellos. Tristemente la estadística nunca nos va a poder dar la verdad absoluta pero, si lo hacemos bien, es quizá lo más cerca que podamos estar de ella. 1.2 Modelos La estadística funciona a partir de la construcción de modelos. Estos pretenden ser una forma de describir el mundo mediante teoría de la probabilidad y lo que se busca es utilizar dicha teoría para realizar inferencias. Estos modelos teóricos representan la forma en la que suponemos funciona la población. Para propósitos de estas notas diremos que los modelos viven en el mundo de los modelos o mundo de las ideas. Los datos observados, para poder distinguirlos, viven en el mundo real. Muchos de los modelos (no todos) se componen de parámetros que requieren para poder funcionar los cuales son estimados mediante estadísticos que se construyen a partir de los datos. Para nuestros propósitos, los modelos que usaremos siempre construirán una población de la siguiente forma: Población Una población es un conjunto no vacío de variables (o vectores) aleatorias. \\[ \\mathcal{X} = \\{ X_1, X_2, \\dots \\} \\] Una población no necesariamente es finita. Por ejemplo, si nos interesa saber el tiempo que tarda un cliente en ser atendido en una llamada telefónica al banco quizá podemos suponer que la llamada telefónica tiene una duración descrita por un modelo \\(\\text{Exponencial}(\\lambda)\\). La población sería el conjunto infinito de todas las posibles llamadas telefónicas que se pueden realizar bajo este modelo. Por otro lado, un ejemplo finito de una población, son las caras de una moneda en un experimento donde busquemos, para una moneda específica, si caen más caras que cruces (cae más de un lado que del otro). Una muestra es cualquier subconjunto (posiblemente infinito también) de la población. Muestra Una muestra \\(\\mathcal{M}\\) de una población \\(\\mathcal{X}\\) es cualquier subconjunto no vacío de \\(\\mathcal{X}\\). Es decir, \\(\\mathcal{M}\\) es una muestra de \\(\\mathcal{X}\\) si: \\[ \\mathcal{M} \\subseteq \\mathcal{X} \\] Pocas veces hablaremos de muestras de manera general y nos enfocaremos, sobre todo, en muestras aleatorias: Muestra aleatoria Una muestra aleatoria de tamaño \\(n\\), \\(\\mathcal{X}_{(n)}\\), de una población \\(\\mathcal{X}\\) es un subconjunto finito (de tamaño \\(n\\)), no vacío de \\(\\mathcal{X}\\) donde sus elementos son variables aleatorias independientes idénticamente distribuidas. Es decir, \\(\\mathcal{X}_{(n)}\\) es una muestra de \\(\\mathcal{X}\\) si: Es una muestra: \\(\\mathcal{X}_{(n)} \\subseteq \\mathcal{X}\\), de tamaño \\(n\\): \\(\\textrm{Cardinalidad}\\Big( \\mathcal{X}_{(n)} \\Big) = n\\), con variables independientes: si \\(X_i, X_j \\in \\mathcal{X}_{(n)}\\) entonces \\(\\mathbb{P}(X_i \\in A , X_j \\in B) = \\mathbb{P}(X_i \\in A)\\cdot\\mathbb{P}(X_j \\in B)\\) para \\(A,B\\) conjuntos medibles, idénticamente distribuidas: para todo \\(i = 1,2,\\dots, n\\) se tiene que \\(X_i\\) tiene función de distribución acumulada \\(F_X\\). El punto 4. de la definición pide que todas las variables descritas tengan la misma distribución. Por ejemplo, podemos pedir que todas sean exponenciales con el mismo parámetro o todas sean gamma con los mismos parámetros. El punto es que todas las variables aleatorias estén descritas con el mismo modelo y sean independientes entre sí. El punto 3. de la definición puede escribirse de otras formas más amigables, por ejemplo, si suponemos que las variables aleatorias son continuas y tienen densidad \\(f_X\\) entonces la independencia puede escribirse como: \\[ f_X(x_i, x_j) = f_X(x_i) \\cdot f_X(x_j) \\] mientras que si son discretas con función de masa de probabilidad \\(p_X\\) tenemos: \\[ p_X(x_i, x_j) = p_X(x_i) \\cdot p_X(x_j) \\] La muestra observada así como la muestra aleatoria observada es el conjunto de datos que realmente viste. Mientras que la muestra y la muestra aleatoria viven en el mundo de los modelos y son variables aleatorias (constructos teóricos, como sabes, bastante complejos), la muestra observada es lo que se midió. Antes de dar la definición veamos un ejemplo con un dado. Tiro de un dado Se realiza un experimento para saber si un dado es justo (todos los lados tienen la misma probabilidad). Para ello se tira el dado \\(n = 10\\) veces y se registran los tiros: \\(2,6,1,3,3,3,5,1,3,2\\). Mundo del modelo La población en este caso es el conjunto infinito de todos los posibles tiros del dado. De ese conjunto obtenemos una muestra aleatoria de tamaño \\(n = 10\\) (suponemos que los tiros son independientes entre sí) dada por: \\[ X_{(n)} = \\{ X_1, X_2, X_3, \\dots, X_{10}\\} \\] donde \\(X_i\\) tiene la siguiente distribución: \\[ \\mathbb{P}(X_i = z) = \\begin{cases} p_1 \\text{ si } z = 1 \\\\ p_2 \\text{ si } z = 2 \\\\ p_3 \\text{ si } z = 3 \\\\ p_4 \\text{ si } z = 4 \\\\ p_5 \\text{ si } z = 5 \\\\ p_6 \\text{ si } z = 6 \\\\ 0 \\text{ en otro caso.} \\end{cases} \\] donde \\(\\sum_{k = 1}^n p_k = 1\\) y \\(p_{k} \\geq 0\\) para todo \\(k\\). Lo que interesa en este estudio es inferir quiénes son las \\(p_k\\) para determinar si es más probable que caiga en un lado que en otro. Las \\(p_k\\) se conocen como parámetros. Mundo real Ya en la realidad en esos \\(10\\) tiros no observamos cualquier cosa, observamos valores específicos que hacen que la muestra aleatoria observada sea: \\[ s_n = \\{x_1, x_2, \\dots, x_{10} \\} = \\{2,6,1,3,3,3,5,1,3,2\\}. \\] Por supuesto que repitiendo el experimento (volviendo a tirar el dado 10 veces) lo más probable es que la muestra aleatoria observada cambie (y veamos otros números) pero el modelo, reflejado en la muestra aleatoria (teórica), permanezca inmutable. Una forma de estimar las probabilidades podría ser mediante proporciones y calcular, por ejemplo, la probabilidad de que aparezca \\(1\\) como: \\[ \\hat{p}_1 = \\frac{\\text{Veces que aparece 1}}{n} = \\frac{2}{10} \\] En este caso, \\(\\hat{p}_1\\) dado por \\(\\frac{2}{10}\\) es un estimador observado de la verdadera probabilidad \\(p_1\\) que vive en el mundo de los modelos (y jamás podremos conocer) Armados con el ejemplo anterior realicemos la definición de las muestras observadas: Muestra observada Una muestra observada es una colección no vacía de valores codificados como números reales los cuales corresponden a realizaciones de una muestra \\(\\mathcal{X}\\). Usualmente la denotamos: \\[ s = \\{ x_1, x_2, \\dots\\} \\] donde las \\(x_i\\) NO SON VARIABLES ALEATORIAS sino que son datos fijos ya observados. Muestra aleatoria observada Una muestra aleatoria observada es una colección no vacía de tamaño \\(n\\) de valores codificados como números reales los cuales corresponden a realizaciones de una muestra aleatoria \\(\\mathcal{X}_{(n)}\\). En particular suponemos que \\(x_1\\) es el valor observado de la variable aleatoria \\(X_1\\), \\(x_2\\) es el valor observado de la variable aleatoria \\(X_2\\) y así sucesivamente. Generalmente la denotamos por: \\[ s_{(n)} = \\{ x_1, x_2, \\dots, x_n\\} \\] donde las \\(x_i\\) NO SON VARIABLES ALEATORIAS sino que son datos fijos ya observados. Veamos un segundo ejemplo: Cantidad de personas que llegan a una tienda En muchos casos la llegada de personas se supone que sigue una distribución Poisson. En este caso nos interesa estimar el número promedio de personas por día que hay en una tienda de la cual se han medido las siguientes cantidades (por día). Suponemos que las llegadas son independientes entre sí (la cantidad de gente que llegó un día no influye en la cantidad que llegó el otro). Día Número de personas 1 50 2 45 3 60 4 65 5 55 6 40 Mundo del modelo La población en este caso es el conjunto infinito de todas las posibles formas en que en un día pueden llegar personas. De ese conjunto obtenemos una muestra aleatoria de tamaño \\(n = 6\\) (suponemos que las observaciones son independientes entre sí) dada por: \\[ X_{(n)} = \\{ X_1, X_2, X_3, X_4, X_5, X_6\\} \\] donde las \\(X_i \\sim \\text{Poisson}(\\lambda)\\) (todas con el mismo \\(\\lambda\\)). Recordamos que la media de una Poisson es \\(\\lambda\\) por lo que el parámetro que nos interesa estimar es \\(\\lambda\\). Mundo real A partir de las \\(6\\) llegadas observadas construimos la muestra aleatoria observada: \\[ s_n = \\{x_1, x_2, x_3, x_4, x_5, x_6 \\} = \\{50, 45, 60, 65, 55, 40\\}. \\] Una forma de estimar la media \\(\\lambda\\) es mediante el siguiente estimador observado: \\[ \\hat{\\lambda} = \\frac{1}{6} \\sum_{i = 1}^6 x_i = 52.5 \\] Ojo, esto no significa que \\(\\lambda\\) sea \\(52.5\\). Significa que nuestra hipótesis de quién es \\(\\lambda\\) es \\(52.5\\) y que esperaríamos la próxima vez en la tienda \\(52\\) ó \\(53\\) personas. En el mundo real quién sabe cuánto vale \\(\\lambda\\) , nuestra hipótesis es que vale \\(52.5\\) pero eso no necesarimente es la realidad. Como ya establecimos, muchas veces el modelo utiliza un parámetro el cual es desconocido. A partir de los datos construimos un estimador observado el cual es nuestra hipótesis del verdadero valor del parámetro. En general va a ser imposible que le atinemos al verdadero valor del parámetro pero la idea es que el estimador observado esté lo suficientemente cerca. En el ejemplo anterior nos gustaría, por ejemplo, que el verdadero parámetro quizá fuera \\(\\lambda = 52\\) ó \\(\\lambda = 54\\) pero nos sacaría mucho de onda que el parámetro real fuera \\(\\lambda = 1000000\\). Distirbución paramétrica Una función de distirbución acumulada es una distribución paramétrica con parámetro \\(\\vec{\\theta}\\) si dada una colección de distribuciones \\[ \\{ F_{\\vec{\\theta}} | \\theta \\in \\Theta \\} \\] determinar \\(\\vec{\\theta}\\) determina la distribución. Es decir, la familia de distribuciones está indizada por \\(\\vec{\\theta}\\). A \\(\\vec{\\theta}\\) se le conoce como el parámetro o vector de parámetros. La definición anterior suena muy compleja sin embargo los ejemplos ya los conocemos. La normal La distribución normal es una distribución paramétrica con \\[ \\vec{\\theta} = (\\mu, \\sigma^2)^T \\] el vector de parámetros dado por la media y la varianza. La exponencial La distribución exponencial es una distribución paramétrica con \\[ \\theta = \\lambda \\] el parámetro que establece la tasa de la exponencial. La normal con varianza 1 La distribución normal con varianza 1 es una distribución paramétrica con \\[ \\theta = \\mu \\] En este caso la varianza es conocida (\\(\\sigma^2 = 1\\)) pero la media no por eso sólo la media es el parámetro. Podemos entonces definir un estimador: Estimador Dada una distribución paramétrica \\(F_{\\theta}\\) con parámetro \\(\\theta\\) un estimador \\(\\hat{\\theta}\\) de \\(\\theta\\) es una variable aleatoria que se construye como función de la muestra aleatoria: \\[ \\hat{\\theta}: \\mathcal{X}_{(n)} \\to \\Theta \\] Como \\(\\hat{\\theta}\\) es una función de la muestra aleatoria entonces puede representarse como: \\[ \\hat{\\theta} = \\hat{\\theta}(X_1, X_2, \\dots, X_n) \\] Dado un conjunto de datos, el estimador observado de \\(\\theta\\) es el estimador \\(\\hat{\\theta}\\) de \\(\\theta\\) evaluado en los datos. Estimador observado Dada una distribución paramétrica \\(F_{\\theta}\\) con parámetro \\(\\theta\\) con estimador \\(\\hat{\\theta}\\) y datos observados \\(s_{(n)} = \\{x_1, x_2, \\dots, x_n\\}\\) el estimador observado corresponde a la evaluación de \\(\\hat{\\theta}\\) en \\(s_{(n)}\\); es decir: \\[ \\hat{\\theta}(x_1, x_2, \\dots, x_n) \\] Veamos ejemplos para entender mejor cómo funciona esto. Tiros de una moneda Se tiene una moneda que cae más de un lado que del otro. Interesa estimar \\(p\\) la probabilidad de que caiga cruz. Para ello se toma una muestra aleatoria de \\(5\\) tiros de la moneda: \\[ X_{(n)} = \\{X_1, X_2, \\dots, X_{5} \\} \\] Suponemos que los tiros son independientes. El modelo entonces implicaría que \\[ X_i \\sim \\text{Bernoulli}(p) \\] para cada \\(i = 1, 2, \\dots, 5\\). Si codificamos cruz como \\(1\\) y cara como \\(0\\), la muestra aleatoria observada es: \\[ s_{(n)} = \\{1,1,1,0,1\\} = \\{x_1, x_2, \\dots, x_5\\} \\] donde tuvimos tres cruces continuas, luego una cara y finalmente una cruz. Una opción de estimador observado sería contar la proporción de cruces haciendo: \\[ \\hat{\\theta}(x_1, \\dots, x_5) = \\frac{1}{n} \\sum_{k = 1}^n x_i = \\frac{4}{5} \\] de donde diríamos que nuestra hipótesis de cuánto vale el parámetro \\(p\\) es \\(4/5\\). Por otro lado, el estimador teórico es: \\[ \\hat{\\theta}(X_1, \\dots, X_5) = \\frac{1}{n} \\sum_{k = 1}^n X_i \\] el cual tiene una distribución de probabilidad sencilla pues \\(\\sum_{k = 1}^n X_i \\sim \\textrm{Binomial}(n,p)\\). Particularmente podemos calcular su valor esperado, por ejemplo, \\[ \\mathbb{E}\\Big[ \\hat{\\theta}(X_1, \\dots, X_5) \\Big] = \\mathbb{E}\\Big[ \\frac{1}{n} \\sum_{k = 1}^n X_i \\Big] = \\frac{1}{n}\\sum_{k = 1}^n \\mathbb{E}\\Big[X_i \\Big] = \\frac{1}{n}\\sum_{k = 1}^n p = \\frac{1}{n} np = p \\] lo cual implica que el estimador, en promedio, devolvería el parámetro que nos interesa (esta propiedad se conoce como ser insesgado y lo veremos más adelante). Error de medición de una app Una app que se dedica a medir la altura de edificios mediante la toma de videos tiene un error de medición con distribución normal y cuya varianza es \\(1\\). Interesa determinar el error de medición promedio, el parámetro \\(\\mu\\). Para ello se toman videos y se miden edificios para obtener una colección de 7 errores de medición independientes en la siguiente muestra aleatoria: \\[ X_{(n)} = \\{X_1, X_2, \\dots, X_{5} \\} \\] El modelo es \\[ X_i \\sim \\text{Normal}(\\mu, 1) \\] para cada \\(i = 1, 2, \\dots, 7\\). Si los datos fueron: Edificio Error de medición Bellas Artes 12.11 Torre Latinoamericana 40.54 Catedral Metropolitana 22.07 Palacio Nacional 15.22 Rectoría de la UNAM 45.18 Guerrero Chimalli 33.39 Estadio Azteca 41.76 la muestra aleatoria observada en este caso correspondió : \\[ s_{(n)} = \\{12.11,40.54,22.07,15.22,45.18, 33.39, 41.76\\} = \\{x_1, x_2, \\dots, x_7\\} \\] Una opción de estimador observado sería calcular la media muestral haciendo: \\[ \\hat{\\theta}(x_1, \\dots, x_7) = \\frac{1}{n} \\sum_{k = 1}^n x_i = 30.03857 \\] de donde diríamos que nuestra hipótesis de cuánto vale el parámetro \\(\\mu\\) es \\(30.03857\\). Por otro lado, el estimador teórico es: \\[ \\hat{\\theta}(X_1, \\dots, X_7) = \\frac{1}{n} \\sum_{k = 1}^n X_i \\] tiene una distribución de probabilidad sencilla pues sabemos que \\(\\sum_{k = 1}^n X_i \\sim \\textrm{Normal}(\\mu,\\sigma^2)\\). Particularmente podemos calcular su valor esperado, por ejemplo, \\[ \\mathbb{E}\\Big[ \\hat{\\theta}(X_1, \\dots, X_7) \\Big] = \\mathbb{E}\\Big[ \\frac{1}{n} \\sum_{k = 1}^n X_i \\Big] = \\frac{1}{n}\\sum_{k = 1}^n \\mathbb{E}\\Big[X_i \\Big] = \\frac{1}{n}\\sum_{k = 1}^n \\mu = \\frac{1}{n} n\\mu = \\mu \\] lo cual implica que el estimador, en promedio, devolvería el parámetro que nos interesa (este también es insesgado). A modo de resumen y para concluir este capítulo introductorio, veamos un ejemplo más desarrollado de inferencia estadística. 1.3 Inferencia estadística: ejemplo. Se tiene una caja con cinco pelotas de colores rojo \\(R\\) y azul \\(A\\). Las pelotas son indistinguibles2 entre sí salvo por el color. Se desconoce exactamente la proporción de colores de la caja (es decir no se sabe cuál de las siguientes opciones es: \\(\\{ R, R, R, R, R\\}\\), \\(\\{ R, R, R, R, A\\}\\), \\(\\{ R, R, R, A, A\\}\\), \\(\\{ R, R, A, A, A\\}\\), \\(\\{ R, A, A, A, A\\}\\), \\(\\{ A, A, A, A, A\\}\\)) y eso es lo que se desea determinar. Para ello se extrae una bola, se anota que su color fue rojo, \\(R\\), y se devuelve a la caja. Se extrae otra bola (que, pudo haber sido la misma que la inicial, recuerda que las bolas son indistinguibles y que la anterior se devolvió a la caja), se anota que su color fue rojo \\(R\\) y se devuelve a la caja. Finalmente en la tercera extracción sale una pelota azul \\(A\\). Los datos observados (y ordenados) son los siguientes \\(( R, R, A)\\). Hay dos estimadores posibles de la probabilidad de que salga rojo \\(p\\) que podemos calcular a partir de la muestra aleatoria ordenada \\(( R, R, A)\\) 1.3.1 Estimador de momentos Una opción es estimar la probabilidad de que salga rojo, \\(p\\), mediante el conteo de cuántos rojos salieron divididos entre el total de extracciones. En este caso tendríamos el estimador evaluado en la muestra: \\[ \\hat{p}_M = \\frac{2}{3} \\] Aquí una nota bien importante: es imposible (¿por qué?) que en la vida real la probabilidad \\(p\\) de que salga rojo sea \\(2/3\\). El \\(\\hat{p}\\) es un estimador pero que jamás va a coincidir con el valor de verdad.3. Sin embargo este estimador \\(\\hat{p}\\) tiene características interesantes. Para verlas, definamos primero una muestra aleatoria de tamaño \\(3\\) de las pelotas en la urna: \\[ X_{(n)} = \\{ X_1, X_2, X_3\\} \\] donde marcaremos \\(X_i = 1\\) si salió rojo y \\(X_i = 0\\) si salió azul. Preguntarnos por la probabilidad de rojo es lo mismo que preguntarnos por la probabilidad de que \\(X_i = 1\\). El estimador \\(\\hat{p}\\) evaluado en la muestra, la suma ponderada de todos, (los rojos aportan \\(1\\) y los azules nada) está dado por: \\[ \\hat{p}(X_1, X_2, X_3) = \\frac{1}{3}(X_1 + X_2 + X_3) \\] Notamos que para este caso los datos (muestra aleatoria observada) son \\(x_1 = 1\\), \\(x_2 = 1\\) y \\(x_3 = 0\\). Por lo que el estimador observado es: \\[ \\hat{p}(x_1, x_2, x_3) = \\frac{1}{3}(x_1 + x_2 + x_3) = \\frac{2}{3} \\] Notamos que el estimador \\(\\hat{p}\\) es una variable aleatoria que depende de la muestra (aleatoria). Una vez que se tiene la muestra el estimador \\(\\hat{p}\\) colapsa en un único número real definido por la tabla. Pero antes de hacer el experimento (o bien si repetimos el experimento) el \\(\\hat{p}\\) es una variable aleatoria que puede obtener múltiples valores distintos. Como es una variable aleatoria podemos entonces calcular su varianza, por ejemplo, así como su media: \\[ \\begin{aligned} \\mathbb{E}\\Big[ \\hat{p} \\Big] &amp; = \\frac{1}{3}\\mathbb{E}\\big[ X_1 + X_2 + X_3\\big] \\\\ &amp; = \\frac{1}{3}\\big( \\mathbb{E}[ X_1] + \\mathbb{E}[ X_2] + \\mathbb{E}[ X_3]\\big) \\\\ &amp; = \\frac{1}{3} 3p \\\\ &amp; = p \\end{aligned} \\] por lo que si hiciéramos el ejercicio de muestreo múltiples veces los estimadores \\(\\hat{p}\\) que obtuviéramos le atinarían en promedio a \\(p\\). Por otro lado la varianza está dada por: \\[ \\begin{aligned} \\textrm{Var}Big[ \\hat{p} \\Big] &amp; = \\frac{1}{9}\\textrm{Var}\\big[ X_1 + X_2 + X_3\\big] \\\\ &amp; = \\frac{1}{9}\\big( \\textrm{Var}[ X_1] + \\textrm{Var}[ X_2] + \\textrm{Var}[ X_3]\\big) \\\\ &amp; = \\frac{1}{9} 3p (1 - p) \\\\ &amp; = \\frac{1}{3} p (1 - p) \\end{aligned} \\] Podemos calcular más propiedades probabilísticas de \\(\\hat{p}\\) pero el punto importante es que el \\(\\hat{p}\\) tiene su propia distribución. En R podemos simular este proceso de extracción de los \\(\\hat{p}\\): library(ggplot2) # Número de veces que haremos el experimento de extraer 3 pelotas nsim &lt;- 100 tamaño.muestra &lt;- 3 #La verdadera población poblacion &lt;- c(&quot;R&quot;,&quot;R&quot;,&quot;R&quot;,&quot;A&quot;,&quot;A&quot;) #Aquí guardaremos los valores de pgorro pgorro &lt;- rep(NA, nsim) #Repetimos el proceso de muestreo n veces for (i in 1:nsim){ muestra &lt;- sample(poblacion, tamaño.muestra, replace = T) conteo_rojos &lt;- length(which(muestra == &quot;R&quot;)) pgorro[i] &lt;- conteo_rojos/tamaño.muestra } ggplot() + geom_histogram(aes(x = pgorro, y = ..count../100), bins = 10, fill = &quot;purple&quot;, color = &quot;white&quot;) + theme_classic() + labs( x = &quot;Valores de pgorro&quot;, y = &quot;Masa de probabilidad de pgorro&quot;, title = &quot;Función de masa de probabilidad de pgorro&quot;, subtitle = &quot;Aproximación por simulaciones&quot; ) Esto implica que aunque tengamos datos fijos \\(\\{ R,R,R A, A\\}\\) como la extracción de la muestra es aleatoria el valor de \\(\\hat{p}\\) va a variar por el simple proceso de selección aleatoria de la muestra. Darnos cuenta de qué tanto varía nuestro valor a estimar y cómo se aleja (o no) de la verdad va a ser un punto muy importante (en general queremos estimadores \\(\\hat{p}\\) que no se alejen de los valores verdaderos). Ahora, los estimadores \\(\\hat{p}\\) no son únicos y se nos puede ocurrir otro estimador como ejemplo: 1.3.2 Estimador de máxima verosimilitud (maximización discreta) Una segunda opción es buscar bajo qué valor de \\(p\\) (de muchos posibles) son más probables los datos observados. Este criterio se conoce como el criterio de máxima verosimilitud. La idea es ver, bajo cada una de las construcciones posibles de la caja (i.e. \\(\\{ R, R, R, R, R\\}\\), \\(\\{ R, R, R, R, A\\}\\), \\(\\{ R, R, R, A, A\\}\\), \\(\\{ R, R, A, A, A\\}\\), \\(\\{ R, A, A, A, A\\}\\), \\(\\{ A, A, A, A, A\\}\\)) son más probables los datos observados \\(( R, R, A)\\) y elegir esa caja. Vamos a resolver este problema calculando bajo cada escenario de caja las probabilidades de obtener la combinación observada \\(( R, R, A)\\). La pregunta, antes de resolver el problema, es ¿y esto para qué sirve? . Si bien los ejercicios de pelotas de colores y cajas son divertidos, estos por sí mismos no llegan muy lejos. Lo importante es que los ejercicios de urnas son abstracciones de problemas reales. Por ejemplo, el problema de urnas ocurre de manera poblacional cuando nos interesa estimar una proporción. En un país hay personas que padecen diabetes R y sin diabetes A. Se sabe que en el país hay 100 millones de personas. La proporción exacta (dentro del país) es desconocida. Sin embargo podemos hacer una encuesta y obtener una muestra de 100 personas dentro de las cuáles 20 padecieron diabetes y 80 no (\\(\\{20 R, 80 A\\}\\)). De aquí, usando el mismo razonamiento que usaremos con la caja de pelotas podemos buscar la combinación de personas con diabetes y sin diabetes en el país donde la combinación de \\(\\{20 R, 80 A\\}\\) es la más probable. Este mismo razonamiento puede cambiarse de múltiples formas. En una encuesta podemos tener más de dos opciones. Por ejemplo, si interesa determinar la cantidad de personas que votarían por el partido rojo \\(R\\), por el azul \\(A\\) o por el negro \\(B\\) el modelo de pelotas en una caja ahora tiene tres tipos de pelota (y si hay \\(n\\) partidos habría \\(n\\) colores). Puede que los colores estén relacionados entre sí y extraer uno garantice la extracción de otro (por ejemplo si se entrevista gente en casas es muy probable que si se vive un niño en una casa a fuerza viva un adulto en ella mientras que la relación inversa no funciona: que un adulto habite una casa no determina que viva un niño en la misma). Otros cambios posibles son en el mecanismo de selección. Pudiera ser que las pelotas rojas \\(R\\) fueran más grandes que las azules \\(A\\) de tal forma que cuando se extrajeran hubiera mayor probabilidad de tener rojas en la muestra. Esto pasa, por ejemplo, cuando se hacen encuestas de productos. Generalmente sólo aquellas personas que tienen un sentimiento muy fuerte hacia un producto contestan la encuesta. De ahí que haya muchísimas reseñas diciendo que los productos son malísimos o buenísimos y nada intermedio: la gente que reseña algo con 3 estrellas son las pelotas azules que son más difíciles de extraer. Poco a poco veremos otros problemas con pelotas y urnas con sus análogos al mundo real. Por ahora resolvamos el que se especifica más arriba. 1.3.3 Ejemplo 5: Muestreo de urna con dos clases, con orden, con reemplazo Considera una urna con cinco pelotas de colores rojo \\(R\\) y azul \\(A\\). Se desconoce la proporción de pelotas en la urna. Las pelotas son indistinguibles entre sí salvo por el color. Se extrae de manera ordenada primero una bola roja, \\(R\\), y se devuelve a la urna. Luego se extrae una segunda bola roja, \\(R\\), y se devuelve a la urna. Finalmente se extrae una tercera bola y resulta azul: \\(A\\). La combinación ordenada de pelotas extraídas es: \\(( R, R, A)\\). Suponiendo todas las pelotas tienen la misma probabilidad de salir, cuál urna genera con mayor probabilidad los datos observados (y por tanto sería nuestra opción para decidir qué urna es la que tenemos): \\(\\{ R, R, R, R, R\\}\\), \\(\\{ R, R, R, R, A\\}\\), \\(\\{ R, R, R, A, A\\}\\), \\(\\{ R, R, A, A, A\\}\\), \\(\\{ R, A, A, A, A\\}\\) ó \\(\\{ A, A, A, A, A\\}\\). 1.3.3 Solución 5: Muestreo de urna con dos clases, con orden, con reemplazo Hay dos combinaciones de posible urna que podemos descartar desde un inicio: la que sólo tiene rojos \\(\\{ R, R, R, R, R\\}\\) y la que sólo tiene azules \\(\\{ A, A, A, A, A\\}\\). Esto porque los datos observados nos muestran que obtuvimos tantos rojos como azules. En estas dos descartadas la probabilidad de obtener \\(( R, R, A)\\) es cero. Quedan como cajas posibles: la de cuatro rojas \\(\\{ R, R, R, R, A\\}\\), la de tres rojas \\(\\{ R, R, R, A, A\\}\\), la de tres azules \\(\\{ R, R, A, A, A\\}\\), la de una roja \\(\\{ R, A, A, A, A\\}\\). Comenzaré mi análisis con la primera que puse: los otros análisis son similares. Análisis de \\(\\{ R, R, R, R, A\\}\\) Una de las formas más posibles de enlistar todos los escenarios es con un árbol. La forma larga (e impráctica) consiste en enlistar cada una de las formas de extraer las pelotas de la urna como muestra la siguiente imagen: Árbol de decisión para el análisis de \\(\\{ R, R, R, R, A\\}\\) con reemplazo Esta es una forma “segura” de resolver un problema: puede ser largo pero si no se te olvida nada ¡siempre es una posibilidad! Observa que de todas las opciones (combinadas del primer nodo,el segundo y el tercero) se tienen 16 extracciones de la forma \\(( R, R, A)\\) de un total de 125 extracciones (\\(5\\) opciones en el último nodo por \\(5\\) formas de extraer el segundo por \\(5\\) opciones primeras dan 125). La probabilidad entonces de extraer \\(( R, R, A)\\) bajo este esquema es: \\[ \\textrm{Probabilidad de ( R, R, A) dada la urna \\{ R, R, R, R, A\\} con reemplazo} = \\dfrac{16}{125} \\] Análisis de \\(\\{ R, R, R, A, A\\}\\) Vale la pena para este segundo análisis podar un poco el árbol. Podemos darnos cuenta que en el caso anterior todas las ramas rojas son iguales por lo que quizá no vale la pena ponerlas todas. Al calcular la probabilidad de la extracción \\(( R, R, A)\\) dado que la urna es de la forma \\(\\{ R, R, R, A, A\\}\\) trabajaremos más a fondo el árbol. El árbol inicial es como sigue: Árbol de decisión en el caso \\(\\{ R, R, R, A, A\\}\\) con reemplazo En este caso notamos que todos los caminos iniciados por rojo, R, son idénticos lo mismo que los caminos iniciados por azul A por lo que podemos simplificar el árbol copiando sólo las ramas distintas y anotando cada rama a cuántas representa (las azules son \\(2\\) de \\(5\\) mientras que las rojas \\(3\\) de \\(5\\) de ahí los números \\(2/5\\) y \\(3/5\\)). Árbol de decisión simplificado para el análisis de \\(\\{ R, R, R, A, A\\}\\) con reemplazo Para el caso que nos atañe en esta ocasión: obtener primero roja, luego otra roja y finalmente azul, \\(( R, R, A)\\), la única opción es la rama de abajo con probabilidades dadas por: \\[ \\textrm{Probabilidad de ( R, R, A) dada la urna \\{R, R, R, A, A\\} con reemplazo} = \\dfrac{3 \\times 3 \\times 2}{5 \\times 5 \\times 5} = \\dfrac{18}{125} \\] Esta forma reducida es equivalente a la que hubiéramos obtenido haciendo el análisis completo del árbol (por la primera imagen donde hay \\(18\\) opciones de \\(125\\) resultados). El producto de arriba refiere a el número de opciones para primera roja, por el número de opciones para segunda roja dado que la primera fue roja y la última son las opciones para una tercera azul condicional en que las dos primeras fueron rojas. En el denominador sólo multiplicamos los casos totales que son \\(5\\) ramas iniciales que ramifican en \\(5\\) nodos secundarios y \\(5\\) hojas finales. Tómate unos minutos para intentar justificar por qué este conteo de multiplicaciones es equivalente a haber contado todas. Asegúrate de entenderlo bien antes de continuar ¡volveremos a ello más adelante! Análisis de \\(\\{ R, R, A, A, A\\}\\) En este análisis usaremos el mismo truco de un árbol de decisiones reducido que usamos la vez pasada. Lo construiremos paso a paso para mostrar el proceso. De manera inicial tenemos dos opciones: roja (\\(2\\) bolas de \\(5\\)) ó azul (\\(3\\) bolas de \\(5\\)) por lo que a partir de la raíz construimos el árbol con las dos opciones y sus probabilidades Versión 1 del árbol de decisión simplificado para \\(\\{ R, R, A, A, A\\}\\) con reemplazo Dentro de la rama azul de nuevo tenemos la posibilidad de extraer rojas (\\(2\\) de \\(5\\)) o azules (\\(3\\) de \\(5\\)) por lo que se acoplan a la rama: Versión 2 del árbol de decisión simplificado para \\(\\{ R, R, A, A, A\\}\\) con reemplazo La lógica es idéntica si salió roja: hay \\(2/5\\) de probabilidad de extraer una roja o \\(3/5\\) de extraer una azul Versión 3 del árbol de decisión simplificado para \\(\\{ R, R, A, A, A\\}\\) con reemplazo Finalmente las últimas ramas del árbol se agregan de manera idéntica: \\(2/5\\) de rojas y \\(3/5\\) de azul en cada una de las posibles extracciones. Versión final del árbol de decisión simplificado para \\(\\{ R, R, A, A, A\\}\\) con reemplazo Notamos entonces que podemos hacer el cálculo multiplicando (como hicimos la vez pasada) para obtener: \\[ \\textrm{Probabilidad de ( R, R, A) dada la urna \\{R, R, A, A, A\\} con reemplazo} = \\dfrac{2}{5} \\cdot \\dfrac{2}{5} \\cdot \\dfrac{3}{5} = \\dfrac{12}{125} \\] Análisis de \\(\\{ R, A, A, A, A\\}\\) Ya para este último caso los árboles de decisiones te son familiares por lo que puedes verificar que en este caso el árbol es el siguiente: Árbol de decisión simplificado para el análisis de \\(\\{ R, A, A, A, A\\}\\) con reemplazo donde la probabilidad de $( R, R, A) $ dado que la urna es \\(\\{ R, A, A, A, A\\}\\) está dada por: \\[ \\textrm{Probabilidad de ( R, R, A) dada la urna \\{ R, A, A, A, A\\} con reemplazo} = \\dfrac{1}{5} \\cdot \\dfrac{1}{5} \\cdot \\dfrac{4}{5} = \\dfrac{4}{125}. \\] Conclusión Después de que analizamos las probabilidades bajo todas las combinaciones posibles de urna concluimos que la que tiene probabilidad más alta de generar en ese orden rojo, luego rojo y finalmente azul, \\(( R, R, A)\\), es la urna \\(\\{R, R, R, A, A\\}\\) pues si la urna tiene esa combinación de pelotas la probabilidad de extraer \\(( R, R, A)\\) es: \\[ \\textrm{Probabilidad de ( R, R, A) dada la urna \\{R, R, R, A, A\\} con reemplazo} = \\dfrac{18}{125} = 0.144. \\] Siguiendo la idea de que la urna que tenemos es la que tiene mayor probabilidad de generar los datos observados (esto se conoce como criterio de máxima verosimilitud en estadística) entonces estimaríamos que la urna que tenemos es \\(\\{R, R, R, A, A\\}\\) por lo que \\(\\hat{p} = \\frac{3}{5}\\) es el estimador de máxima verosimilitud de \\(p\\). OJO Dadas las observaciones \\(( R, R, A)\\) no podemos determinar a ciencia cierta cuál es la combinación de pelotas en la urna que tenemos. Por lo que siempre puede pasar que la combinación real sea distinta. Aquí el modelo nos dice por cuál optar (de manera lógica, aquella que genera con mayor probabilidad lo que observamos) pero la realidad ¡puede estar por otro lado! Una vez que obtuvimos el estimador, \\(\\hat{p}\\), la segunda pregunta que nos interesará resolver es qué tan buen estimador es \\(\\hat{p}\\). Para ello también buscaríamos describir su distribución probabilística (su masa, su varianza, su valor esperado). Esto lo dejaremos para más adelante. 1.4 Resumen de la sección En esta sección aprendimos que, en el mundo de las ideas, tenemos poblaciones de las cuales obtenemos subconjuntos (muestras). Las muestras observadas corresponden a los datos mientras que las muestras (sin el “observadas”) corresponden al modelo en el mundo de las ideas. Lo que buscamos es estimar los valores que necesitamos para determinar el modelo (parámetros) y esos valores se obtienen a través de estimadores (puede haber varios para el mismo parámetro). Los estimadores viven también en el mundo de los modelos y ahí tienen su distribución de probabilidad, sus masas y densidades, sus valores esperados y sus varianzas. Describir esto nos ayuda a saber cómo se comportan los estimadores y decidir cuáles son los buenos. Profundizaremos más en la siguiente sección donde haremos un ejemplo muy específico: ¿qué pasa si mi población está compuesta de variables aleatorias normales? Jugaremos con ello y veremos por qué es normal usar la normal. Referencias "],["R.html", "Capítulo 2 R 2.1 Programación en R 2.2 Instalando cosas 2.3 Instalación de RStudio 2.4 Primeros pasos en R usando RStudio 2.5 Cálculos numéricos 2.6 Variables 2.7 Observaciones sobre la aritmética de punto flotante 2.8 Leer y almacenar variables en R 2.9 Instalación de paquetes 2.10 Comentarios adicionales sobre el formato 2.11 Loops 2.12 For 2.13 While 2.14 If-else (condicionales) 2.15 And-or (Operadores lógicos) 2.16 Or 2.17 Ejercicio 1 2.18 Media 2.19 Desviación estándar 2.20 Ejercicio 2 2.21 Ejercicio 3 2.22 Advertencias y otras cosas poco intuitivas 2.23 Donde fallan estas cosas 2.24 Números pseudoaleatorios 2.25 Ejercicio 4 2.26 Aleatoreidad en R 2.27 Las semillas 2.28 Ejercicio 5 2.29 Ejercicio 6", " Capítulo 2 R 2.1 Programación en R Figura 2.1: R es un programa chido de estadística. FIN. Una de las primeras cosas que necesitamos saber es que R (por más que sus más ávidos defensores digan lo contrario) no es para todo. Si tú ya conoces otro lenguaje (sea Stata, Excel, SAS, Python, Matlab, Julia, etc) sabrás utilizar muchas de sus opciones. Estoy seguro que, de conocer uno de estos, te será muchísimo más fácil seguir sacando promedios en tu lenguaje favorito que en R, realizar regresiones lineales es probablemente más sencillo en Stata mientras que las gráficas de barras para mí son más simples en Excel, Python excede en aplicaciones de inteligencia artificial mientras que Matlab es más veloz que R, Julia tiene muchas cosas de ecuaciones diferenciales que nadie más. Lo que probablemente no sea más sencillo de hacer en otro lenguaje es realizar análisis estadístico, gráficas de todo tipo y modelos de simulación. Para eso, R es, indiscutiblemente, una de las mejores opciones para quienes no conocen de programación4. Finalmente, uno de los consejos más importantes que te puedo dar es que este curso no te va a servir si no practicas. Igual que como pasa con los idiomas uno no aprende R en una semana sin practicarlo después. Mi sugerencia es que, a la vez que sigues estas notas comiences a trabajar un proyecto tuyo específico junto con el buscador de Internet de tu preferencia a la mano y empieces a usar R en él. Practica5. 2.1.1 Puntos a favor de R Todo el mundo lo usa. Quizá éste es el punto más a favor. Si mucha gente lo conoce y lo utiliza, hay más opciones de ayuda. Los sitios de StackOverflow en inglés y en español son excelentes para pedir apoyo en R; los grupos de usuarios de Google son otra fuente muy buena. Entre más gente usa el programa; es más fácil obtener ayuda porque seguro alguien más tuvo hace ya tiempo el mismo problema que tú. Todas las personas que trabajan en estadística publican sus métodos y su código en R (eso, claro, cuando publican sus métodos). Es raro encontrar un nuevo método estadístico en el mundo y que no se pueda usar, de alguna forma, en R. Dentro de los lenguajes de programación R es de los más sencillos. Quienes lo hicieron realmente se preocuparon por su público (de no especialistas) y en general desarrollan para él. R es gratis. Y en esta época de austeridad, cualquier ahorro es bueno. Que sea gratis no significa que no esté respaldado: existen versiones de R respaldadas por grandes compañías como Microsoft Todo lo que se hace en R es público. R no tiene métodos secretos ni es una caja negra. Todo lo que hace cada una de las funciones de R, cualquiera lo puede revisar, por completo. En R puedes hacer libros o notas ¡como este! donde guardes todo tu trabajo, reportes automatizados e incluso documentos interactivos para facilitar el análisis de datos. R puede hacer gráficas bonitas: Por supuesto, no todo es miel sobre hojuelas con R. Particularmente, algunos de los problemas con el lenguaje: Figura 2.2: La curva de aprendizaje de R es más empinada pero después de un rato vale la pena La curva de aprendizaje es mucho más empinada que para otros programas estadísticos (como Stata, SAS o SPSS) ¡particularmente si es tu primera vez programando! La mayor parte de las personas que trabajan en R no son programadores de verdad. Gran parte del código que te puedes encontrar en el mundo real está escrito con prisa para salir del aprieto sin mucha planeación, con pocos comentarios, falta de control de versiones y pocas herramientas de revisión. ¡Internet está lleno de creaturas espantosas escritas en R! Figura 2.3: R puede ser muy lento pero eso te da oportunidad de hacer otras cosas ;) . R de ninguna manera es veloz por lo que algunos programas (lo veremos en simulación) pueden ser extremada (y dolorosamente) lentos. 2.1.2 Bienvenidx a R, Bunny-Wunnies Freak Out (sí, así se llama esta versión) R es un lenguaje de cómputo y un programa estadístico libre, gratuito, de programación funcional (¿qué es eso?), orientado a objetos (what??) que mutó a partir de otros dos lenguajes conocidos como Scheme y S6. El primero de estos fue desarrollado en el MIT por Sussman y Steele mientras que el segundo surgió en los laboratorios Bell7 creado por Becker, Wilks y Chambers. R nació en junio de 1995 a partir del trabajo de Ross Ihaka y Robert Gentleman8. Desde su creación, la mayor parte del desarrollo de R ha sido trabajo completamente voluntario de la Fundación R, del equipo de R Core y de miles de usuarios que han creado funciones específicas para R conocidas como paquetes (packages). Actualmente el repositorio más importante de R, CRAN, contiene más de 16000 paquetes con distintas funciones para hacer ¡lo que quieras! Como todo el trabajo en R es voluntario hace falta: Una homologación en los métodos. Puedes encontrar varias funciones que supuestamente hacen exactamente lo mismo (como es el caso de emojifont, fontemoji y emoGG para graficar usando emojis). Estandarizar la notación. Algunos paquetes como aquellos del tidyverse (veremos más adeltna) utilizan pipes (%&gt;%); estos sólo funcionan en el tidyverse pero no fuera del mismo. Sin embargo, también es una gran ventaja que sean los usuarios de R quienes guían su desarrollo. El lenguaje va mutando según peticiones de las personas que lo usan. Si hay algo que te gustaría R tuviera y aún no existe ¡lo puedes proponer! 2.2 Instalando cosas 2.2.1 Instalación de R Figura 2.4: Oficialmente, la página de R es de las páginas más feas del mundo. ¡No te dejes llevar por las apariencias! A lo largo de estas notas estaré trabajando con: R version 4.0.3 (2020-10-10) Bunny-Wunnies Freak Out. La más reciente versión de R la puedes encontrar en CRAN. Para ello ve al sitio y selecciona tu plataforma. Nota usuarios de Mac En algunas Mac, al abir R, aparece el siguiente mensaje de advertencia: During startup - Warning messages: 1: Setting LC_CTYPE failed [...] para solucionarlo ve a Aplicaciones y abre Terminal. Copia y pega en ella el siguiente texto: defaults write org.R-project.R force.LANG en_US.UTF-8 Da enter, cierra la Terminal y reinicia R. En el caso de Windows da clic en Download R for Windows y luego en install R for the first time. Finalmente, ejecuta el instalable que aparece al dar click en Download R 4.0.3 for Windows . Para este curso pudiera ser que requirieras las herramientas de desarrollador Rtools. En el caso de Mac selecciona Download R for (Mac) OS X y luego elige R-4.0.3.pkg. En Mac puede que necesites instalar adicionalmente XQuartz (según tu versión de Mac). Si tu Mac es una versión suficientemente antigua, sigue las instrucciones específicas de CRAN. En el caso de Linux al elegir Download R for Linux tendrás la opción de buscar tu distribución específica. Al elegirla, aparecerán instrucciones para tu terminal de comandos; síguelas. En el caso de Linux, según los paquetes de R que elijamos instalar en la computadora requerirás instalar paquetería adicional para tu distribución de Linux. R te informará de la paquetería necesaria conforme la requiera. Si tienes problemas para instalar puedes usar RStudio Cloud. 2.3 Instalación de RStudio Figura 2.5: RStudio es una empresa que se dedica a hacer cosas para R. RStudio es una interfaz gráfica (IDE) para R. Puedes pensar a R como el Bloc de Notas y a RStudio como Word. El Bloc tiene todas las capacidades que necesitas para poder escribir; empero, es muchísimo mejor trabajar tus papers en Word. De la misma manera, R tiene todas las capacidades para hacer estadística pero un formato horrible y RStudio se ha convertido en la más popular forma de usar R. Por supuesto que no es la única; algunas alternativas son Atom con ide-r, Eclipse con StatET y RKWard. En general es posible seguir estas notas sin que tengas RStudio pero, si es tu primera vez programando, no lo recomiendo. Si ya tienes experiencia con lenguajes como Python, Javascript, Java ó alguno de los mil C que existen, no tendrás ningún problema usando el editor de tu preferencia. Para descargar RStudio ve a su página y da clic en Download RStudio. Baja tu pantalla hasta donde dice Installers for Supported Platforms y elige tu plataforma: Windows, Mac OS X ó tu sabor de Linux preferido. Una vez descargado el archivo, ábrelo y sigue las instrucciones que aparecen en pantalla. 2.4 Primeros pasos en R usando RStudio Una vez hayas instalado R y RStudio, abre RStudio9. Te enfrentarás a una pantalla similar a esta: Figura 2.6: La primera vez que abres RStudio Si tu RStudio tiene sólo 3 páneles, como en mi caso, ve a la esquina superior izquierda (signo de hoja+) y elige un nuevo R Script Figura 2.7: Elige hoja+ para crear un nuevo archivo Tendrás, entonces, 4 páneles como se ve a continuación: Figura 2.8: RStudio &lt;3 El primer panel (esquina inferior izquierda) es la Consola. Aquí es donde se ejecutan las acciones. Prueba escribir 2 + 3 en él y presiona enter. Aparece el resultado de la suma. Definitivamente, R es la calculadora que más trabajo cuesta instalar. Figura 2.9: La consola de R es la calculadora más difícil de instalar que existe. El segundo panel (esquina superior izquierda) es el panel con el Script. Aquí se escribe el programa pero no se ejecuta. Prueba escribir 10 + 9. ¿Ves que no pasa nada? Lo que acabas de hacer es crear un programa que, cuando se ejecute, hará la suma de 10 + 9. ¡Qué programa más aburrido! Sin embargo, no todo está perdido: presiona CTRL+Enter (Cmd+Enter en Mac) al final de la línea o bien da clic en Run y verás que, en la consola, aparece la instrucción y el resultado de la misma. El Script es una excelente fuente para tener un historial de lo que estás haciendo. Figura 2.10: El Script sirve para salvar las instrucciones en el orden en que las vas a ejecutar. El tercer panel contiene el ambiente. Aquí aparecerán las variables que vayamos creando. Por ahora, para poner un ejemplo, importaremos el archivo Example1.csv (con valores simulados) disponible en Github dando clic en Import Dataset y From Text (base). Selecciona el archivo y elige las opciones en la ventana de previsualización que hagan que se vea bien. Nota que una vez realizada la importación aparece en el panel derecho Example1. Al dar clic podrás ver la base de datos. Las bases de datos y variables que utilices durante tus análisis aparecerán en esa sección. Figura 2.11: El Ambiente muestra las variables (incluyendo bases de datos) que estás utilizando en este momento. A diferencia de otros programas estadísticos (o sea Stata) en R es posible tener múltiples bases de datos abiertas a la vez. Para entender mejor lo que ocurre en el último de los páneles, lo mejor es trabajar con nuestra base. Escribe en la consola plot(Example1) . En el cuarto pánel aparecerá una gráfica. El cuarto de los páneles para nosotros tendrá esa utilidad: mostrará las gráficas que hagamos así como la ayuda. Para ver la ayuda para las instrucciones de R puedes escribir ?. Prueba teclear ?plot en la consola. El signo de interrogación es un help() que muestra las instrucciones para usar una función. Figura 2.12: La gráfica que aparece de hacer un plot de la base de datos de ejemplo. Figura 2.13: El cuarto panel muestra respectivamente las gráficas y la ayuda. Mi sugerencia personal es que escribas todo lo que haces en el Script y que sólo utilices la consola para verificar valores. De esta manera podrás almacenar todas las instrucciones ejecutadas y volver a ellas cuando se requieran. Por último te sugiero utilizar # gatos para comentar tu código. Así, el código anterior lo podrías ver en la consola como: #Aquí pruebo cómo R hace las sumas 10 + 9 Comenta. Comenta. Comenta, por favor. Tu ser del futuro que regrese a sus archivos de R un mes después de haberlos hecho te lo agradecerá (y tu profe también). Finalmente y como aclaración para estas notas, el código de R aparece como: #Esto es código de R 7 - 2 Mientras que los resultados de evaluar en R se ven con #: ## [1] 5 Así, la evaluación con su resultado se ve de la siguiente forma: #Esto es código de R 7 - 2 ## [1] 5 2.5 Cálculos numéricos R sirve como calculadora para las operaciones usuales. En él puedes hacer sumas, #Esto es una suma en R 12 + 31 ## [1] 43 Figura 2.14: Ada Lovelace (1815-1852), la primera en diseñar un algoritmo computacional ¡y sin tener computadoras! restas, #Esto es una resta en R 3 - 4 ## [1] -1 multiplicaciones, #Esto es una multiplicación en R 7*8 ## [1] 56 divisiones, #Esto es una división en R 4/2 ## [1] 2 sacar logaritmos naturales \\(\\ln\\), #Para sacar logaritmo usas el comando log log(100) ## [1] 4.60517 o bien logaritmos en cualquier base,10 #Puedes especificar la base del logaritmo con base log(100, base = 10) ## [1] 2 también puedes elevar a una potencia (por ejemplo hacer \\(6^3\\)), #Así se calculan potencias 6^3 ## [1] 216 calcular la exponencial \\(e\\), #Para exponenciales puedes usar exp exp(1) ## [1] 2.718282 o bien exponenciar cualquier variable \\(e^{-3}\\), #O bien exponenciales específicas, e^-3 exp(-3) ## [1] 0.04978707 también puedes usar el número \\(\\pi\\). #Cálculo de pi pi ## [1] 3.141593 No olvides que R usa el orden de las operaciones de matemáticas. Siempre es de izquierda a derecha con las siguientes excepciones: Primero se evalúa lo que está entre paréntesis. En segundo lugar se calculan potencias. Lo tercero en evaluarse son multiplicaciones y divisiones. Finalmente, se realizan sumas y restas. Por ejemplo, en la siguiente ecuación \\[ 2 - 2 \\cdot \\frac{(3^4 - 9)}{(5 + 4)} \\] se resuelven primero los paréntesis \\((3^4 - 9) = 81 - 9 = 72\\) y \\((5 + 4) = 9\\); luego se resuelve la división: \\(\\frac{72}{9}=8\\), se multiplica por el \\(2\\): \\(2 \\cdot 8 = 16\\) y finalmente se hace la resta: \\(2-16 = -14\\). 2.5.1 Ejercicio Determina, sin evaluar, los resultados de los siguientes segmentos de código: #Primer ejercicio (9 - 3)^2 * (2 - 1) - 6 #Segundo ejercicio 6 * 2 / (7 - 3) * 5 #Tercer ejercicio 2 * 3 ^ 2 * 2 / (5 - 4) * 1 / 10 Evalúa para comprobar tu respuesta. 2.5.2 Ejercicio Calcula el área y el perímetro de un círculo de radio 5. Recuerda que la fórmula del área es \\(\\pi \\cdot r^2\\) donde \\(r\\) es el radio; mientras que la del perímetro es: \\(\\pi \\cdot d\\) donde \\(d\\) es el díametro (= dos veces el radio). 2.5.3 Respuestas ## Área = 78.5398163397448 ## Perímetro = 31.4159265358979 2.6 Variables R es un programa orientado a objetos; esto quiere decir que R almacena la información en un conjunto de variables que pueden tener diferentes clases y opera con ellos según su clase. Por ejemplo, un conjunto de caracteres, entre comillas, es un Character (R lo piensa como texto) #Un conjunto de caracteres es un char &quot;Hola&quot; ## [1] &quot;Hola&quot; Un número (por ejemplo 2 tiene clase numeric)11. Hay que tener mucho cuidado con combinar floats con Strings: #Código que sí funciona porque ambos son números 2 + 4 ## [1] 6 Figura 2.15: El algoritmo diseñado por Ada Lovelace. #Código que no funciona porque uno es caracter 2 + &quot;4&quot; ## Error in 2 + &quot;4&quot;: non-numeric argument to binary operator Si lo piensas, este último error ¡tiene todo el sentido! no puedes sumar un número a un texto. ¿O qué significaría 'Felices' * 4 ? La magia de R comienza con que puedes almacenar valores en variables. Por ejemplo, podemos asignar un valor a una variable: #Asignamos x = 10 x &lt;- 10 Hay dos formas de asignar valores, una es con la flecha de asignación \\(\\leftarrow\\) y otra con el signo de igual: #Podemos asignar valores con el signo de = y = 6 Nota que, cuando realizamos operaciones, la asignación es la última que se realiza: #Aquí z = 106 z &lt;- y + x^2 Los valores que fueron asignados en las variables, R los recuerda y es posible calcular con ellos: #Podemos realizar una suma x + y ## [1] 16 #O bien podemos realizar una multiplicación 3*y - x ## [1] 8 Podemos preguntarnos por el valor de las variables numéricas mediante los operadores == (sí, son dos iguales), != (que es un \\(\\neq\\)) &gt;, &gt;=, &lt;= y &lt;: #Podemos preguntarnos si x vale 4 x == 4 ## [1] FALSE El operador de asignación también se puede utilizar al revés \\(2 \\rightarrow x\\) pero no lo hagas, por favor. Nota que no estamos asignando el valor de x: x ## [1] 10 Podemos preguntarnos por diferencia: x != 4 ## [1] TRUE Así como por mayores, menores incluyendo posibles igualdades (i.e. los casos \\(\\geq\\) y \\(\\leq\\)) #Nos preguntamos si x &gt; y x &gt; y ## [1] TRUE #Nos preguntamos si x &gt;= 10 x &gt;= 10 ## [1] TRUE #Nos preguntamos si y &lt; 6 y &lt; 6 ## [1] FALSE #O bien si y &lt;= 6 y &lt;= 6 ## [1] TRUE En todos los casos los resultados han sido TRUE ó FALSE. La clase de variables que toma valores TRUE ó FALSE se conoce como booleana. Hay que tener mucho cuidado con ellas porque, puedes acabar con resultados muy extraños: #MALAS PRÁCTICAS, NO HAGAS ESTO #Cuando lo usas como número TRUE vale 1 100 + TRUE ## [1] 101 #MALAS PRÁCTICAS, NO HAGAS ESTO #Cuando lo usas como número FALSE vale 0 6*FALSE ## [1] 0 Aquí puedes encontrar una lista de malas prácticas en computación a evitar. Finalmente, nota que es posible reescribir una variable y cambiar su valor: #Aquí x vale 10, como antes x ## [1] 10 #Aquí cambianos el valor de x y valdrá 0.5 x &lt;- 0.5 x ## [1] 0.5 2.6.1 Ejercicios Determina el valor que imprime R en cada caso, sin que corras los siguientes pedazos de código. Después, verifica tu respuesta con R: #Primer ejercicio x &lt;- 100 y &lt;- 3 x &gt; y #Segundo ejercicio z &lt;- (4 - 2)^3 z &lt;- z + z + z z #Tercer ejercicio x &lt;- 3 y &lt;- 2 z &lt;- x * y x &lt;- 5 y &lt;- 10 z #Cuarto ejercicio variable1 &lt;- 1000 variable2 &lt;- 100 variable3 &lt;- variable1/variable2 &lt;= 10 variable3 #Quinto ejercicio &quot;2&quot; - 2 #Sexto ejercicio (0.1 + 0.1 + 0.1) == 0.3 2.6.2 NIVEL 3 Determina, sin correr el programa, qué regresa la consola en este caso x &lt;- 2 x &lt;- 5 + x -&gt; y -&gt; x x &lt;- x^2 x Comprueba con la consola tus resultados; puede que encuentres respuestas poco intuitivas. 2.7 Observaciones sobre la aritmética de punto flotante Si hiciste el penúltimo ejercicio (el cual, obviamente hiciste y comprobaste con la consola) podrás haber notado una trampa. Analicemos qué ocurre; quizá hicimos mal la suma #Veamos si este lado está mal (0.1 + 0.1 + 0.1) ## [1] 0.3 #O si éste es el que tiene la trampa 0.3 ## [1] 0.3 Aparentemente no hay nada malo ¿qué rayos le pasa a R? La respuesta está en la aritmética de punto flotante. Podemos pedirle a R que nos muestre los primeros 100 dígitos de la suma 0.1 + 0.1 + 0.1: Figura 2.16: Réplica de la Z3, la primer computadora con punto flotante (1941). #Veamos qué pasa con la suma options(digits = 22) #Cambiamos dígitos (0.1 + 0.1 + 0.1) #Sumamos ## [1] 0.3000000000000000444089 El comando options(digits = 22) especifica que R debe imprimir en la consola 22 dígitos. No más. ¡Ahí está el detalle! R no sabe sumar. En general, ningún programa de computadora sabe hacerlo. Veamos otros ejemplos: 4.1 - 0.1 #Debería dar 4 ## [1] 3.999999999999999555911 3/10 #Debería ser 0.3 ## [1] 0.2999999999999999888978 log(10^(12345), base = 10) #Debería dar 12345 ## [1] Inf El problema está en cómo las computadoras representan los números. Ellas escriben los números en binario. Por ejemplo, 230 lo representan como 11100110 mientras que el 7 es: 111. El problema de las computadoras radica en que éstas tienen una memoria finita por lo que números muy grandes como: \\(124765731467098372654176\\) la computadora hace lo mejor por representarlos eligiendo el más cercano: #Nota la diferencia entre lo que le decimos a R #y lo que resulta x &lt;- 124765731467098372654176 x ## [1] 124765731467098377420800 Un error de punto flotante en la vida real ocasionó en los años noventa, la explosión del cohete Ariane 5. Moraleja: hay que tener cuidado y respeto al punto flotante. No olvides cambiar la cantidad de dígitos que deseas que imprima R en su consola de vuelta: options(digits = 6) #Cambiamos dígitos El mismo problema ocurre con números decimales cuya representación binaria es periódica; por ejemplo el \\(\\frac{1}{10}\\) en binario se representa como \\(0.0001100110011\\overline{0011}\\dots\\). Como es el cuento de nunca acabar con dicho número, R lo trunca y almacena sólo los primeros dígitos de ahí que, cada vez que escribes 0.1, R en realidad almacene el 0.1000000000000000055511 que es casi lo mismo pero no es estrictamente igual. Hay que tener mucho cuidado con esta inexactitud de las computadoras (inexactitud estudiada por la rama de Análisis Numérico) pues puede generar varios resultados imprevistos. 2.7.1 ¿Cómo checar un if? En general lo que hacen las computadoras para comparar valores es que verifican que, en valor absoluto, el error sea pequeño. Recuerda que el valor absoluto de \\(x\\), \\(|x|\\), regresa siempre el positivo: \\[ |4| = 4 \\qquad \\textrm{y} \\qquad |-8| = 8 \\] Para verificar que algo es más o menos \\(0.3\\) suele usarse el valor absoluto12 de la siguiente manera: abs( (0.1 + 0.1 + 0.1) - 0.3 ) &lt; 1.e-6 ## [1] TRUE donde 1.e-6 es notación corta para 0.000001 (también escrito como \\(1\\times 10^{-6}\\)). La pregunta que nos estamos haciendo es que si el error entre sumar \\(0.1+0.1+0.1\\) y \\(0.3\\) es muy pequeño \\(&lt; 0.000001\\): \\[ | (0.1 + 0.1 + 0.1) - 0.3 | &lt; 0.000001 \\] 2.8 Leer y almacenar variables en R Para terminar esta sección, aprenderemos cómo guardar variables en R. Para eso, el concepto de directorio es uno de los más relevantes. En general, en computación, el directorio se refiere a la dirección en tu computadora donde estás trabajando. Por ejemplo, si estás en una carpeta en tu escritorio de nombre “Ejercicios_R” probablemente tu directorio sea ‘~/Desktop/Ejercicios_R/’ (en Mac) o bien ‘~\\Desktop\\Ejercicios_R\\’ en Windows13. La forma de saber tu directorio (en general) es ir a la carpeta que te interesa y con clic derecho ver propiedades (o escribir ls en la terminal Unix). R tiene un directorio default que quién sabe dónde está (depende de tu instalación, generalmente está donde tu Usuario). Usualmente lo mejor es elegir un directorio para cada uno de los proyectos que hagas. Para ello si estás en RStudio puedes utilizar Shift+Ctrl+H (Shift+Cmd+H en Mac) o bien ir a Session &gt; Set Working Directory &gt; Choose Directory y elegir el directorio donde deseas trabajar tu proyecto. Pensando que elegiste el escritorio (Desktop en mi computadora) notarás que en la consola aparece el comando setwd(\"~/Desktop\") (o bien con ‘\\’ si eres Windows). Mi sugerencia es que copies ese comando en tu Script para que, la próxima vez que lo corras ya tengas preestablecido el directorio. #Si eres Mac/Linux setwd(&quot;~/Desktop&quot;) #Si eres Windows setwd(&quot;C:\\Users\\Rodrigo\\Desktop&quot;) #Rodrigo = Mi usuario Podemos verificar el directorio elegido con getwd(): getwd() En general es buena práctica en R establecer, hasta arriba del Script, el comando de directorio. Esto con el propósito de que, cuando compartas un archivo, la persona a quien le fue compartido el archivo pueda rápidamente elegir su propio directorio en su computadora. Probemos guardar unas variables en un archivo dentro de nuestro directorio. Para ello utilizaremos el comando save. #Crear las variables x &lt;- 200 y &lt;- 100 #Los archivos de variables de R son rda save(x,y, file = &quot;MisVariables.rda&quot;) Si vas a tu directorio, notarás que el archivo MisVariables.rda acaba de ser creado. De esta forma R puede almacenar objetos creados en R que sólo R puede leer (más adelante veremos cómo exportar bases de datos y gráficas). Observa que en tu ambiente (si estás en RStudio puedes verlas en el panel 3) deben aparecer las variables que hemos usado hasta ahora: ## [1] &quot;x&quot; &quot;y&quot; &quot;z&quot; &quot;tamaño.muestra&quot; ## [5] &quot;Example1&quot; &quot;muestra&quot; &quot;pgorro&quot; &quot;i&quot; ## [9] &quot;nsim&quot; &quot;poblacion&quot; &quot;conteo_rojos&quot; Podemos probar sumar nuestras variables y todo funciona súper: x + y #Funciona magníficamente ## [1] 300 Limpiemos el ambiente. El comando equivalente al clear all en R es un poco más complicado de memorizar: #EL clear all de R rm(list = ls()) Ahora, si vuelves a ver el ambiente, éste estará vacío: ¡hemos limpiado el historial! Nota que si intentamos operar con las variables, R ya no las recuerda: x + y #Error ## Error in eval(expr, envir, enclos): object &#39;x&#39; not found Así como hay que lavarse las manos antes de comer, es buen hábito limpiar todas las variables del ambiente de R antes de usarlo. Podemos leer la base de datos usando load: #Leemos las variables load(&quot;MisVariables.rda&quot;) #Una vez leídas podemos empezar a jugar con ellas x + y #Ya funciona ## [1] 300 Por último, es necesario resaltar la importancia del directorio. Para ello crea una nueva carpeta en tu escritorio de nombre Mi_curso_de_R. Mueve el archivo \"MisVariables.rda\" dentro de la carpeta. Borra todo e intenta leer de nuevo el archivo: #Borramos todo rm(list = ls()) #Intentamos leer el archivo de nuevo load(&quot;MisVariables.rda&quot;) Este error es porque R sigue pensando que nuestro directorio es el escritorio y está buscando el archivo ahí sin hallarlo. Para encontrarlo hay que cambiar el directorio a través de RStudio (ya sea Ctrl+Shift+H o Session &gt;Set Working Directory &gt; Choose Directory) o bien a través de comandos en R: #Si eres Mac/Linux setwd(&quot;~/Desktop/Mi_curso_de_R&quot;) #Si eres Windows setwd(&quot;C:\\Users\\Rodrigo\\Desktop\\Mi_curso_de_R&quot;) #Rodrigo = Mi usuario #Aquí sí se puede leer load(&quot;MisVariables.rda&quot;) 2.8.1 Ejercicio Responde a las siguientes preguntas: ¿Qué es el directorio y por qué es necesario establecerlo? Si R me da el error 'No such file or directory' ¿qué hice mal? En RStudio, ¿qué hace Session &gt; Restart R? ¿cuál es la diferencia con rm(list = ls())? ¿Qué hace el comando cat(\"\\014\")? (Ojo puede que no haga nada). Si funciona, ¿cuál es la diferencia con rm(list = ls()) y con Restart R? 2.9 Instalación de paquetes Un paquete de R es un conjunto de funciones adicionales elaboradas por los usuarios, las cuales permiten hacer cosas adicionales en R. Para instalarlos requieres de una conexión a Internet (o bien puedes instalarlos a partir de un archivo, por ejemplo, mediante una USB). El comando de instalación es install.packages seguido del nombre del paquete. Por ejemplo (y por ocio) descarguemos el paquete beepr para hacer reproducir sonidos en la computadora14. Para ello: install.packages(&quot;beepr&quot;) [...] * DONE (beepr) The downloaded source packages are in ‘/algun/lugar/downloaded_packages’ Esto significa que el paquete ha sido instalado. Nos interesa usar la función beep que emite un sonido (??beep para ver la ayuda). Si la llamamos así tal cual, nos da error: beep(3) R es incapaz de hallar la función porque aún no le hemos dicho dónde se encuentra. Para ello podemos llamar al paquete mediante la función library y decirle a R que incluya las funciones que se encuentran dentro de beepr: library(beepr) beep(3) #Esto produce un sonido El comando library le dice a R ¡hey, voy a usar unas funciones que creó alguien más y que están dentro del paquete beepr! De esta manera, al correr beep(3), R ya sabe dónde hallar la función y por eso no arroja error. 2.9.1 Ejercicios NIVEL 1 Instala los paquetes tidyverse en R. De tidyverse haz lo necesario para que el siguiente bloque de código te arroje una gráfica: #Aquí tienes que hacer algo # # RELLENA AQUÍ # #Esto genera un histograma set.seed(1364752) mis.datos &lt;- data.frame(x = rnorm(1000)) ggplot(mis.datos, aes(x = x)) + geom_histogram(bins = 50, fill = &quot;deepskyblue3&quot;) + ggtitle(&quot;Histograma generado por el código&quot;) NIVEL 3 Instala el paquete devtools (para hacerlo probablemente necesites instalar más cosas en tu computadora; averigua cuáles) Usa devtools para instalar el paquete emoGG desde Github. Verifica que tu instalación fue correcta haciendo la siguiente gráfica: library(emoGG) ggplot(mtcars, aes(wt, mpg))+ geom_emoji(emoji=&quot;1f697&quot;) 2.10 Comentarios adicionales sobre el formato Así como en el español existen reglas de gramática para ponernos todos de acuerdo y entendernos entre todos, en R también existen sugerencias a seguir para escribir tu código. Las sugerencias que aquí aparecen fueron adaptadas de las que utiliza el equipo de Google. No escribas líneas de más de 80 caracteres (si se salió de tu pantalla, mejor continúa en el siguiente renglón). Coloca espacios entre operadores +,*,/,-,&lt;-,=, &lt;, &lt;=, &gt;, &gt;=, == y usa paréntesis para agrupar: #Esto no se ve muy bien abs(3*5/(4-9)^2-60/100-888+0.1*8888-4/10*2) &lt; 1.e-6 #Los espacios permiten distinguir el orden de las operaciones abs( (3 * 5) / (4 - 9)^2 - 60 / 100 - 888 + (0.1 * 8888) - (4 / 10) * 2 ) &lt; 1.e-6 Intenta alinear la asignación de variables para legibilidad: #Esto no tanto altura &lt;- 1.80 peso &lt;- 80 edad &lt;- 32 #Esto se ve bien altura &lt;- 1.80 peso &lt;- 80 edad &lt;- 32 Utiliza nombres que evoquen la variable que representas #Cuando regreses a esto no sabrás ni qué x &lt;- 10 y &lt;- 2 z &lt;- 3.14 W &lt;- z * x^y #¿Qué calculé? #Es mejor especificar la variable radio &lt;- 10 potencia &lt;- 2 pi_aprox &lt;- 3.14 area_circulo &lt;- pi_aprox * radio^potencia No utilices un nombre demasiado similar para cosas diferentes. #Aquí, seguro eventualmente te vas a equivocar altura &lt;- 10 #Altura del edificio Altura &lt;- 1.8 #Mi altura ALTURA &lt;- 2000 #La altitud de la CDMX #Siempre elegir nombres claros, aunque largos altura.edificio &lt;- 10 #Altura del edificio altura.Rodrigo &lt;- 1.8 #Mi altura altura.CDMX &lt;- 2000 #La altitud de la CDMX Comenta: #¿Qué hace esto? x &lt;- 168 x &lt;- x/100 y &lt;- 71.2 print(y/x^2) #Es mejor así altura &lt;- 168 #en centímetros altura &lt;- altura/100 #en metros peso &lt;- 71.2 #peso en kg print(peso/altura^2) #índice masa corporal Figura 2.17: Trad: Un periodista se acerca a un programador a preguntarle ¿qué hace que un código sea malo? -Sin comentarios. Siempre pon las llamadas a los paquetes y el directorio al inicio de tu archivo para que otro usuario sepa qué necesita. Código limpio y legible: #Asumiendo aquí inicia el archivo: setwd(&quot;Mi directorio&quot;) #Llamamos la librería library(beepr) library(tidyverse) #Analizamos una base de datos de R data(iris) #Base de datos de flores #Agrupamos la base por especie iris.agrupada &lt;- group_by(iris, Species) #Obtenemos la media por longitud de sépalo iris.media &lt;- summarise(iris.agrupada, SL.mean = mean(Sepal.Length)) #Avisa que ya terminó beep(5) es siempre preferible a código escrito con prisas : Figura 2.18: Yo, leyendo mi código no comentado y con mala edición 6 meses después de haberlo hecho. data(iris);setwd(&quot;Mi directorio&quot;) library(tidyverse);x&lt;-group_by(iris,Species ) #Aquí hacemos esto iris.means=summarise( x,SL.mean=mean(Sepal.Length));library(beepr);beep(5)#FIN Siempre escribe tu código pensando que alguien más (y ese alguien más puedes ser tú) va a leerlo. ¡No olvides comentar! 2.11 Loops Vamos a analizar los ciclos (se encuentran en el manual de R por si gustas). 2.12 For Al for lo alimentas con una lista de elementos y él (o ella) realizan la operación indicada con todos los elementos de la lista. Es decir el for recorre de uno por uno los elementos de una lista y les aplica una instrucción. La estructura es como sigue: \\[ \\begin{equation} \\textrm{ for } \\Big( \\overbrace{i}^\\text{Nombre de tu variable} \\textrm{ in } \\underbrace{1:10}_\\text{Lista de elementos}\\Big) \\overbrace{ \\left \\{\\textrm{Házle algo a $i$} \\right \\} }^\\text{Instrucción que aplicarle a cada elemento} \\end{equation} \\] Por ejemplo una función que imprima los cuadrados de los primeros 10 números: for (i in 1:10){ print(i^2) } ## [1] 1 ## [1] 4 ## [1] 9 ## [1] 16 ## [1] 25 ## [1] 36 ## [1] 49 ## [1] 64 ## [1] 81 ## [1] 100 2.13 While El while es una instrucción peligrosa. Un while realiza una instrucción indefinidamente mientras se cumpla una condición. La estructura es como sigue: \\[ \\begin{equation} \\textrm{ while } \\overbrace{\\Big( \\textrm{Condición fabulosa} \\Big)}^\\text{Cosas que tienen que cumplirse para seguir operando} \\underbrace{ \\left \\{ \\textrm{Cosas por hacer} \\right \\}}_\\text{Instrucciones} \\end{equation} \\] Por ejemplo, mientras nuestro número \\(i\\) sea \\(\\leq 5\\) le sumamos \\(1\\). Por ejemplo, mientras nuestro número \\(i\\) sea \\(\\leq 5\\) le sumamos \\(1\\). i=1 while (i&lt;=5){ print(i) i=i+1 } Es muy importante que pusiéramos \\(i = i + 1\\) ya que esto obliga a que cada vez que da una vuelta la computadora sume el valor de \\(i\\). Sin ésta instrucción la \\(i\\) valdría siempre 1 y jamás saldríamos del loop (jamás llegaría a valer 5). Si alguna vez quedas atrapado en un loop puedes usar el botón de STOP que tiene R junto a la consola o la tecla escape del teclado. 2.14 If-else (condicionales) En el manual podemos encontrar la sección de condicionales. Los condicionales evalúan el camino que debe de seguir el código según una condición. Los condicionales tienen la siguiente estructura: \\[ \\begin{align*} if (Condición) \\left \\{ \\right. \\\\ \\\\ \\text{Cosas por hacer si ocurre la condición} \\\\ \\\\ \\left. \\right \\} else \\left \\{ \\right. \\\\ \\\\ \\text{Cosas por hacer si no se cumple} \\\\ \\\\ \\left. \\right \\} \\\\ \\\\ \\end{align*} \\] ¡Vamos al ejemplo! Asignemos, primero, el valor de 2 a \\(i\\): i = 2 Luego pongamos el condicional if (i == 5){ # Nota el doble signo de igual &#39;==&#39;. i=i^2 # Si sólo pones uno, R hace que i=5. } Veamos cuánto vale \\(i\\) print(i) ## [1] 10 Hagamos ahora \\(i = 5\\) y veamos qué pasa cuando atraviesa el condicional: i = 5 if (i == 5){ i=i^2 } print(i) ## [1] 25 Hagamos ahora un condicional más complicado: pongamos que si \\(i = 5\\) entonces haga \\(i^2\\) pero si \\(i \\neq 5\\) entonces al valor de \\(i\\) le sume \\(1\\): i=1 if (i == 5){ i=i^2 } else { i=i+1 } print(i) ## [1] 2 2.15 And-or (Operadores lógicos) ¿Qué pasa si tienes varias condiciones que necesitas se cumplan a la vez? ¡Para eso están los operadores lógicos! . 2.15.1 And Por ejemplo, supongamos queremos usar dos condiciones dentro del if. ¡Es muy fácil! Basta con escribir Condición 1 &amp; Condición 2. Ejemplo: i=1 j=7 if( (i == 1) &amp; (i &lt; j) ) { i=i+j } print(i) ## [1] 8 Por otro lado si ahora hacemos \\(i &gt; j\\): i=11 j=7 if( (i == 1) &amp; (i &lt; j) ) { i=i+j } print(i) ## [1] 11 Y si hacemos que \\(i = 1\\) pero \\(i &gt; j\\): i=1 j=0 if( (i == 1) &amp; (i &lt; j) ) { i=i+j } print(i) ## [1] 1 2.16 Or El or se usa en el caso de que querramos que se cumpla al menos una de dos condiciones del if. Es decir, si tenemos dos condiciones el or se cumple cuando se cumple una de ellas o cuando se cumplen ambas. Por ejemplo: Cuando \\(i = 1\\) con \\(i &gt; j\\): i=1 j = 1/2 if( (i == 1) | (i &lt; j) ) { i=i+j } print(i) ## [1] 1.5 Cuando \\(i \\neq 1\\) pero \\(i &lt; j\\): i=21 j = 22 if( (i == 1) | (i &lt; j) ) { i=i+j } print(i) ## [1] 43 O bien cuando \\(i \\neq 1\\) e \\(i &gt; j\\) i=121 j = 22 if( (i == 1) | (i &lt; j) ) { i=i+j } print(i) ## [1] 121 La siguiente tabla resume cuándo se cumplen las condiciones: \\[ \\begin{array}{ccccc} Condición 1 &amp; Condición 2 &amp; And &amp; Or \\\\ \\hline Si &amp; Si &amp; Si &amp; Si \\\\ Si &amp; No &amp; No &amp; Si \\\\ No &amp; Si &amp; No &amp; Si \\\\ No &amp; No &amp; No &amp; No \\\\ \\end{array} \\] 2.17 Ejercicio 1 Crea los comandos necesarios para calcular la media y desviacion estandar del siguiente vector: numeros &lt;- c(7.65688984, 0.45416281, -0.53197482, -11.68901517, -0.22092715, -6.65860576, -0.96411401, -0.04875882, -0.88076032, -9.47716275, 12.48699956, 58.37690942, 0.75332369, -0.07644519, -0.47168251, 0.04574367, 0.21158367, 6.57919350, 1.61654489, -144.28602691) Tus resultados deberían ser: ## [1] &quot;Media: -4.356206118&quot; ## [1] &quot;Desviación: 35.8220144835353&quot; Nota Para el ejercicio puedes usar cualquier función de R excepto: mean, sd, var. Por si no lo recuerdas, aquí están las definiciones de media y desviacion estándar. Si bien no es la única forma pues ¡hay varias definiciones equivalentes!. 2.18 Media \\[ \\begin{equation} \\textrm{Media de }X = \\frac{x_1 + x_2 + \\cdots + x_n}{n} \\end{equation} \\] 2.19 Desviación estándar \\[ \\begin{equation} \\textrm{Desviación Estándar de }X = \\sqrt{\\textrm{Media de }X^2 - \\Big(\\textrm{Media de }X\\Big)^2} \\end{equation} \\] 2.20 Ejercicio 2 Sin correr el siguiente pedazo de código en R, estima cuánto valdrá \\(k\\) al final: k &lt;- 3 for (i in 1:6){ if (i &gt; k || k == 3){ k &lt;- k^2 } else if (i == 3 &amp; k == 7) { k &lt;- k - 2 } else if (k &gt; i &amp; i &lt; 5) { k &lt;- k*i/2 } else if (k &gt; i &amp; i &gt;= 5) { k &lt;- k + 1 } else { k &lt;- k/2 } } 2.21 Ejercicio 3 Un grupo de investigadores tienen tres vectores de datos sobre individuos: sexo, edad y exposición (horas) a humo de tabaco expo. sexo &lt;- c(&quot;Hombre&quot;,&quot;Mujer&quot;,&quot;Mujer&quot;,&quot;Mujer&quot;,&quot;Hombre&quot;, &quot;Mujer&quot;,&quot;Hombre&quot;,&quot;Hombre&quot;) edad &lt;- c(28, 12, 77, 32, 46, 53, 17, 20, 88) expo &lt;- c(1, 0, 1.5, 2.2, 2, 5, 1.01, 3.2) Ellos saben que por cada hora de exposición el riesgo relativo de enfermedad cardiovascular es de \\(1.025\\) para hombres menores a 45 y \\(1.032\\) para mujeres de la misma edad. Para mayores de 45, el riesgo es \\(1.052\\) en caso de hombres y \\(1.066\\) en caso de mujeres. Los investigadores hicieron el siguiente código para estimar los riesgos de cada uno de los individuos. Ayúdalos a que su código funcione: #Hay n personas: para cada una hay que calcular su riesgo n &lt;- length(sexo) riesgo &lt;- c() while (persona &lt; n){ #Checar la edad de la persona if (edad[persona] &lt; 45){ #Checar el sexo if (sexo[persona] = Hombre){ riesgo[persona] &lt;- expo[persona]*1.025 } else { riesgo[persona] &lt;- expo[persona]*1.032 } } else { #Checar el sexo if (sexo[persona] = Hombre){ riesgo[persona] &lt;- expo[persona]*1.052 } else { riesgo[persona] &lt;- expo[persona]*1.066 } } } Para que cheques que funcione, te dejo la respuesta. El riesgo es: ## [1] 1.02500 0.00000 1.59900 2.27040 2.10400 5.33000 1.03525 2.22 Advertencias y otras cosas poco intuitivas Es importante entender cómo funcionan las computadoras para poder simular (y entender los problemas de la simulación). Aunque los números son infinitos, las computadoras no tienen una cantidad infinita de dígitos. Por ejemplo, nosotros (humanos) podemos representar: \\[ \\begin{equation} 1 - 0.000000001 = 0.99999999 \\end{equation} \\] La computadora no puede hacerlo: 1 - 0.000000001 ## [1] 1 Tampoco puede representar números muy grandes: exp(1000) ## [1] Inf Mientras que para los humanos no hay ``un número positivo más chico’’ (si dices, por ejemplo, que \\(0.00000000001\\) es el más chico de todos los positivos (no cero), siempre puedes dividirlo entre \\(2\\): \\(0.00000000001/2\\) y obtener un número más pequeño) para las computadoras sí hay. Eso quiere decir que cuando hacemos una operación la computadora NO da la respuesta correcta sólo su mejor aproximación. A veces su mejor aproximación es la respuesta correcta: sqrt(100) ## [1] 10 Otras veces está cerca: sqrt(12345678.12345678^2) ## [1] 12345678 Pero… 2.23 Donde fallan estas cosas Intuitivamente, los decimales que nos acabamos de comer en el inciso anterior no importan ¡son simples decimales! El siguiente ejemplo muestra que sí importan. Este ejemplo calcula una función recursivamente. ¿Puedes explicar qué estamos haciendo? ejemplo &lt;- c() a &lt;- 2.701 for (i in 1:100){ if (i == 1){ ejemplo[i] &lt;- 10 } else { ejemplo[i] &lt;- ejemplo[i-1] + a*ejemplo[i-1]*(1-ejemplo[i-1]/100) } } print(ejemplo[100]) ## [1] 125.863 El ejemplo anterior resulta en un maravilloso resultado de ejemplo[100]. Redondeemos a dos decimales el valor de \\(a\\) para que sea \\(2.70\\). Intuitivamente, el valor debería estar cerca y ser ciento y tantos. Pues no… ejemplo2 &lt;- c() a &lt;- 2.70 for (i in 1:100){ if (i == 1){ ejemplo2[i] &lt;- 10 } else { ejemplo2[i] &lt;- ejemplo2[i-1] + a*ejemplo2[i-1]*(1-ejemplo2[i-1]/100) } } print(ejemplo2[100]) ## [1] 87.0938 ¡Resulta que con cambiar un decimal, el resultado cambió hasta ejemplo2[100]! La gráfica siguiente muestra como varían los valores coincidiendo al inicio y alejándose después: 2.24 Números pseudoaleatorios Para simular necesitamos generar números aleatorios. La única forma que tenemos de hacerlo (actualmente) es por medio de isótopos radiactivos que decaen aleatoriamente. ¡Si tienes uno guardado por ahí es el momento de usarlo! Como no es muy bueno que tengamos por ahí material radiactivo, los matemáticos han generado números que se conocen como pseudoaleatorios. Estas son funciones (como la del apartado anterior) que si conoces el valor inicial (en el caso pasado, \\(a\\)) las funciones son tan alocadas que los números que resultan de ella ‘’parecen aleatorios’’. 2.25 Ejercicio 4 Considera los siguientes dos fragmentos de código. Analiza los resultados. ¿Cuál de ellos es un mejor generador pseudoaleatorio y por qué? a &lt;- 2 aleatorio &lt;- c() for( i in 1:100){ aleatorio[i] &lt;- a + i } ## [1] 3 4 5 6 7 8 aleatorio2 &lt;- c() for (i in 1:100){ if (i == 1){ aleatorio2[i] &lt;- 0.9 } else { aleatorio2[i] &lt;- aleatorio2[i-1] + 2.81*aleatorio2[i-1]*(1-aleatorio2[i-1]/17) } } ## [1] 0.90000 3.29511 10.75965 21.85816 4.30552 13.33989 2.26 Aleatoreidad en R Nuestra semilla Si un día despiertas con ganas de tener 10 números aleatorios, en R ¡puedes hacerlo!: runif(10) ## [1] 0.47 0.34 0.33 0.48 0.25 0.31 0.03 0.40 0.72 0.22 Además puedes especificar la distribución. Por ejemplo, ahora sacaremos 7 números aleatorios de una normal estándar: rnorm(7) ## [1] 1.08 -0.72 -0.33 -0.57 0.58 0.55 0.40 O bien 8 valores de una exponencial con parámetro 5: rexp(8,rate=5) ## [1] 0.18 0.14 0.02 0.44 0.03 0.23 0.76 0.25 Vamos entonces a simular 1,000 números alearorios de una normal estándar: y=rnorm(1000) Un comando muy útil es la función summary que resume los cuantiles principales de la distribución así como el mínimo, el máximo y el promedio. summary(y) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -3.18147 -0.65403 -0.00277 -0.01685 0.64435 3.63367 Al momento de pedir ayuda para el comando rnorm nos podemos dar cuenta de otras funciones interesantes relacionadas con la normal: ?rnorm Podemos, estimar, por ejemplo, la densidad acumulada de una normal estándar en el 0. Es decir, ¿a qué percentil de una normal corresponde el 0? pnorm(0) ## [1] 0.5 Podemos hacer lo mismo para los números entre -10 y 10: pnorm(-10:10) ## [1] 7.61985e-24 1.12859e-19 6.22096e-16 1.27981e-12 9.86588e-10 2.86652e-07 ## [7] 3.16712e-05 1.34990e-03 2.27501e-02 1.58655e-01 5.00000e-01 8.41345e-01 ## [13] 9.77250e-01 9.98650e-01 9.99968e-01 1.00000e+00 1.00000e+00 1.00000e+00 ## [19] 1.00000e+00 1.00000e+00 1.00000e+00 Igualmente podemos graficar cómo se ven: plot(-10:10,pnorm(-10:10)) O bien graficar la función de distribución de una normal entre -10 y 10: plot(-10:10,dnorm(-10:10)) ¿Notas cómo nos faltan números en medio? Es porque el comando -10:10 recorre los números entre -10 y 10 de 1 en 1. -10:10 ## [1] -10 -9 -8 -7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7 8 ## [20] 9 10 Para hacer más refinada la cantidad de puntos podemos hacer ahora una nueva secuencia pero yendo de 0.1 en 0.1: x = seq(-10,10,0.1) Puedes ver cómo se guardaron estos valores (este documento nada más muestra los primeros 9 porque es desperdiciar mucho espacio poner los 201 valores que hizo R) x ## [1] -10.0 -9.9 -9.8 -9.7 -9.6 -9.5 -9.4 -9.3 -9.2 ¡La gráfica ahora se ve genial! plot(x,dnorm(x)) 2.27 Las semillas En el apartado anterior dijimos que los números de R no eran aleatorios sino pseudoaleatorios y que estos se generaban por medio de una función. Cuando estamos haciendo investigación con simulaciones, para que nuestro estudio sea reproducible, aunque usemos números aleatorios, debemos usar siempre los mismos. La semilla se asegura de ello. Para poner una semilla usa el comando set.seed y pon dentro un entero. set.seed(1234) Obtengamos un número aleatorio normal: rnorm(1) ## [1] -1.20707 rnorm(1) ## [1] 0.277429 Volvamos a poner la semilla y saquemos un tercero: set.seed(1234) rnorm(1) ## [1] -1.20707 ¿Notas que es el mismo número que al inicio? 2.28 Ejercicio 5 Considera una población cuyo peso se distribuye normal con media 69 y desviación estándar 4.7. Esa misma población tiene una altura normal con media 1.8 \\(m^2\\) y desviación estándar de \\(0.05\\). Simula su índice de masa corporal. Calcula la media y desviación estándar del mismo. ¡No te olvides de usar una semilla! 2.29 Ejercicio 6 Un grupo de investigadores ha decidido que el índice de masa corporal tiene una distribución Cauchy y han simulado el índice de masa corporal como sigue: set.seed(6207) #Simular IMC imc &lt;- rcauchy(100,25,0.9) #Calcular la media mean(imc) El código es correcto. Pero la hipótesis de la distribución Cauchy no. Calcula la media varias veces usando diferentes semillas ¿Cuál es el problema? ¿Ocurre lo mismo si calculas la mediana? Modelos de simulación más avanzados suelen hacerse en C, C++ o Fortran por su velocidad; empero, es necesario conocer más de programación.↩︎ La práctica hace al maestro↩︎ De ahí que se llame R porque la R es una mejor letra que la S (todos lo sabemos) -Atte. Rodrigo, el autor de este documento.↩︎ Mejor conocidos ahora como AT&amp;T, la compañía celular que nunca tiene señal.↩︎ Sus nombres empiezan con la letra R ¿coincidencia?↩︎ Si decidiste no instalar RStudio salta al final de esta sección.↩︎ Recuerda que un logaritmo base \\(a\\) te dice a qué potencia \\(b\\) tuve que elevar \\(a\\) para llegar a \\(b\\). Por ejemplo \\(\\log_{10}(100) = 2\\) te dice que para llegar al \\(100\\) tuviste que hacer \\(10^2\\).↩︎ Puede ser float, int, double pero no nos preocuparemos por eso.↩︎ En R el comando abs toma el valor absoluto.↩︎ Windows usa backslash. Y hay toda una historia detrás de ello↩︎ En los siguientes capítulos descargaremos paquetes más interesantes; pero no desprecies la utilidad de beepr yo lo he usado en múltiples ocasiones para que la computadora me avise que ya terminó de correr un código.↩︎ "],["análisis-exploratorio-de-datos.html", "Capítulo 3 Análisis Exploratorio de Datos 3.1 Inicio 3.2 Librerías 3.3 Base a analizar 3.4 Definiciones y notación 3.5 Estadísticos univariados 3.6 Ejercicios 3.7 Gráficas univariadas 3.8 Gráficas bivariadas 3.9 Estadísticos bivariados 3.10 Ejercicio 3.11 Ajuste funcional", " Capítulo 3 Análisis Exploratorio de Datos OJO Capítulo en construcción va a cambiar para cuando sea la clase 3.1 Inicio Siempre que inicies un nuevo trabajo en R ¡no olvides borrar el historial! rm(list = ls()) #Clear all 3.2 Librerías Para este análisis vamos a tener que llamar a las siguientes librerías previamente instaladas (por única vez) con install.packages: library(tidyverse) library(dplyr) library(moments) library(lubridate) library(ggcorrplot) library(ks) Si no tienes una librería puedes instalarla escribiendo en la consola el install junto con su nombre: install.packages(&quot;lubridate&quot;) 3.3 Base a analizar Como ejemplo analizaremos la base de Carpetas de Investigación de la Fiscalía General de Justicia de la CDMX para el año 2018 y mes de Diciembre misma que se encuentra en este link Si el link anterior no abre ve al sitio https://datos.cdmx.gob.mx/explore/dataset/carpetas-de-investigacion-pgj-cdmx/table/?refine.ao_hechos=2018 y elige la opción de año 2018, mes diciembre y descargar como csv. La forma más fácil en RStudio es yéndonos a Import Dataset en el panel derecho seguido de From Text y seleccionamos el archivo. En este caso hay dos opciones cualquiera de las dos opciones funciona: si en tu ordenador no sirve una, ¡prueba la otra! En mi caso el archivo está en una carpeta que se llama datasets y se lee de la siguiente manera: datos &lt;- read.csv(&quot;datasets/carpetas-de-investigacion-pgj-cdmx.csv&quot;) 3.4 Definiciones y notación Siguiendo la definición de Gelman et al. (2013) , denotamos el conjunto de datos observados como la matriz (base de datos) de \\(n \\times \\ell\\) \\[ Z = \\begin{pmatrix} z_1 \\Big| z_2 \\Big| \\dots \\Big| z_{\\ell} \\end{pmatrix} \\] donde \\(\\ell \\in \\mathbb{N}\\) con \\(\\ell &gt; 0\\) y las \\(z_i\\) sin pérdida de generalidad, son vectores columna de longitud \\(n\\) (\\(z_i = (z_{i,1}, z_{i,2}, \\dots, z_{i,n})^T\\)). Una columna \\(z_{k}\\) con \\(0 \\leq k \\leq \\ell\\) se le conoce como: Numérica si \\(z_{k} \\in \\mathbb{R}^{n}\\). En particular es entera si \\(z_{j} \\in \\mathbb{Z}^{n}\\). Categórica si cada entrada de \\(z_{k}\\) es una indicadora de pertenencia a algún conjunto (por ejemplo Hombre / Mujer ó Ingresos Altos / Ingresos Medios / Ingresos Bajos). Usualmente \\(z_{k}\\) se representa con un caracter o con un entero. Una variable cateórica puede ser lógica si \\(z_{k}\\) es un indicador que toma alguno de los dos valores: TRUE ó FALSE. Ordinal Una variable ordinal es aquél \\(z_{k} \\in \\mathcal{C}\\) donde sobre \\(\\mathcal{C}\\) existe un orden total; es decir si \\(x,y,w\\in z_{k}\\) se tiene que: Ocurre al menos una de las siguientes: \\(x \\leq y\\) ó \\(x \\geq y\\). Si \\(x \\leq y\\) y \\(y \\geq w\\) entonces \\(x \\leq w\\) Si \\(x \\leq y\\) y \\(x \\geq y\\) entonces \\(x = y\\). Variables numéricas univariadas son ordinales por el orden natural de \\(\\mathbb{R}\\). Caracter si \\(z_{k}\\) es un caracter o una cadena de caracteres donde los caracteres son el objeto de análisis en sí (no como pertenencia). Por ejemplo si cada entrada \\(z_{k,m}\\) representa un Tweet. OJO Los datos \\(z_{k,m}\\) son variables fijas ya dadas y NO SON ALEATORIAS. En el caso de nuestra base de datos podemos resumir la información contenida en la misma mediante glimpse: datos %&gt;% glimpse() ## Rows: 19,861 ## Columns: 18 ## $ año_hechos &lt;int&gt; 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, … ## $ mes_hechos &lt;chr&gt; &quot;Diciembre&quot;, &quot;Diciembre&quot;, &quot;Diciembre&quot;, &quot;Diciembr… ## $ fecha_hechos &lt;chr&gt; &quot;2018-12-13 12:00:00&quot;, &quot;2018-12-22 19:00:00&quot;, &quot;2… ## $ delito &lt;chr&gt; &quot;USURPACIÓN DE IDENTIDAD&quot;, &quot;SUSTRACCION DE MENOR… ## $ categoria_delito &lt;chr&gt; &quot;DELITO DE BAJO IMPACTO&quot;, &quot;DELITO DE BAJO IMPACT… ## $ fiscalía &lt;chr&gt; &quot;INVESTIGACIÓN EN MIGUEL HIDALGO&quot;, &quot;INVESTIGACIÓ… ## $ agencia &lt;chr&gt; &quot;MH-2&quot;, &quot;59&quot;, &quot;BJ-1&quot;, &quot;IZP-9&quot;, &quot;75TER&quot;, &quot;FDS-5&quot;,… ## $ unidad_investigacion &lt;chr&gt; &quot;UI-1SD&quot;, &quot;UI-1CD&quot;, &quot;UI-1SD&quot;, &quot;UI-2SD&quot;, &quot;3 S/D&quot;,… ## $ colonia_hechos &lt;chr&gt; &quot;LOMAS DE SOTELO&quot;, NA, &quot;DEL VALLE CENTRO&quot;, &quot;AMPL… ## $ alcaldia_hechos &lt;chr&gt; &quot;MIGUEL HIDALGO&quot;, &quot;CUAUTLA&quot;, &quot;BENITO JUAREZ&quot;, &quot;I… ## $ fecha_inicio &lt;chr&gt; &quot;2019-06-16 12:14:09&quot;, &quot;2019-06-06 16:26:15&quot;, &quot;2… ## $ mes_inicio &lt;chr&gt; &quot;Junio&quot;, &quot;Junio&quot;, &quot;Febrero&quot;, &quot;Febrero&quot;, &quot;Abril&quot;,… ## $ ao_inicio &lt;int&gt; 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, … ## $ calle_hechos &lt;chr&gt; &quot;AV. CONSCRIPTO&quot;, &quot;AVENIDFA DIEZ DE MARZO&quot;, &quot;FEL… ## $ calle_hechos2 &lt;chr&gt; &quot;.&quot;, &quot;HECHOS EN CUAUTLA MORELOS&quot;, &quot;ESQUINA COYOA… ## $ longitud &lt;dbl&gt; -99.2254, NA, -99.1709, -99.0302, -99.1342, -99.… ## $ latitud &lt;dbl&gt; 19.4403, NA, 19.3721, 19.3480, 19.5479, 19.3442,… ## $ Geopoint &lt;chr&gt; &quot;19.4402832543,-99.2253527208&quot;, &quot;&quot;, &quot;19.37206828… Notamos que el vector columna año_hechos es una variable numérica mientras que mes_hechos es categórica. No hay variables lógicas en esta base. Una variable caracter es el vector columna calle_hechos que no denota un conjunto sino una cadena de caracteres (véanse las faltas de ortografía, por ejemplo). Al ser la tabla de datos una matriz podemos acceder a la entrada en la fila \\(j\\) y columna \\(k\\) haciendo: \\[ \\textrm{base}[j,k] \\] por ejemplo: datos[4,6] ## [1] &quot;INVESTIGACIÓN EN IZTAPALAPA&quot; NOTACIÓN Para facilitar la notación en lo que sigue de estas notas y hasta nuevo aviso, si \\(z_k\\) es una columna categórica de \\(Z\\) denotaremos a los elementos de dicha columna como \\(C = (c_1, c_2, \\dots, c_n) = z_k^T\\). Si \\(z_k\\) es numérica denotamos a los elementos de dicha columna como \\(\\vec{x} = (x_1, x_2, \\dots, x_n) = z_k^T\\). 3.5 Estadísticos univariados 3.5.1 Definición [Estadístico observado] Un estadístico (observado) es una función cuyo dominio es la matriz de datos observados \\(Z\\) o una columna de la misma. Es decir, un estadístico es cualquier función de los datos (ver Wolfe and Schneider (2017)). A continuación veremos algunos ejemplos de estadísticos así como su interpretación. 1. Media poblacional Dado un vector de datos numéricos \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) definimos la media poblacional como: \\[ \\bar{x} = \\dfrac{1}{n} \\sum\\limits_{i=1}^{n} x_i, \\qquad x_i \\in \\mathbb{R} \\] En el caso de nuestros datos podemos calcular el promedio de delitos por día como sigue. Primero necesitamos especificar a R que la fecha_hechos es una fecha. Esto lo hacemos mediante la función ymd_hms (year-month-day_hour-minute-second) del paquete de lubridate y la función mutate (que cambia una columna de la base de datos). El siguiente código le indica a R que cambie la columna fecha_hechos volviéndola a leer como fecha: datos &lt;- datos %&gt;% mutate(fecha_hechos = ymd_hms(fecha_hechos)) Para mantener sólo la fecha y eliminar la hora de fecha_hechos podemos generar una nueva columna como sigue: datos &lt;- datos %&gt;% mutate(fecha = date(fecha_hechos)) Finalmente podemos contar (tally) observaciones agrupadas (group_by) por día mediante la combinación de ambas funciones: conteo_delitos &lt;- datos %&gt;% group_by(fecha) %&gt;% tally() ## # A tibble: 6 x 2 ## fecha n ## &lt;date&gt; &lt;int&gt; ## 1 2018-12-01 674 ## 2 2018-12-02 584 ## 3 2018-12-03 790 ## 4 2018-12-04 640 ## 5 2018-12-05 724 ## 6 2018-12-06 718 Hay distintas formas de calcular la media. La primera es tomando la columna directo, para acceder a una columna utilizamos el signo de pesos \\(\\$\\) como sigue: \\[ \\texttt{base} \\texttt{\\$} \\texttt{columna} \\] En nuestro caso: mean(conteo_delitos$n) ## [1] 640.677 O bien podemos usar la función summarise integrada en dplyr: conteo_delitos %&gt;% summarise(mean(n)) ## # A tibble: 1 x 1 ## `mean(n)` ## &lt;dbl&gt; ## 1 641. 2. Total poblacional (ver Särndal, Swensson, and Wretman (2003)) Dado un vector de datos numéricos \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) definimos el total poblacional como: \\[ t_{\\vec{x}} = \\sum\\limits_{i=1}^{n} x_i, \\qquad x_i \\in \\mathbb{R} \\] En este caso de las carpetas de investigación el total nos daría todas las carpetas abiertas durante diciembre. Para ello calculamos el total sumando todos los elementos: sum(conteo_delitos$n) ## [1] 19861 O bien (y esto es una de las cosas interesantes de tidyverse) agregándolo a los cálculos previos: conteo_delitos %&gt;% summarise(mean(n), sum(n)) ## # A tibble: 1 x 2 ## `mean(n)` `sum(n)` ## &lt;dbl&gt; &lt;int&gt; ## 1 641. 19861 3. Varianza poblacional (no ajustada) Dado un vector de datos numéricos \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) definimos la varianza poblacional como15: \\[ \\sigma^2_{\\vec{x}} = \\dfrac{1}{n} \\sum\\limits_{i=1}^{n} (x_i - \\bar{x})^2, \\qquad x_i \\in \\mathbb{R} \\] Misma que podemos calcular con el comando var ya sea directamente en la columna: var(conteo_delitos$n) ## [1] 10046.2 O bien a través del summarise integrando con el anterior: conteo_delitos %&gt;% summarise(mean(n), sum(n), var(n)) ## # A tibble: 1 x 3 ## `mean(n)` `sum(n)` `var(n)` ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 641. 19861 10046. La raíz cuadrada de la varianza se conoce como desviación estándar y se denota como sigue: \\[ \\sigma_{\\vec{x}} = \\sqrt{\\sigma^2_{\\vec{x}}} \\] Recuerda que la varianza se interpreta como la distancia cuadrática promedio a la que están los datos. En particular la varianza casi no considera valores que están a menos de \\(1\\) de distancia de \\(\\bar{x}\\) (pues \\((x_i - \\bar{x})^2 &lt; 1\\) en ese caso) pero le da mayor peso a valores que están muy lejanos (donde \\((x_i - \\bar{x})^2 \\gg 1\\) si \\(x_i\\) está muy lejos de \\(\\bar{x}\\)). Gráficamente: Si nos interesara que todos los valores (tanto los cercanos a \\(\\bar{x}\\) como los lejanos) pesaran de manera idéntica entonces usaríamos el MAD: 4. Desviación Media Absoluta (MAD) Dado un vector de datos numéricos \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) definimos la desviación media absoluta, MAD, como (Panaretos (2016)): \\[ \\text{MAD}_{\\vec{x}} = \\frac{1}{n} \\sum\\limits_{i=1}^{n} | x_i - \\bar{x} | \\] Misma que se puede calcular en R como: mad(conteo_delitos$n, center = mean(conteo_delitos$n)) ## [1] 114.638 o bien dentro del summarise: conteo_delitos %&gt;% summarise(mean(n), sum(n), var(n), mad(n, center = mean(n))) ## # A tibble: 1 x 4 ## `mean(n)` `sum(n)` `var(n)` `mad(n, center = mean(n))` ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 641. 19861 10046. 115. La MAD también es una forma de medir distancia pero en este caso se tiene que todos aportan por igual los muy alejados y los que no: Para pensarle: En el caso de una variable que se supone que es uniforme y no interesa penalizar valores lejanos de la media ¿cuál sería una mejor manera de cuantificar la dispersión MAD ó varianza? ¿en qué casos importaría la otra? Las siguientes dos definiciones son con base en conceptos de proba. ¿Los recuerdas? 5. Coeficiente de asimetría Dado un vector de datos numéricos \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) definimos el coeficiente de asimetría de Fisher (skewness) como: \\[ \\text{Skewness}_{\\vec{x}} = \\frac{1}{n \\sigma^3_{\\vec{x}} } \\sum\\limits_{i = 1}^{n} (x_i - \\bar{x})^3 \\] Para más referencias ver Panaretos (2016). A fin de interpretar el coeficiente de asimetría podemos dividir esa suma en dos pedazos (olvidándonos de la constante): \\[ \\sum\\limits_{i = 1}^{n} (x_i - \\bar{x})^3 = \\underbrace{\\sum\\limits_{\\substack{i = 1 \\\\ \\\\ x_i &gt; \\bar{x}}}^{n} (x_i - \\bar{x})^3}_{\\text{A}} + \\underbrace{\\sum\\limits_{\\substack{i = 1 \\\\ \\\\ x_i &lt; \\bar{x}}}^{n} (x_i - \\bar{x})^3}_{\\text{B}} \\] Notamos que si \\(|A| &gt; |B|\\) la mayor parte de las \\(x_i\\) (o las que se alejan más de la media) son mayores a \\(\\bar{x}\\) y por tanto los datos van a estar sesgados a la derecha:. Por otro lado si \\(|B| &gt; |A|\\) significa que hay más \\(x_i\\) (o con mayor peso) del lado izquierdo de la media que del lado derecho de la misma y por tanto los datos están sesgados a la izquierda. Datos insesgados son aquellos donde \\(\\text{Skewness}_{\\vec{x}} = 0\\). En el caso de las carpetas podemos calcular la asimetría que no se encuentra preprogramada en R como sigue: #Estimación de la desviación estándar desv.est &lt;- sd(conteo_delitos$n) #Estimación del x barra x.barra &lt;- mean(conteo_delitos$n) #Obtención de la n (longitud del vector) n.longitud &lt;- length(conteo_delitos$n) #Cálculo de la asimetría (1/desv.est^3)*mean((conteo_delitos$n - x.barra)^3) ## [1] -0.452821 ¿Qué implica el resultado anterior? 6. Curtosis Dado el mismo vector \\(\\vec{x}\\) que en el enunciado anterior el coeficiente de curtosis se define como \\[ \\text{Curtosis}_{\\vec{x}} = \\frac{1}{n \\sigma^4_{\\vec{x}} } \\sum\\limits_{i = 1}^{n} (x_i - \\bar{x})^4 \\] La interpretación de la curtosis es similar a la que hicimos de la varianza en el sentido que el elevar a la cuarta va a magnificar los efectos de aquellos valores que estén a más de \\(\\sigma\\) de distancia de la media pues podemos reescribir la suma como: \\[ \\frac{1}{n \\sigma^4_{\\vec{x}} } \\sum\\limits_{i = 1}^{n} (x_i - \\bar{x})^4 = \\frac{1}{n \\sigma^4_{\\vec{x}} } \\underbrace{\\sum\\limits_{\\substack{i = 1 \\\\ \\\\ | x_i - \\bar{x}| &lt; \\sigma}}^{n} (x_i - \\bar{x})^4}_{\\text{A}} + \\frac{1}{n \\sigma^4_{\\vec{x}} } \\underbrace{\\sum\\limits_{\\substack{i = 1 \\\\ \\\\ | x_i - \\bar{x}| &gt; \\sigma}}^{n} (x_i - \\bar{x})^4}_{\\text{B}} \\] Notamos que la única parte importante que apota a la curtosis es la dada por B que es la que capta las colas de la distribución (pues ese lado es \\(\\gg 1\\)) . De ahí que podamos decir que, entre dos vectores de datos, uno tiene colas más pesadas que el otro si su curtosis es mayor. En este caso podemos analizar la latitud y longitud de los datos a través de la curtosis: datos %&gt;% summarise(kurtosis(latitud, na.rm = T), kurtosis(longitud, na.rm = T)) ## kurtosis(latitud, na.rm = T) kurtosis(longitud, na.rm = T) ## 1 2.85793 3.04504 donde se agregó el comando na.rm = T para eliminar los valores de no respuesta (missing) marcados como NA. Del análisis notamos que la longitud tiene colas más pesadas que la latitud. NOTACIÓN Dado un vector \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) de valores numéricos denotamos el \\(j\\)-ésimo valor muestral (\\(1 \\leq j \\leq n\\)) como \\(x_{(j)}\\) tal que \\(x_{(1)} = \\min \\{ x_1, x_2, \\dots, x_n \\}\\) y \\[ x_{(j)} = \\min \\{ x_1, x_2, \\dots, x_n \\} \\setminus \\{ x_{(1)}, x_{(2)}, \\dots, x_{(j-1)} \\} \\] Es decir \\(x_{(j)}\\) es el valor en orden \\(j\\) al momento de ordenar la muestra. Como nota adicional se define \\(x_{(0)} = 0\\) y \\(x_{(n+1)} = 0\\). Nota La curtosis a veces se define con un denominador distinto (en términos de las \\(n\\)) como en Myatt and Johnson (2007). 7. Mediana Dado un vector de valores numéricos \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) definimos la mediana como (Panaretos (2016)): \\[ \\text{Mediana}_{\\vec{x}} = \\dfrac{x_{(\\lfloor \\frac{n+1}{2} \\rfloor)} + x_{(\\lceil \\frac{n+1}{2} \\rceil)}}{2} \\] La mediana puede calcularse fácilmente haciendo: median(conteo_delitos$n) ## [1] 646 8. Cuantil Dado un vector de valores numéricos \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) el \\(\\alpha\\)-ésimo cuantil está dado por: \\[ \\text{Cuantil}_{\\vec{x}}(\\alpha) = \\dfrac{x_{(\\lfloor \\alpha\\cdot (n+1) \\rfloor)} + x_{(\\lceil \\alpha\\cdot (n+1)\\rceil)}}{2} \\] donde \\(x_{(0)} = x_{(n+1)} = 0\\). R no calcula los cuantiles de manera exacta sino que por velocidad los aproxima mediante la función quantile. Por ejemplo en el cálculo de los cuantiles \\(\\alpha = 0.1\\) y \\(\\alpha = 0.66\\): conteo_delitos %&gt;% summarise(quantile(n, c(0.1, 0.66))) ## # A tibble: 2 x 1 ## `quantile(n, c(0.1, 0.66))` ## &lt;dbl&gt; ## 1 501 ## 2 707 La función summary también es bastante útil resumiendo múltiples observaciones de la base: summary(conteo_delitos$n) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 397 568 646 641 721 790 Ésta incluye los cuartiles los cuales corresponden a los cuantiles asociados a \\(\\alpha =0.25, 0.5, 0.75\\) y \\(1\\). Nota Hay múltiples definiciones de cuantil (ver Hyndman and Fan (1996) para un intento de homologación). En particular R utiliza una distinta y tus cómputos no van a coincidir si lo haces con esta definición y con la de R. Si quieres saber más de R consulta ?quantile 9. Rango intercuartílico Definimos el rango intercuartílico (Panaretos (2016)) para valores numèricos \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) como la distancia entre el cuantil \\(0.75\\) y el \\(0.25\\) (primer y tercer cuartil): \\[ \\text{IQR}_{\\vec{x}} = \\text{Cuantil}_{\\vec{x}}(0.75) - \\text{Cuantil}_{\\vec{x}}(0.25) \\] IQR(conteo_delitos$n) ## [1] 153 10. Valores atípicos (outliers) Dado un vector de datos numéricos \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) seguimos a Panaretos (2016) para definir los valores atípicos outliers como aquellas observaciones: \\[ \\textrm{Outliers}_{\\vec{x}} = \\Big\\{ x_i \\in \\vec{x} \\big| x_i \\not\\in \\big[ \\text{Cuantil}_{\\vec{x}}(0.25) - \\frac{3}{2} \\text{IQR}_{\\vec{x}}, \\text{Cuantil}_{\\vec{x}}(0.75) + \\frac{3}{2} \\text{IQR}_{\\vec{x}}\\big] \\Big\\} \\] Los outliers en esta definción son valores que serían verdaderamente improbables bajo una distribución normal. Particularmente en el caso de la normal los outliers son valores que tienen una probabilidad de salir aproximadamente de 0.006977 (por eso son atípicos porque no se esperaría que aparecieran nunca). Para identificar los outliers calculamos el IQR primero y los cuartiles: iqr &lt;- IQR(conteo_delitos$n) cuartil1 &lt;- quantile(conteo_delitos$n, 0.25) cuartil3 &lt;- quantile(conteo_delitos$n, 0.75) después identificamos el límite inferior y superior del conjunto lim.inf &lt;- cuartil1 - 3/2*iqr lim.sup &lt;- cuartil3 + 3/2*iqr finalmente preguntamos por cuáles están antes o después: outliers &lt;- conteo_delitos %&gt;% filter(n &lt; lim.inf | n &gt; lim.sup) En este caso no tenemos outliers. NOTA Según la aplicación que tenemos la definición de outlier cambia. La actual es la que se utiliza para datos que pudieran ser descritos mediante una Normal; empero, no siempre esta definición de outlier es un buen modelo (por ejemplo en datos como ingreso que son cantidades positivas, con mucha asimetría y cola pesada). Un buen tratamiento sobre los outliers puedes encontrarlo en SURI, Murty, and Athithan (2019). 11. Rango El rango (Peck, Olsen, and Devore (2015)) se define como la diferencia entre el mínimo y el máximo de los valores de un vector numérico \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\): \\[ \\textrm{Rango}_{\\vec{x}} = \\max \\{x_1, x_2, \\dots, x_n\\} - \\min \\{x_1, x_2, \\dots, x_n\\} \\] En R puede calcularse con la resta: #Obtenemos máximo y mínimo maximo &lt;- max(conteo_delitos$n) minimo &lt;- min(conteo_delitos$n) #Rango maximo - minimo ## [1] 393 Nota En algunos casos el rango se refiere al intervalo \\([a,b]\\) de valores donde \\(a = \\min \\{x_1, x_2, \\dots, x_n\\}\\) y \\(b = \\max \\{x_1, x_2, \\dots, x_n\\}\\). Éste es el caso de la función range en R: range(conteo_delitos$n) ## [1] 397 790 12. Conteo asociado a un conjunto Sea \\(\\vec{y} = (y_1, y_2, \\dots, y_n)^T\\) un vector de datos de cualquier tipo (numéricos, categóricos, lógicos, caracteres, etc). Para un conjunto \\(A\\) definimos el conteo asociado al conjunto \\(A\\) como: \\[ \\text{Conteo}_{\\vec{y}}(A) = \\sum\\limits_{i = 1}^{n} \\mathbb{I}_A (y_i) \\] donde \\[ \\mathbb{I}_A (y) = \\begin{cases} 1 &amp; \\text{ si } y \\in A, \\\\ 0 &amp; \\text{ en otro caso }, \\end{cases} \\] es una variable indicadora. Una forma rápida de obtener dicho conteo en R es mediante table: table(datos$delito) ## ## ABANDONO DE PERSONA ABORTO ABUSO DE AUTORIDAD ABUSO DE CONFIANZA ## 53 15 102 276 ## ABUSO SEXUAL ACOSO SEXUAL ## 252 30 O bien si se desean contar en la base de datos por ejemplo los delitos de ABANDONO DE PERSONA pueden hacerse mediante un filtro. datos %&gt;% filter(delito == &quot;ABANDONO DE PERSONA&quot;) %&gt;% tally() ## n ## 1 53 Al filtro pueden agregárseles grupos por si se desea obtener por fecha: datos %&gt;% filter(delito == &quot;ABANDONO DE PERSONA&quot;) %&gt;% group_by(fecha) %&gt;% tally() ## # A tibble: 21 x 2 ## fecha n ## &lt;date&gt; &lt;int&gt; ## 1 2018-12-01 3 ## 2 2018-12-02 3 ## 3 2018-12-04 2 ## 4 2018-12-05 8 ## 5 2018-12-06 1 ## 6 2018-12-07 1 ## 7 2018-12-10 1 ## 8 2018-12-12 2 ## 9 2018-12-13 3 ## 10 2018-12-14 2 ## # … with 11 more rows El filtro funciona igual que un if pudiéndose usar (&amp;) u (|): datos %&gt;% filter(delito == &quot;ABANDONO DE PERSONA&quot; | delito == &quot;ABORTO&quot;) %&gt;% group_by(fecha) %&gt;% tally() ## # A tibble: 25 x 2 ## fecha n ## &lt;date&gt; &lt;int&gt; ## 1 2018-12-01 3 ## 2 2018-12-02 3 ## 3 2018-12-04 5 ## 4 2018-12-05 8 ## 5 2018-12-06 1 ## 6 2018-12-07 2 ## 7 2018-12-10 1 ## 8 2018-12-12 2 ## 9 2018-12-13 4 ## 10 2018-12-14 2 ## # … with 15 more rows datos %&gt;% filter(delito == &quot;ABANDONO DE PERSONA&quot; &amp; fiscalía == &quot;INVESTIGACIÓN EN IZTAPALAPA&quot;) %&gt;% group_by(fecha) %&gt;% tally() ## # A tibble: 3 x 2 ## fecha n ## &lt;date&gt; &lt;int&gt; ## 1 2018-12-02 1 ## 2 2018-12-13 1 ## 3 2018-12-20 1 13. Moda En términos simples, la moda es el conjunto de los valores que más se repiten. Matemáticamente (ver Peck, Olsen, and Devore (2015)) la moda es el conjunto \\(\\textrm{Moda}_{\\vec{y}} = \\{ m_1, m_2, \\dots, m_k \\}\\) tal que \\(m \\in \\textrm{Moda}\\) sí y sólo si \\[ \\sum_{i = 1}^{n} \\mathbb{I}_{\\{m\\}}(y_i) \\geq \\sum_{i = 1}^{n} \\mathbb{I}_{\\{ \\ell\\}}(y_i) \\qquad \\forall \\ell \\neq m \\textrm{ donde } y_i \\in \\vec{y}. \\] Para calcularla en R no existe una función predefinida para calcular la moda. Nosotros podemos crearla con el comando function. El término function nos sirve para construir funciones; por ejemplo, una función que eleva al cuadrado: elevar.cuadrado &lt;- function(x){ return(x^2) } Observa la estructura que siempre será de esta forma: nombre de la función &lt;- function(parámetro, otro parámetro){ #Lo que sea que haga return(lo que devuelve) } Podemos llamar a la función con un número: elevar.cuadrado(8) ## [1] 64 o bien con un vector: elevar.cuadrado(12) ## [1] 144 En nuestro caso vamos a crear una función que se llame moda para estimar la moda: #Función para estimar la moda de un vector x moda &lt;- function(x){ #Contar cuántas veces aparecen las observaciones conteo &lt;- table(x) #Obtengo el máximo que aparece max_aparece &lt;- max(conteo) #Busco cuáles aparecen más y obtengo los nombres moda &lt;- names(conteo)[which(conteo == max_aparece)] #Finalmente checo que si los datos eran numéricos moda debe #ser numérico if (is.numeric(x)){ moda &lt;- as.numeric(moda) } return(moda) } Podemos probar nuestra función con datos que ya sepamos su resultado nada más para asegurarnos que funciona: #Creamos un vector numérico con dos modas vector.ejemplo.1 &lt;- c(1,6,6,1,2,7,8,10) moda(vector.ejemplo.1) ## [1] 1 6 Podemos probarlo también con caracteres: #Creamos un vector numérico con dos modas vector.ejemplo.2 &lt;- c(&quot;manzana&quot;,&quot;pera&quot;,&quot;guayaba&quot;,&quot;perejil&quot;,&quot;manzana&quot;) moda(vector.ejemplo.2) ## [1] &quot;manzana&quot; Una vez sabemos funciona podemos buscar el delito que ocurrió más: moda(datos$delito) ## [1] &quot;VIOLENCIA FAMILIAR&quot; 3.6 Ejercicios Construye una función que tome de input dos variables: \\(x\\) un vector y \\(k\\) un entero de tal manera que calcule el \\(k\\)-ésimo momento central de los datos: \\[ \\text{Momento}_{\\vec{x}}(k) = \\frac{1}{n} \\sum\\limits_{i=1}^n (x_i - \\bar{x})^k \\] La función debe tener la siguiente estructura: kesimo.momento &lt;- function(x, k){ #Rellena aquí } Sin usar la opción de trim ni trimmed.mean crea una función que calcule la media de los datos que están entre el cuantil \\(\\alpha/2\\) y el cuantil \\(1 - \\alpha/2\\) (\\(0 \\leq \\alpha \\leq 1\\). A esta media se le conoce como media truncada al nivel \\(\\alpha \\times 100\\%\\). Matemáticamente se define como: \\[ \\textrm{Media Truncada}_{\\vec{x}}(\\alpha) = \\frac{1}{n_\\alpha} \\sum\\limits_{i = 1}^{n} x_i \\cdot \\mathbb{I}_{[q_{\\alpha/2}, q_{1-\\alpha/2}]}(x_i) \\] donde \\(n_{\\alpha} = \\sum_{i=1}^n \\mathbb{I}_{[q_{\\alpha/2}, q_{1-\\alpha/2}]}(x_i)\\) es la cantidad de \\(x_i\\) que están en el intervalo \\([q_{\\alpha/2}, q_{1-\\alpha/2}]\\) donde \\(q_{\\alpha/2} = \\text{Cuantil}_{\\vec{x}}(\\alpha/2)\\) y \\(q_{1 - \\alpha/2} = \\text{Cuantil}_{\\vec{x}}(1 - \\alpha/2)\\). Una función llamada jesimo.dato de dos argumentos que dado un vector de datos \\(\\vec{x}\\) me devuelva el \\(j\\)-ésimo dato ordenado (es decir el \\(x_{(j)}\\)). NOTA No confundir con devolver el \\(x_j\\) que es la \\(j\\)-ésima entrada. Como sugerencia usar arrange, order ó sort. Un ejemplo de lo que debe hacer la función es: x &lt;- c(12,8,9,7,14, 21) jesimo.dato(x, 4) ## [1] 12 3.7 Gráficas univariadas 1. Gráfica de caja (boxplot) Una gráfica de caja pretende resumir los cuartiles, la mediana e identificar los outliers todo en una sola imagen (Panaretos (2016)). Para ello considera un vector numérico \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) tal que: \\(q_1\\) sea el primer cuartil (\\(\\textrm{Cuantil}_{\\vec{x}}(0.25)\\)), \\(q_2\\) sea la mediana (que es lo mismo que el segundo cuartil o bien \\(\\textrm{Cuantil}_{\\vec{x}}(0.5)\\)) y \\(q_3\\) corresponda al tercer cuartil (\\(\\textrm{Cuantil}_{\\vec{x}}(0.75)\\)). \\(w_1 = \\min \\{x_j \\in \\vec{x} | x_j \\geq q_1 - \\frac{3}{2} IQR \\}\\) es el valor más pequeño de \\(\\vec{x}\\) que no es outlier y \\(w_2 = \\max \\{x_j \\in \\vec{x} | x_j \\leq q_3 + \\frac{3}{2} IQR \\}\\) es el valor más grande de \\(\\vec{x}\\) que no es outlier. Sea \\(\\textrm{Outliers}_{\\vec{x}}\\) el conjunto de outliers como lo definimos anteriormente: \\[ \\textrm{Outliers}_{\\vec{x}} = \\Big\\{ x_i \\in \\vec{x} \\big| x_i \\not\\in \\big[ q_1 - \\frac{3}{2} \\text{IQR}_{\\vec{x}}, q_3 + \\frac{3}{2} \\text{IQR}_{\\vec{x}}\\big] \\Big\\} \\] donde \\(\\textrm{Outliers}_{\\vec{x}} = \\{ o_1, o_2, \\dots, o_d \\}\\). Una gráfica de caja corresponde al siguiente diagrama: La imagen anota la mediana, los cuartiles así como el rango de valores donde se sabe que no hay outliers. Finalmente la gráfica identifica los outliers si es que hay. Para armar una gráfica de boxplot usamos la librería de ggplot2 especificando dentro de la función ggplot la base de datos de donde sale nuestra información: ggplot(conteo_delitos) + geom_boxplot(aes(x = n)) la cual pone la mediana en 646 como habíamos calculado, los cuartiles en 568 y 721 respectivamente. Finalmente no presenta outliers pues nuestro análisis previo nos mostraba que no había outliers. Podemos personalizar nuestra gráfica agregando títulos con la función lab: ggplot(conteo_delitos) + geom_boxplot(aes(x = n)) + labs( x = &quot;Cantidad de carpetas de investigación abiertas por día&quot;, y = &quot;&quot;, title = &quot;Gráfica de cajas de los delitos en CDMX&quot;, subtitle = &quot;Fuente: Carpetas de investigación FGJ de la Ciudad de México&quot;, caption = &quot;Datos de Diciembre 2018&quot; ) Finalmente, podemos personalizar los colores de la gráfica editando directamente en el geom_boxplot: ggplot(conteo_delitos) + geom_boxplot(aes(x = n), color = &quot;red&quot;, fill = &quot;deepskyblue4&quot;) + labs( x = &quot;Cantidad de carpetas de investigación abiertas por día&quot;, y = &quot;&quot;, title = &quot;Gráfica de cajas de los delitos en CDMX&quot;, subtitle = &quot;Fuente: Carpetas de investigación FGJ de la Ciudad de México&quot;, caption = &quot;Datos de Diciembre 2018&quot; ) 2. Gráfica de barras Sea \\(\\vec{c} = (c_1, c_2, \\dots, c_n)^T\\) un vector de datos categóricos. Sea \\(C = \\{ a_i | a_i \\in \\vec{c} \\}\\) el conjunto de \\(\\ell\\) valores únicos que se tienen registrados en el vector \\(\\vec{c}\\). Denotamos la cantidad de veces que aparece \\(a_i\\) en \\(\\vec{c}\\) como \\(n_i\\); es decir: \\[ n_i = \\sum\\limits_{k = 1}^n \\mathbb{I}_{\\{a_i\\}}(c_k) \\] Una gráfica de barras consiste en una representación gráfica del conjunto: \\[ \\text{Barras} = \\{ (a_i, n_i) | a_i \\in C \\} \\] Gráficamente: Podemos crear una gráfica de barras con el comando geom_col para ello creemos unas barras correspondientes al tipo de delito (sólo en delitos que categoria_delito dice ROBO) haciendo una nueva base que cuente por delito: conteo_tipo &lt;- datos %&gt;% filter(str_detect(categoria_delito,&quot;ROBO&quot;)) %&gt;% group_by(delito) %&gt;% tally() Y hagamos la gráfica: ggplot(conteo_tipo) + geom_col(aes(x = delito, y = n), color = &quot;white&quot;) + theme_bw() Para evitar que se encime todo el texto podemos establecer un ángulo del mismo al usar theme: ggplot(conteo_tipo) + geom_col(aes(x = delito, y = n), color = &quot;white&quot;) + theme_bw() + theme(axis.text.x = element_text(angle = 90, size = 3)) NOTA Una mala praxis es usar gráficas de pay pues es muy complicado contar una historia a partir de ellas. ¡No lo hagas! 3.8 Gráficas bivariadas 1. Gráfica de puntos (scatterplot) Dada una matriz de datos \\(Z\\) consideramos dos columnas numéricas \\(z_i\\) y \\(z_j\\) (\\(i \\neq j\\)) de dicha matriz. Sea \\(\\mathbb{X} = \\{ (z_{i,1}, z_{j,1}), (z_{i,2}, z_{j,2}), \\dots, (z_{i,n}, z_{j,n}) \\}\\) el conjunto de parejas ordenadas correspondientes a dichas columnas. Una gráfica de puntos consiste en la proyección de dichos puntos sobre \\(\\mathbb{R}^2\\). Para generarla en R podemos usar ggplot: ggplot(datos) + geom_point(aes(x = longitud, y = latitud), size = 1, color = &quot;purple&quot;, alpha = 0.2) donde los parámetros size establecen el tamaño del punto, color su color y alpha su nivel de transparencia (\\(0 \\leq \\alpha \\leq 1\\)). 2. Gráfica de líneas (lineplot) Dada una matriz de datos \\(Z\\) consideramos dos columnas numéricas \\(z_i\\) y \\(z_j\\) (\\(i \\neq j\\)) de dicha matriz. Sea \\(\\mathbb{X} = \\{ (z_{i,1}, z_{j,1}), (z_{i,2}, z_{j,2}), \\dots, (z_{i,n}, z_{j,n}) \\}\\) el conjunto de parejas ordenadas correspondientes a dichas columnas. Para evitar confusión de subíndices escribiré a las \\(z_i\\) como \\(x\\) y a las \\(z_j\\) como \\(y\\) de tal forma que \\(\\mathbb{X} = \\{ (x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n) \\}\\) Supongamos, sin pérdida de generalidad que los datos están ordenados según las \\(x\\): \\(x_1 \\leq x_2 \\leq \\dots \\leq x_n\\). Sea \\(f\\) la función de interpolación lineal dada por: \\[ f(x) = \\begin{cases} y_1 + \\frac{y_2 - y_1}{x_2 - x_1} (x -x_1) &amp; \\text{ si } x_1 \\leq x \\leq x_2 \\\\ \\vdots \\\\ y_{k-1} + \\frac{y_k - y_{k-1}}{x_k - x_{k-1}} (x -x_{k-1}) &amp; \\text{ si } x_{k-1} \\leq x \\leq x_k \\\\ \\vdots \\\\ y_{n-1} + \\frac{y_{n} - y_{n-1}}{x_n - x_{n-1}} (x -x_{n-1}) &amp; \\text{ si } x_{n-1} \\leq x \\leq x_n \\\\ \\end{cases} \\] Una gráfica de líneas corresponde a la representación gráfica del conjunto \\[ \\textrm{Gr}_f = \\Big\\{ \\big(x, f(x)\\big) | x_1 \\leq x \\leq x_n \\Big\\} \\] De manera un poco más intuitiva notamos que si tenemos, por ejemplo, \\(\\mathbb{X} = \\{(x_1, y_1),(x_2, y_2), (x_3, y_3), (x_4, y_4)\\}\\) una gráfica de líneas se construye interpolando una línea entre \\((x_1, y_1)\\) y \\((x_2, y_2)\\), otra línea entre \\((x_2, y_2)\\) y \\((x_3, y_3)\\) y, finalmente, otra recta entre \\((x_3, y_3)\\) y \\((x_4, y_4)\\). Usando la ecuación de la línea \\[ y = \\frac{y_2 - y_1}{x_2 - x_1} (x - x_1) + y_1 \\] interpolamos cada uno de los puntos como en la gráfica siguiente: Para realizar una gráfica de líneas podemos usar de nuevo ggplot2 con la opción de geom_line: ggplot(conteo_delitos) + geom_line(aes(x = fecha, y = n)) Podemos cambiar el tema y agregar puntos de otro color para que nuestra gráfica se vea más bonita: ggplot(conteo_delitos) + geom_line(aes(x = fecha, y = n)) + geom_point(aes(x = fecha, y = n), color = &quot;red&quot;, size = 3) + theme_classic() + labs( x = &quot;Fecha de apertura de la carpeta de investigación&quot;, y = &quot;Cantidad de carpetas de investigación en FGJ&quot; ) Finalmente con geom_label podemos agregar anotaciones a nuestra gráfica: ggplot(conteo_delitos) + geom_line(aes(x = fecha, y = n)) + geom_point(aes(x = fecha, y = n), color = &quot;red&quot;, size = 3) + theme_classic() + labs( x = &quot;Fecha de apertura de la carpeta de investigación&quot;, y = &quot;Cantidad de carpetas de investigación en FGJ&quot; ) + geom_label(aes(x = dmy(&quot;25/12/2018&quot;), y = 425), label = &quot;Efecto de Navidad&quot;) 3.8.1 Ejercicio Utiliza las siguiente bases de datos para replicar exactamente el formato de las gráficas que se muestran abajo de las bases. No todo viene en estas notas, la idea es que investigues y para ello te sugiero consultar este libro Gráfica de barras datos.barras &lt;- data.frame(Pais = c(&quot;EEUU&quot;,&quot;Canadá&quot;,&quot;México&quot;), PIB = c(20.54, 17.13, 1.21)) Los colores usados son firebrick, deepskyblue3 y forestgreen: Línea x &lt;- seq(-2*pi, 2*pi, length.out = 100) datos.linea &lt;- data.frame(x = x, y = sin(x)) Boxplot x &lt;- c(1,10, 100, -2, 3, 5, 6, 12, -8, 31, 2, pi, 3) datos.linea &lt;- data.frame(Dientes = x) Puntos datos.arbol &lt;- data.frame(altura = c(1.7, 1.4, 1.8, 1.9, 1.5, 1.7, 1.6, 1.8, 1.7, 1.8), ancho = c(1.2, 1.4, 1.2, 1, 1.5, 1.7, 1.6, 1.2, 1.2, 1), tipo = c(&quot;Pino&quot;,&quot;Sauce&quot;,&quot;Sauce&quot;,&quot;Sauce&quot;,&quot;Pino&quot;, &quot;Pino&quot;,&quot;Pino&quot;,&quot;Sauce&quot;,&quot;Sauce&quot;,&quot;Sauce&quot;)) 3.9 Estadísticos bivariados NOTACIÓN Para esta sección vamos a considerar dos (vectores) columnas de la matriz de datos \\(Z\\) y los denominaremos \\(\\vec{x}\\) y \\(\\vec{y}\\) (en lugar de \\(z_i\\) y \\(z_j\\)). En particular, denotaremos \\(\\mathcal{X} = \\{ a_{i, x } | a_{i,x} \\in \\vec{x} \\}\\) el conjunto de valores únicos del vector \\(\\vec{x}\\) y \\(\\mathcal{Y} = \\{ a_{y ,j} | a_{y ,j} \\in \\vec{y} \\}\\) el conjunto de valores únicos de \\(\\vec{y}\\). La cardinalidad de dichos conjuntos es \\(\\ell_{x}\\) y \\(\\ell_{y}\\) respectivamente. Finalmente, definimos el conteo de cuántas veces aparece el valor \\(a_{i,x }\\) (respectivamente el \\(a_{y ,j}\\)) en los vectores \\(\\vec{x}\\) (respectivamente \\(\\vec{y}\\)) como: \\[\\begin{equation} \\begin{aligned} n_{i,x } &amp; = \\sum\\limits_{k=1}^{n} \\mathbb{I}_{\\{ a_{i,x } \\}}(x_k) \\\\ n_{y ,j} &amp; = \\sum\\limits_{k=1}^{n} \\mathbb{I}_{\\{ a_{y ,j} \\}}(y_k) \\end{aligned} \\end{equation}\\] para \\(1 \\leq i \\leq \\ell_{x}\\) y \\(1 \\leq j \\leq \\ell_{y}\\). Por poner un ejemplo, considera el siguiente conjunto de datos: x y Rojo Coche Azul Taza Verde Árbol Rojo Taza Verde Libro En este sentido el vector es \\(\\vec{x} = (\\text{Rojo},\\text{Azul},\\text{Verde},\\text{Rojo},\\text{Verde})^T\\) mientras que el conjunto de valores únicos asociados está dado por \\(\\mathcal{X} = \\{ \\text{Rojo},\\text{Azul},\\text{Verde} \\}\\). En este sentido (siguiendo el conjunto) se tiene que \\(a_{1,x} = \\text{Rojo}\\), \\(a_{2,x} = \\text{Azul}\\) y \\(a_{3,x } = \\text{Verde}\\) mientras que (siguiendo el vector) se observa \\(x_1 = \\text{Rojo}\\), \\(x_2 = \\text{Azul}\\), \\(x_3 = \\text{Verde}\\), \\(x_4 = \\text{Rojo}\\), \\(x_5 = \\text{Verde}\\). Finalmente notamos que el conteo de veces que aparece cada cosa es: \\(n_{1,x} = 2\\) (aparece el \\(a_{1, x}\\) que es rojo dos veces), \\(n_{2,x} = 1\\) y \\(n_{3,x } = 2\\) (el azul y verde dados por \\(a_{2,x}\\) y \\(a_{3,x}\\) respectivamente aparecen una vez para azul y dos veces para verde). Por otro lado, \\(\\vec{y} = (\\text{Coche},\\text{Taza},\\text{Árbol},\\text{Taza},\\text{Libro})^T\\) con su conjunto de valores únicos \\(\\mathcal{Y} = \\{ \\text{Coche},\\text{Taza},\\text{Árbol}, \\text{Libro} \\}\\). Para el caso de \\(\\vec{y}\\) se tiene que \\(y_1 = \\text{Coche}\\), \\(y_2 = \\text{Taza}\\), \\(y_3 = \\text{Árbol}\\), \\(y_4 = \\text{Taza}\\), \\(y_5 = \\text{Libro}\\) mientras que en el caso de valores únicos \\(a_{y, 1} = \\text{Coche}\\), \\(a_{y, 2} = \\text{Taza}\\), \\(a_{y, 3} = \\text{Árbol}\\), \\(a_{\\cdot, 4} = \\text{Libro}\\). Los conteos asociados son: \\(n_{y,1} = n_{y,3} = n_{y,4} = 1\\) (aparecen el coche, el árbol y el libro una vez) mientras que \\(n_{y,2} = 2\\) representa que la taza está dos veces. Por otro lado denotamos a la submatriz de \\(Z\\) compuesta solamente por las columnas \\(\\vec{x}\\) y \\(\\vec{y}\\) como: \\[ Z_{(x,y)} = \\begin{pmatrix} x_1 &amp; y_1 \\\\ x_2 &amp; y_2 \\\\ \\vdots &amp; \\vdots \\\\ x_n &amp; y_n \\\\ \\end{pmatrix} \\] Sea \\(\\mathcal{X}\\times\\mathcal{Y} = \\{ a_{i,j} = (x_i,y_j) | x_i \\in \\mathcal{X} \\quad \\&amp; \\quad y_j \\in \\mathcal{Y}\\}\\) el conjunto de observaciones únicas posibles de las parejas \\((x,y)\\) (todas las permutaciones). Finalmente, el conteo de cuántas veces aparece el vector bivariado \\(a_{i,j}\\) en los datos está dado por: \\[ n_{i,j} = \\sum\\limits_{k = 1}^{n} \\mathbb{I}_{\\{ a_{i,j} \\}}\\big( (x_k, y_k) \\big) \\] En el ejemplo anterior, la tabla se vería: ## Warning: Setting row names on a tibble is deprecated. Coche Taza Árbol Libro Total (fila) Rojo 1 1 0 0 2 Azul 0 1 0 0 1 Verde 0 0 1 1 2 Total (columna) 1 2 1 1 5 Una excelente referencia para esta sección es el capítulo 4 de Peck, Olsen, and Devore (2015). 1. Tabla de contingencia Para \\(\\vec{x}\\), \\(\\vec{y}\\) definidas como al inicio de la sección (y siguiendo la notación anterior), definimos una tabla de contingencia como la matriz \\(N_{x,y}\\) dada por: \\[ N_{x,y} = \\begin{pmatrix} n_{1,1} &amp; n_{1,2} &amp; \\dots &amp; n_{1, \\ell_y} \\\\ n_{2,1} &amp; n_{2,2} &amp; \\dots &amp; n_{2, \\ell_y} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ n_{\\ell_x,1} &amp; n_{\\ell_x,2} &amp; \\dots &amp; n_{\\ell_x, \\ell_y} \\\\ \\end{pmatrix} \\] Al vector \\(n_x = (n_{1,x}, n_{2,x}, \\dots, n_{\\ell_x,x})^T\\) se le conoce como distribución frecuencial (observada) marginal de \\(\\vec{x}\\) mientras que \\(n_y = (n_{y,1}, n_{y,2}, \\dots, n_{y,\\ell_y})^T\\) es la distribución frecuencial (observada) marginal de \\(\\vec{y}\\). Una tabla de contingencia representa el conteo de observaciones de una variable ajustado por la otra. Para crear una tabla de contingencia en R podemos usar el mismo comando table que ya usamos antes pero esta vez introduciendo dos vectores como en el siguiente ejemplo donde notamos alcaldía contra año del registo: table(datos$alcaldia_hechos, datos$ao_inicio) ## ## 2018 2019 ## VERACRUZ 0 1 ## VILLAGRAN 1 0 ## XALATLACO 2 0 ## XOCHIMILCO 465 115 ## XOCHITEPEC 1 0 ## ZACATECAS 0 2 Para agregar las distribuciones frecuenciales marginales a la tabla podemos usar el comando addmargins: addmargins(table(datos$alcaldia_hechos, datos$ao_inicio)) ## ## 2018 2019 Sum ## VILLAGRAN 1 0 1 ## XALATLACO 2 0 2 ## XOCHIMILCO 465 115 580 ## XOCHITEPEC 1 0 1 ## ZACATECAS 0 2 2 ## Sum 15952 3896 19848 2. Tabla de frecuencias Una tabla de frecuencia es la matriz \\(\\text{Freq}_{x,y}\\) dada por: \\[ \\text{Freq}_{x,y} = \\begin{pmatrix} f_{1,1} &amp; f_{1,2} &amp; \\dots &amp; f_{1, \\ell_y} \\\\ f_{2,1} &amp; f_{2,2} &amp; \\dots &amp; f_{2, \\ell_y} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ f_{\\ell_x,1} &amp; f_{\\ell_x,2} &amp; \\dots &amp; f_{\\ell_x, \\ell_y} \\\\ \\end{pmatrix} \\] donde \\(f_{i,j} = \\frac{n_{i,j}}{n}\\) representa la frecuencia relativa de la observación de \\((a_{i,x}, a_{y,j})\\) i.e. cuánto representa del total. Al vector \\(f_{x} = (f_{1,x}, f_{2,x}, \\dots, f_{\\ell_x,x})^T\\) se le conoce como la distribución frecuencial marginal relativa de \\(\\vec{x}\\). Análogamente para \\(y\\) se tiene la distribución frecuencial marginal relativa de \\(\\vec{y}\\) dada por: \\(f_{y} = (f_{y,1}, f_{y,2}, \\dots, f_{y,\\ell_y})^T\\). Las entradas de dichos vectores son de la forma \\(f_{i,x} = n_{i,x}/n\\) y \\(f_{y,j} = n_{y,j}/n\\). En R podemos obtener las frecuencias mediante prop.table: prop.table(table(datos$alcaldia_hechos, datos$ao_inicio)) ## ## 2018 2019 ## VERACRUZ 0.00000e+00 5.03829e-05 ## VILLAGRAN 5.03829e-05 0.00000e+00 ## XALATLACO 1.00766e-04 0.00000e+00 ## XOCHIMILCO 2.34281e-02 5.79403e-03 ## XOCHITEPEC 5.03829e-05 0.00000e+00 ## ZACATECAS 0.00000e+00 1.00766e-04 Así mismo, podemos agregar las marginales: addmargins(prop.table(table(datos$alcaldia_hechos, datos$ao_inicio))) ## ## 2018 2019 Sum ## VILLAGRAN 5.03829e-05 0.00000e+00 5.03829e-05 ## XALATLACO 1.00766e-04 0.00000e+00 1.00766e-04 ## XOCHIMILCO 2.34281e-02 5.79403e-03 2.92221e-02 ## XOCHITEPEC 5.03829e-05 0.00000e+00 5.03829e-05 ## ZACATECAS 0.00000e+00 1.00766e-04 1.00766e-04 ## Sum 8.03708e-01 1.96292e-01 1.00000e+00 3. Riesgo Relativo (discreto) Para definir Riesgo Relativo empezaremos por un ejemplo. Tomamos la tabla donde se guardó un registro de personas según si fumaban o no así como si dichas personas desarrollaron o no enfisema pulmonar. ## Warning: Setting row names on a tibble is deprecated. FUMA NO FUMA Con enfisema 100 40 Sin enfisema 30 50 Si quisiéramos analizar la hipótesis de que FUMAR está asociado con ENFISEMA tendríamos que ver, dentro de la población de fumadores (FUMAR = SÍ) cuántos hay (proporcionalmente) con ENFISEMA. La hipótesis es que si no hubiera relación, saldría que las proporciones de fumadores con y sin enfisema serían \\(50\\%\\) cada una. La proporción de fumadores con enfisema está dada por \\(100/130\\) mientras que la de no fumadores con enfisema es \\(40/90\\). El riesgo relativo (intuitivamente). se define como la división entre ambas proporciones: \\[ \\text{Riesgo Relativo de Enfisema} = \\dfrac{\\frac{\\text{Expuestos enfermos}}{\\text{Total de expuestos}}}{\\frac{\\text{No Expuestos enfermos}}{\\text{Total de no expuestos}}} = \\dfrac{100/130}{40/90} \\approx 1.73 \\] Lo que se interpreta como que los fumadores tienen \\(1.73\\) veces más riesgo de desarrollar enfisema que los no fumadores ya que si despejamos de la fórmula anterior: \\[ \\frac{\\text{Expuestos enfermos}}{\\text{Total de expuestos}} \\approx 1.73 \\times \\frac{\\text{No Expuestos enfermos}}{\\text{Total de no expuestos}} \\] De manera general, dadas dos vectores lógicos \\(\\vec{x}\\) (interpretada como el resultado) y \\(\\vec{y}\\) (interpretada como la exposición) con una tabla de contingencia y frecuencias marginales dadas por la tabla: ## Warning: Setting row names on a tibble is deprecated. Expuesto (y) NO expuesto (y) Resultado (x) a b Sin resultado (x) c d definimos el riesgo relativo de \\(\\vec{x}\\) dado \\(\\vec{y}\\) como: \\[ RR(\\vec{x}|\\vec{y}) = \\dfrac{\\frac{a}{a + c}}{\\frac{b}{b + d}} \\] Mientras que el riesgo relativo de no \\(\\vec{x}\\) dado \\(\\vec{y}\\) está dado por: \\[ RR(\\neg \\vec{x}|\\vec{y}) = \\dfrac{\\frac{c}{a + c}}{\\frac{d}{b + d}} \\] La base de datos de los delitos no contiene información suficiente para poder calcular un riesgo relativo pero podemos crear la base de datos correspondiente a la tabla como sigue: fumadores &lt;- data.frame(SI_FUMA = c(100, 30), NO_FUMA =c(40, 50)) Podemos agregar nombres a las filas para tener la base de datos mejor: rownames(fumadores) &lt;- c(&quot;ENFISEMA&quot;,&quot;NO_ENFISEMA&quot;) La tabla se ve así: fumadores ## SI_FUMA NO_FUMA ## ENFISEMA 100 40 ## NO_ENFISEMA 30 50 Luego el riesgo relativo de ENFISEMA está dado por: numerador &lt;- fumadores[&quot;ENFISEMA&quot;,&quot;SI_FUMA&quot;]/sum(fumadores$SI_FUMA) denominador &lt;- fumadores[&quot;ENFISEMA&quot;,&quot;NO_FUMA&quot;]/sum(fumadores$NO_FUMA) rr &lt;- numerador/denominador #El riesgo relativo rr ## [1] 1.73077 Por otro lado, el riesgo relativo de no enfisema es: numerador &lt;- fumadores[&quot;NO_ENFISEMA&quot;,&quot;SI_FUMA&quot;]/sum(fumadores$SI_FUMA) denominador &lt;- fumadores[&quot;NO_ENFISEMA&quot;,&quot;NO_FUMA&quot;]/sum(fumadores$NO_FUMA) rr_neg &lt;- numerador/denominador #El riesgo relativo rr_neg ## [1] 0.415385 Éste último se interpreta como si la proporción de individuos sin enfisema es \\(0.41\\) veces más pequeña entre fumadores que no fumadores. 4. Razón de momios (discreto) Para dos vectores lógicos \\(\\vec{x}\\) y \\(\\vec{y}\\) definimos la razón de momios como: \\[ \\textrm{OR}(\\vec{x}|\\vec{y}) =\\dfrac{RR(\\vec{x}|\\vec{y})}{RR(\\neg\\vec{x}|\\vec{y})} \\] Podemos calcular en R la razón de momios a partir de los datos: razon.momios &lt;- rr/rr_neg donde la razón de momios de 4.17 se interpreta como “si un individuo tiene enfisema, la factibilidad de que dicho individuo sea fumador es 4.17 veces más alta”. Esta interpretación se obtiene a partir de un despeje y sustitución: \\[\\begin{equation}\\nonumber \\begin{aligned} RR(\\vec{x}|\\vec{y}) &amp; = 4.16 \\cdot RR(\\neg\\vec{x}|\\vec{y}) \\\\ \\\\ \\Leftrightarrow \\dfrac{\\frac{\\text{Expuestos enfermos}}{\\text{Total de expuestos}}}{\\frac{\\text{No Expuestos enfermos}}{\\text{Total de no expuestos}}} &amp; = 4.16 \\cdot \\dfrac{\\frac{\\text{Expuestos no enfermos}}{\\text{Total de expuestos}}}{\\frac{\\text{No Expuestos no enfermos}}{\\text{Total de no expuestos}}} \\\\ \\\\ \\Leftrightarrow \\frac{\\text{Expuestos enfermos}}{\\text{No Expuestos enfermos}} &amp; = 4.16\\cdot \\frac{\\text{Expuestos no enfermos}}{\\text{No Expuestos no enfermos}} \\\\ \\\\ \\Leftrightarrow \\frac{\\text{Expuestos enfermos}}{\\text{Expuestos no enfermos}} &amp; = 4.16\\cdot \\frac{\\text{No Expuestos enfermos}}{\\text{No Expuestos no enfermos}} \\end{aligned} \\end{equation}\\] 5. Correlación (Bravais-Pearson) Sean \\(\\vec{x}\\) y \\(\\vec{y}\\) dos vectores columa numéricos de nuestra matriz de datos \\(Z\\). Tomemos \\(\\tilde{x} = (x_1 - \\bar{x}, x_2 - \\bar{x}, \\dots, x_n - \\bar{x})\\) la versión centrada de \\(\\vec{x}\\) y \\(\\tilde{y} = (y_1 - \\bar{y}, y_2 - \\bar{y}, \\dots, y_n - \\bar{y})\\) la versión centrada de \\(\\vec{y}\\). Al coseno entre dichos vectores (bajo el producto punto) se le conoce como correlación de Bravais-Pearson y se le denota \\(\\rho_{\\vec{x},\\vec{y}}\\). Es decir: \\[ \\rho_{\\vec{x},\\vec{y}} = \\cos(\\tilde{x},\\tilde{y}) = \\dfrac{\\tilde{x} \\cdot \\tilde{y}}{\\|\\tilde{x}\\| \\cdot \\|\\tilde{y}\\|} \\] donde \\(\\tilde{x}\\cdot\\tilde{y} = \\sum_{i=1}^{n} (x_i - \\bar{x}) \\cdot (y_i - \\bar{y})\\) representa el producto de los vectores \\(\\tilde{x}\\) y \\(\\tilde{y}\\) y se conoce como covarianza entre \\(\\vec{x}\\) y \\(\\vec{y}\\). Por otro lado, \\[ \\|\\tilde{x}\\| = \\sqrt{\\sum\\limits_{i=1}^{n} (x_i - \\bar{x})^2} = \\sigma_{\\vec{x}} \\] Por tanto la correlación también puede medirse como: \\[ \\rho_{\\vec{x},\\vec{y}} = \\cos(\\tilde{x},\\tilde{y}) = \\frac{1}{\\sigma_{\\vec{y}} \\sigma_{\\vec{x}}}\\sum\\limits_{i=1}^{n} (x_i - \\bar{x}) \\cdot (y_i - \\bar{y}) \\] Para matriz de datos \\(Z\\) con \\(\\ell\\) columnas, definimos la matriz de correlaciones \\(\\mathcal{C}\\) como la matriz dada por: \\[ \\mathcal{C} = \\begin{pmatrix} \\rho(z_1,z_1) &amp; \\rho(z_1,z_2) &amp; \\dots &amp; \\rho(z_1,z_{\\ell}) \\\\ \\rho(z_2,z_1) &amp; \\rho(z_2,z_2) &amp; \\dots &amp; \\rho(z_2,z_{\\ell}) \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\rho(z_{\\ell},z_1) &amp; \\rho(z_{\\ell},z_2) &amp; \\dots &amp; \\rho(z_{\\ell},z_{\\ell}) \\end{pmatrix} \\] Donde notamos (demuestra) que \\(\\rho(z_i, z_i) = 1\\). Podemos usar la base mtcars precargada en R para analizar las correlaciones: data(mtcars) datos.coches &lt;- mtcars La base está explicada en la ayuda de R: ?mtcars Podemos obtener la correlación entre el número de millas por galón mpg y el peso del automóvil wt haciendo: cor(datos.coches$mpg, datos.coches$wt, method = &quot;pearson&quot;) ## [1] -0.867659 Esta correlación se interpreta como que por cada aumento en el peso corresponde una disminución en las millas por galón. Podemos ver gráficamente que esto es así: ggplot(datos.coches) + geom_point(aes(x = wt, y = mpg), color = &quot;purple&quot;) + theme_bw() Para obtener toda la matriz de correlaciones de la base podemos tomar cor aplicado a toda la base de datos: cor(datos.coches, method = &quot;pearson&quot;) ## mpg cyl disp hp drat wt qsec ## mpg 1.000000 -0.852162 -0.847551 -0.776168 0.6811719 -0.867659 0.4186840 ## cyl -0.852162 1.000000 0.902033 0.832447 -0.6999381 0.782496 -0.5912421 ## disp -0.847551 0.902033 1.000000 0.790949 -0.7102139 0.887980 -0.4336979 ## hp -0.776168 0.832447 0.790949 1.000000 -0.4487591 0.658748 -0.7082234 ## drat 0.681172 -0.699938 -0.710214 -0.448759 1.0000000 -0.712441 0.0912048 ## wt -0.867659 0.782496 0.887980 0.658748 -0.7124406 1.000000 -0.1747159 ## qsec 0.418684 -0.591242 -0.433698 -0.708223 0.0912048 -0.174716 1.0000000 ## vs 0.664039 -0.810812 -0.710416 -0.723097 0.4402785 -0.554916 0.7445354 ## am 0.599832 -0.522607 -0.591227 -0.243204 0.7127111 -0.692495 -0.2298609 ## gear 0.480285 -0.492687 -0.555569 -0.125704 0.6996101 -0.583287 -0.2126822 ## carb -0.550925 0.526988 0.394977 0.749812 -0.0907898 0.427606 -0.6562492 ## vs am gear carb ## mpg 0.664039 0.5998324 0.480285 -0.5509251 ## cyl -0.810812 -0.5226070 -0.492687 0.5269883 ## disp -0.710416 -0.5912270 -0.555569 0.3949769 ## hp -0.723097 -0.2432043 -0.125704 0.7498125 ## drat 0.440278 0.7127111 0.699610 -0.0907898 ## wt -0.554916 -0.6924953 -0.583287 0.4276059 ## qsec 0.744535 -0.2298609 -0.212682 -0.6562492 ## vs 1.000000 0.1683451 0.206023 -0.5696071 ## am 0.168345 1.0000000 0.794059 0.0575344 ## gear 0.206023 0.7940588 1.000000 0.2740728 ## carb -0.569607 0.0575344 0.274073 1.0000000 Finalmente, el paquete ggcorrplot puede ayudarnos a visualizar gráficamente dicha matriz: ggcorrplot(cor(datos.coches, method = &quot;pearson&quot;), lab = TRUE, type = &quot;upper&quot;) + labs(title = &quot;Matriz de Correlaciones&quot;) Una correlación de Pearson igual a \\(1\\) ó \\(-1\\) se interpreta como que hay una relación lineal perfecta mientras que una correlación igual a \\(0\\) se interpreta como que no hay relación lineal (aunque puede existir de otro tipo) #Ejemplo de correlación lineal perfecta x &lt;- seq(0,4, length.out = 9) y &lt;- 2*x + 1 Gráficamente: El valor en este caso de la correlación es: cor(x,y, method = &quot;pearson&quot;) ## [1] 1 Mientras que por otro lado podemos tener variables relacionadas pero sin correlación : #Ejemplo sin correlación lineal pero con variables relacionadas x &lt;- seq(-4, 4,length.out = 9) y &lt;- x^2 cor(x,y, method = &quot;pearson&quot;) ## [1] 0 6. Correlación de rango de Spearman Para hablar de la correlación de rango de Spearman es necesario definir una variable como ordinal. Un vector \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) de variables numéricas o categóricas es ordinal si existe una relación \\(\\leq\\) de orden total sobre los elementos del vector tal que: Es antisimétrica: si \\(x_i \\leq x_j\\) y \\(x_j \\leq x_i\\) entonces \\(x_i = x_j\\). Es transitiva: si \\(x_i \\leq x_j\\) y \\(x_j \\leq x_k\\) entonces \\(x_i \\leq x_k\\). Es conexa: \\(x_i \\leq x_j\\) ó \\(x_j \\leq x_i\\). De manera intuitiva un vector es ordinal si hay un orden para sus entradas. Por ejemplo, cuando calificas un servicio como Malo \\(\\leq\\) Regular \\(\\leq\\) Bueno o bien cuando se compara nivel educativo (en términos de años) Primaria \\(\\leq\\) Secundaria \\(\\leq\\) Preparatoria \\(\\leq\\) Educación superior. Toda variable numérica es ordinal. Para un vector ordinal definimos su ordenamiento como \\(x_{(1)} = \\min \\{ x_1, x_2, \\dots, x_n \\}\\) y \\(x_{(j)} = \\min \\{ x_1, x_2, \\dots, x_n \\} \\setminus \\{ x_{(1)}, x_{(2)}, \\dots, x_{(j-1)} \\}\\) de tal forma que \\(x_{(1)} \\leq x_{(2)} \\leq \\dots \\leq x_{(n)}\\). El rango de \\(x_{(j)}\\) denotado como \\(R(x_{(j)})\\) es \\(j\\) (su posición en el ordenamiento). Es decir: \\[ R(x_i) = j \\Leftrightarrow x_i = x_{(j)} \\] Dado un vector \\(\\vec{x}\\) definimos su vector de rango como: \\[ R(\\vec{x}) = \\big( R(x_1), R(x_2), \\dots, R(x_n) )^T \\] Para dos variables ordinales, \\(\\vec{x}\\) y \\(\\vec{y}\\) se define la correlación de rango de Spearman como la correlación de Pearson entre sus vectores de rangos: \\[ \\rho_{\\text{Spearman}} =\\rho\\big( R(\\vec{x}), R(\\vec{y})) \\] Mientras que la correlación de Pearson mide linealidad; la de Spearman mide monotonicidad (que si una aumenta la otra también; que si una disminuye la otra también). #Comparativo de correlaciones: la de Pearson no encuentra mucha línea x &lt;- seq(0.1, 1, length.out = 25) y &lt;- exp(1/x^2) En este caso la correlación de Pearson es muy mala: cor(x,y, method = &quot;pearson&quot;) ## [1] -0.339683 Mientras que la de Spearman sí muestra la relación: cor(x,y, method = &quot;spearman&quot;) ## [1] -1 8. Ajuste de modelo lineal (versión 1) Sean \\(\\vec{x}\\) y \\(\\vec{y}\\) dos vectores columna de una matriz de datos \\(Z\\). Supongamos, además, se tiene la hipótesis de que existe una relación afín entre los vectores; es decir que: \\[ \\vec{y} \\approx \\beta_1 \\vec{x} + \\beta_0 \\vec{1} \\] donde \\(\\vec{1} = (1, 1, \\dots, 1)^T\\) es un vector con todas las entradas idénticas a \\(1\\) y \\(\\beta_0, \\beta_1 \\in \\mathbb{R}\\). Algunas razónes para tener esta hipótesis podría ser una correlación de Pearson cercana a \\(\\pm 1\\) o por inspección gráfica. Esta hipótesis implica que: \\[ y \\approx \\underbrace{\\beta_1 x + \\beta_0}_{\\hat{y}} \\] Podemos entonces trazar la línea \\(y = \\beta_0 + \\beta_1 x\\) y graficar contra los puntos \\(\\{(x_i,y_i)\\}_{i=1}^{n}\\), Si la línea no ajusta perfecto tendremos errores \\(e_i = (y_i - \\hat{y}_i)^2\\) de predicción las cuales representan la diferencia entre la \\(y\\) observada (\\(y_i\\)) y la \\(y\\) predicha por la línea \\(\\hat{y}_i = \\beta_1 x_i + \\beta_0\\). La suma de estos errores es: \\[ \\textrm{SSR}(\\beta_0, \\beta_1) = \\sum\\limits_{i=1}^{n} e_i = \\sum\\limits_{i=1}^{n} \\big( y_i - \\hat{y}_i \\big)^2 = \\sum\\limits_{i=1}^{n} \\big( y_i - (\\beta_1 x_i + \\beta_0) \\big)^2 \\] El nombre de SSR es por ( Sum of Squared Residuals ) dado que en estadística se define un residual como \\(r_i = (y_i - \\hat{y}_i)\\) Gráficamente: Lo que se busca entonces es minimizar el error respecto a las constantes a determinar: \\(\\beta_0\\) y \\(\\beta_1\\). Para ello buscamos un punto de inflexión derivando: \\[ \\dfrac{\\partial\\textrm{SSR}}{\\partial \\beta_0} = \\sum\\limits_{i=1}^{n} 2\\big(y_i - (\\beta_1 x_i + \\beta_0 ) \\big) = 0 \\] De donde se sigue que: \\[ \\sum\\limits_{i=1}^n y_i - \\beta_1 \\sum\\limits_{i=1}^{n} x_i - n \\beta_0 = 0 \\Rightarrow \\beta_0 = \\dfrac{1}{n} \\sum\\limits_{i=1}^n y_i - \\beta_1 \\dfrac{1}{n} \\sum\\limits_{i=1}^n x_i \\Rightarrow \\bar{y} - \\beta_1 \\bar{x}, \\] de donde concluimos que de cumplirse la relación lineal se tiene que: \\[ \\beta_0 = \\bar{y} - \\beta_1 \\bar{x}. \\] Por otro lado, la derivada respecto a \\(\\beta_1\\) es: \\[ \\dfrac{\\partial\\textrm{SSR}}{\\partial \\beta_1} = - \\sum\\limits_{i=1}^{n} 2\\big(y_i - (\\beta_1 x_i + \\beta_0 ) \\big) \\cdot x_i = 0 \\] De donde se sigue (si suponemos que existe al menos un \\(x_i \\neq 0\\)): \\[\\begin{equation}\\nonumber \\begin{aligned} 0 &amp; = - \\sum\\limits_{i=1}^n \\Big( x_i y_i - \\beta_1 x_i^2 - \\underbrace{\\beta_0}_{\\bar{y} - \\beta_1 \\bar{x}} x_i \\Big) \\\\ &amp; = \\sum\\limits_{i=1}^n \\Big( x_i y_i - \\beta_1 x_i^2 - \\bar{y}x_i + \\beta_1 \\bar{x} x_i \\Big) \\\\ &amp; = \\sum\\limits_{i=1}^n \\Big( y_i + \\beta_1 x_i - \\bar{y} - \\beta_1 \\bar{x} \\Big) x_i \\\\ &amp; = \\sum\\limits_{i=1}^n \\Big( y_i - \\bar{y} \\Big) x_i - \\beta_1 \\sum\\limits_{i=1}^n\\Big( x_i - \\bar{x} \\Big) x_i \\end{aligned} \\end{equation}\\] de donde se sigue (suponiendo que existen \\(i,j\\) tales que \\(x_i \\neq x_j\\) que: \\[ \\beta_1 = \\dfrac{\\sum_{i=1}^n \\Big( y_i - \\bar{y} \\Big)x_i}{\\sum_{i=1}^n\\Big( x_i - \\bar{x} \\Big)x_i} = \\dfrac{\\sum_{i=1}^n \\Big( y_i - \\bar{y} \\Big)\\Big( x_i - \\bar{x} \\Big)}{\\sum_{i=1}^n\\Big( x_i - \\bar{x} \\Big)^2} = \\dfrac{\\sigma_{\\vec{x}} \\cdot \\sigma_{\\vec{y}} \\cdot \\rho_{\\vec{x},\\vec{y}}}{n \\sigma_{\\vec{x}}^2} \\] por lo cual: \\[ \\beta_1 = \\dfrac{\\sigma_{\\vec{y}}}{\\sigma_{\\vec{x}}} \\cdot \\dfrac{\\rho_{\\vec{x},\\vec{y}}}{n} \\] De donde se tienen las fórmulas para el \\(\\beta_0\\) y \\(\\beta_1\\). 3.9.1 Ejercicio Demuestra la igualdad que usamos anteriormente: \\[ \\dfrac{\\sum_{i=1}^n \\Big( y_i - \\bar{y} \\Big) x_i}{\\sum_{i=1}^n\\Big( x_i - \\bar{x} \\Big)x_i} = \\dfrac{\\sum_{i=1}^n \\Big( y_i - \\bar{y} \\Big)\\Big( x_i - \\bar{x} \\Big)}{\\sum_{i=1}^n\\Big( x_i - \\bar{x} \\Big)^2} \\] En R podemos ajustar un modelo lineal para dos variables de una base de datos con lm: modelo.lineal &lt;- lm(mpg ~ wt, data = datos.coches) coef(modelo.lineal) ## (Intercept) wt ## 37.28513 -5.34447 Gráficamente podemos ver el modelo: ggplot(datos.coches) + geom_point(aes(y = mpg, x = wt)) + geom_smooth(aes(y = mpg, x = wt), method = &quot;lm&quot;, formula = y ~ x, se = FALSE) + theme_minimal() ## Warning: Computation failed in `stat_smooth()`: ## unable to load shared object &#39;/Users/rod/Library/R/4.0/library/mgcv/libs/mgcv.so&#39;: ## dlopen(/Users/rod/Library/R/4.0/library/mgcv/libs/mgcv.so, 6): Library not loaded: /usr/local/opt/r/lib/R/lib/libRblas.dylib ## Referenced from: /Users/rod/Library/R/4.0/library/mgcv/libs/mgcv.so ## Reason: image not found Para predecir, dada una nueva observación, cuál debe haber sido el valor de \\(\\hat{y}\\) para una nueva observación \\(x_*\\) (o varias nuevas observaciones) puede usarse la función predict datos_a_predecir &lt;- data.frame(wt = c(5.5, 6, 6.5)) predict(modelo.lineal, datos_a_predecir) ## 1 2 3 ## 7.89053 5.21830 2.54606 Hay que tener mucho cuidado con la generalización de un modelo lineal como los siguientes valores muestran: datos_a_predecir &lt;- data.frame(wt = c(7,8,9)) predict(modelo.lineal, datos_a_predecir) ## 1 2 3 ## -0.126175 -5.470646 -10.815118 O bien el siguiente comic de xkcd: Para hacer la extrapolación gráfica podemos agregar un fullrange = TRUE combinado con un xlim ggplot(datos.coches) + geom_point(aes(y = mpg, x = wt)) + geom_smooth(aes(y = mpg, x = wt), method = &quot;lm&quot;, formula = y ~ x, se = FALSE, fullrange=TRUE) + theme_minimal() + xlim(c(1,6.5)) ## Warning: Computation failed in `stat_smooth()`: ## unable to load shared object &#39;/Users/rod/Library/R/4.0/library/mgcv/libs/mgcv.so&#39;: ## dlopen(/Users/rod/Library/R/4.0/library/mgcv/libs/mgcv.so, 6): Library not loaded: /usr/local/opt/r/lib/R/lib/libRblas.dylib ## Referenced from: /Users/rod/Library/R/4.0/library/mgcv/libs/mgcv.so ## Reason: image not found 3.10 Ejercicio Generaliza el proceso de estimación para cuando se tiene un polinomio \\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2\\) Utiliza los datos confirmados de COVID-19 a nivel nacional (sólo los confirmados) disponibles en este link. Ajusta un modelo cuadrático (en el lm la fórmula ahora es del estilo de y ~ poly(x,2) ) y predice cuántos casos confirmados habrá el 29 de junio. Grafica tu ajuste así como tu predicción en la misma gráfica. 9. Ajuste general Podemos generalizar el ajuste de mínimos cuadrados planteando el modelo \\(y = f(x, \\vec{\\beta})\\) donde \\(x\\) puede ser una matriz y \\(\\vec{\\beta}\\) es un vector de parámetros. Supondremos que \\(f\\) es diferenciable en \\(\\vec{\\beta}\\). Como ejemplo, en el caso del ajuste lineal: \\[ y = f(x, \\vec{\\beta}) = \\beta_0 + \\beta_1 x \\qquad \\text{con} \\qquad \\vec{\\beta}= (\\beta_0,\\beta_1)^T. \\] o bien podríamos pensar en un ajuste polinomial: \\[ y = f(x, \\vec{\\beta}) = \\sum\\limits_{i = 0}^n \\beta_i x^i \\qquad \\text{con} \\qquad \\vec{\\beta}= (\\beta_0,\\beta_1, \\dots, \\beta_n)^T. \\] No tiene que ser un polinomio, \\(f\\) puede ser lo que ella quiera ser siempre y cuando sea diferenciable en los parámetros: \\[ y = f(x, \\vec{\\beta}) = \\Bigg[\\cos(\\beta_0 + x) + \\int\\limits_{0}^{\\beta_1 x} e^{-t^2} dx \\Bigg] \\cdot \\beta_2 \\ln(x) \\qquad \\text{con} \\qquad \\vec{\\beta}= (\\beta_0,\\beta_1, \\beta_3)^T. \\] 3.11 Ajuste funcional Hacemos una apuesta por teléfono. Yo voy a tirar una moneda \\(10\\) veces y si salen más Soles que Águilas yo gano 50 pesos. Si salen más Águilas que Soles tú ganas la misma cantidad. Al realizar el ejercicio yo te comunico que salieron en total \\(10\\) Soles y por tanto me debes el dinero. ¿Sospecharías algo de mí? Si no hablamos de probabilidad no hay forma en la que se pueda justificar que aparentemente hay algo raro con la moneda. Claro, siempre puede ser un caso improbable (hay gente que lo ha hecho) pero es raro que me hayan salido tantos Soles. Para cuantificar qué tan raro es el evento podemos suponer que las monedas siguen un modelo Binomial con parámetro \\(p = 1/2\\) y en este caso \\(n = 10\\) (fueron 10 tiros). La probabilidad de que haya obtenido \\(10\\) soles bajo este modelo es de: dbinom(10,10, 1/2) ## [1] 0.000976562 ¡Rarísimo! Este resultado te haría sospechar que quizá mi moneda no es justa y no se obtienen la misma cantidad de Águilas que Soles cuando la tiro. Esto porque, aparentemente, en mi moneda la probabilidad de Sol debería de ser \\(p = 1\\) (por tu triste experiencia). Si por ejemplo en el onceavo tiro saliera un Águila, concluirías que, en mi moneda, aparentemente, la probabilidad de Sol es \\(p = \\frac{10}{11}\\). Por supuesto, entre más tiros y más información obtienes, mejor podrás caracterizar la moneda y con mayor sustento tendrás sospechas (o no) de que mi moneda es tramposa. Formalmente, en el ejemplo anterior, lo que se hace es suponer que existe una variable aleatoria \\(X \\in\\{ \\text{Águila}, \\text{Sol}\\}\\) (el resultado de la moneda) de la cual observamos \\(n = 11\\) realizaciones codificadas en el siguiente vector: \\[ \\vec{x} = \\big( \\text{Sol}, \\text{Sol},\\text{Sol},\\text{Sol},\\text{Sol},\\text{Sol},\\text{Sol},\\text{Sol},\\text{Sol},\\text{Sol},\\text{Águila}\\big)^T \\] Aproximamos entonces la probabilidad \\(\\mathbb{P}(X = \\text{Sol})\\) mediante: \\[ \\mathbb{P}(X = \\text{Sol}) \\approx \\dfrac{1}{n}\\sum\\limits_{i=1}^n \\mathbb{I}_{\\{\\text{Sol}\\}}(x_i) = \\dfrac{10}{11} \\] Mientras que la de Águila se aproxima mediante: \\[ \\mathbb{P}(X = \\text{Águila}) \\approx \\dfrac{1}{n}\\sum\\limits_{i=1}^n \\mathbb{I}_{\\{\\text{Águila}\\}}(x_i) = \\dfrac{1}{11} \\] Para ver que éstas son buenas aproximaciones, podemos considerar un vector aleatorio de los posibles datos observados: \\[ \\vec{X} = (X_1, X_2, \\dots, X_{11})^T \\] Donde \\(X_1\\) es una variable aleatoria que representa lo que pudo haber salido en el primer tiro, \\(X_2\\) es una v.a. que representa lo que pudo haber salido en el segundo tiro y en general \\(X_k\\) es una v.a. que representa lo que pudo haber salido en el \\(k\\)-ésimo tiro. Suponiendo que la moneda tiene una probabilidad \\(p\\) de arrojar Sol y \\(1-p\\) de arrojar Águila, notamos que las variables indicadoras evaluadas en las \\(X_i\\) (aleatorias) son variables aleatorias \\[ \\mathbb{I}_{\\{\\text{Sol}\\}}(X_i) \\sim \\text{Beroulli}(p) \\] y que por tanto \\[ \\hat{p} = \\dfrac{1}{n}\\sum\\limits_{i=1}^n \\mathbb{I}_{\\{\\text{Sol}\\}}(X_i) \\] es una variable aleatoria (al ser suma de variables aleatorias). Podemos entonces calcular su valor esperado: \\[ \\mathbb{E}\\big[\\hat{p}\\big] = \\mathbb{E}\\bigg[\\dfrac{1}{n}\\sum\\limits_{i=1}^n \\mathbb{I}_{\\{\\text{Sol}\\}}(X_i)\\bigg] = \\dfrac{1}{n}\\sum\\limits_{i=1}^n \\mathbb{E}\\big[ \\mathbb{I}_{\\{\\text{Sol}\\}}(X_i)\\big] = \\dfrac{1}{n}\\sum\\limits_{i=1}^n p = \\dfrac{1}{n}\\cdot np = p \\] Es decir, que en promedio el estimador \\(\\hat{p}\\) va a atinarle al verdadero valor \\(p\\). Esto lo podemos ver si hacemos nsim\\(= 1000\\) simulaciones de \\(100\\) tiros de una moneda con probabilidad p\\(= 8/10\\) de sol. nsim &lt;- 1000 tiros &lt;- 100 p.val &lt;- 8/10 #Creamos un vector para guardar los valores de p gorro p.gorro &lt;- rep(NA, nsim) #Loop recorriendo cada una de las nsim simulaciones for (i in 1:nsim){ experimento &lt;- sample(c(&quot;Sol&quot;,&quot;Águila&quot;), tiros, replace = TRUE, prob = c(p.val, 1 - p.val)) soles &lt;- table(experimento)[&quot;Sol&quot;] p.gorro[i] &lt;- soles/tiros } Podemos ver que en promedio le atinamos al valor verdadero: #Vemos que en promedio le atina: mean(p.gorro) ## [1] 0.80131 Lo mismo podemos verlo gráficamente: #Graficamos ggplot() + geom_point(aes(x = 1:nsim, y = p.gorro, color = as.character(p.gorro)), size = 2, alpha = 0.2) + geom_hline(aes(yintercept = p.val), size = 1.5, linetype = &quot;solid&quot;) + theme_classic() + theme(legend.position = &quot;none&quot;) + labs( x = &quot;Simulaciones&quot;, y = &quot;Estimación de p&quot;, title = &quot;Simulación de proceso de estimación\\nde que una moneda caiga Sol&quot; ) + geom_label(aes(x = nsim/2, y = p.val), label = &quot;Verdadero valor de p&quot;) ¿Qué significa esto? El que en promedio \\(\\hat{p}\\) sea \\(p\\) (formalmente, que \\(\\mathbb{E}\\big[\\hat{p}\\big] = p\\)) significa que, si yo hago muchísimos experimentos (o procesos de muestreo) de la misma cosa, mi \\(\\hat{p}\\) es un buen estimador porque en promedio le va a atinar. Empero, esto no dice nada de qué tan bueno es mi estimador \\(\\hat{p}\\) para mi caso (mi muestra o mi experimento) específico. Puedes pensarlo con los exámenes: que alguien tenga un promedio de 8 dice que en general le ha ido bien en los exámenes, pero no dice nada respecto al primer examen de cálculo que hizo (donde pudo tener \\(10\\) ó \\(5\\) para llegar a ese promedio de \\(8\\) pero no podemos saber de manera específica cuánto fue ). Esto es igual: en promedio el estimador \\(\\hat{p}\\) será \\(p\\) pero para un análisis específico no sabemos. OJO Los datos observados no son variables aleatorias: esos ya son fijos, ya los viste. Los posibles datos observados sí son variables aleatorias ya que ellos, consisten en las variables que se pudieron haber observado y te permiten calcular las probabilidades de tus datos observados bajo algún modelo. En el caso de la moneda, los datos observados son \\(\\vec{x} = \\big( \\text{Sol}, \\text{Sol},\\text{Sol},\\text{Sol},\\text{Sol},\\text{Sol},\\text{Sol},\\text{Sol},\\text{Sol},\\text{Sol},\\text{Águila}\\big)^T\\) pero los que se pudieron haber observado son todas las \\(\\binom{n}{2}\\) formas en las que la moneda pudo haber salido. 1. Estimación de una función de masa de probabilidad Formalmente, para una variable aleatoria discreta \\(X\\) que puede tomar los valores \\(\\{ a_1, a_2, \\dots, a_{\\ell} \\}\\) de la cual se observaron \\(n\\) realizaciones descritas mediante \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) (observados, fijos, constantes). Definimos la función de masa de probabilidad empírica como: \\[ \\hat{p}(x) = \\begin{cases} \\\\ \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{I}_{\\{a_1\\}}(x_i) &amp; \\text{ si } x = a_1 \\\\ \\\\ \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{I}_{\\{a_2\\}}(x_i) &amp; \\text{ si } x = a_2 \\\\ \\\\ \\vdots \\\\ \\\\ \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{I}_{\\{a_{\\ell}\\}}(x_i) &amp; \\text{ si } x = a_{\\ell} \\\\ 0 &amp; \\text{ en otro caso} \\end{cases} \\] donde se supone que \\(\\mathbb{P}(X = x) \\approx \\hat{p}(x)\\). Notamos que lo anterior puede resumirse en: \\[ \\hat{p}(x) = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{I}_{\\{ x\\}}(x_i) \\] Análogamente, nota que para un conjunto (medible) \\(A\\), la aproximación para \\(\\mathbb{P}(X \\in A)\\) está dada por: \\[ \\hat{p}(A) = \\dfrac{1}{n}\\sum\\limits_{i=1}^{n} \\mathbb{I}_A (x_i). \\] Podemos graficar para la base de datos conteo_delitos la probabilidad de que, dado que se cometió un delito, éste haya ocurrido en el dia \\(d_i\\) de diciembre. Para ello usamos un geom_col: ggplot(conteo_delitos) + geom_col(aes(x = fecha, y = n/sum(n), fill = n)) + scale_fill_gradient(&quot;Delito&quot;, low = &quot;orange&quot;, high = &quot;red&quot;) + theme_bw() + theme(legend.position = &quot;none&quot;) + labs( y = &quot;p(x)&quot;, x = &quot;x&quot;, title = &quot;Aproximación a p(x)&quot; ) Una propiedad interesante de la función de masa de probabilidad es que, en promedio, le atina al verdadero valor (lo que comentábamos antes de que \\(\\hat{p} = p\\)). Es decir, suponiendo que \\(X\\) tiene una función de masa dada por: \\[ p(x) = \\begin{cases} p_1 &amp; \\text{ si } x = a_1 \\\\ p_2 &amp; \\text{ si } x = a_2 \\\\ \\vdots \\\\ p_{\\ell} &amp; \\text{ si } x = a_{\\ell} \\\\ \\end{cases} \\] y suponiendo un vector de muestras posibles \\(\\vec{X} = (X_1, X_2, \\dots, X_n)^T\\) notamos que \\[ \\mathbb{I}_{\\{ a_j \\}}(X_i) \\sim \\text{Bernoulli} (p_j) \\] Luego para cualquier \\(x\\) se tiene que: \\[ \\mathbb{E}\\big[ \\hat{p}(x)\\big] = \\mathbb{E}\\bigg[ \\dfrac{1}{n}\\sum\\limits_{i=1}^{n} \\mathbb{I}_{\\{ x \\}}(X_i) \\bigg] = \\dfrac{1}{n}\\sum\\limits_{i=1}^{n}\\mathbb{E}\\big[ \\mathbb{I}_{\\{ x \\}}(X_i) \\big] = \\frac{1}{n} n \\cdot p_j = p_j. \\] 2. Función de distribución empírica Recuerda que para cualquier variable aleatoria \\(X:\\mathbb{R}\\to\\mathbb{R}\\) existe su función de distribución \\(F_X\\) dada por: \\[ F_X(x) = \\mathbb{P}(X \\leq x) \\] La idea de la función de distribución empírica es reconstruir (a partir de los datos observados) a \\(F_X\\). Para ello, notamos que queremos estimar \\[ \\mathbb{P}(X \\leq x) \\qquad \\forall x\\in\\mathbb{R} \\] esto es equivalente a estimar: \\[ \\mathbb{P}\\big(X \\in (-\\infty, x] \\big) \\] y podemos aplicar la aproximación que usamos arriba para un conjunto \\(A\\): \\[ \\mathbb{P}\\big(X \\in (-\\infty, x] \\big) \\approx \\sum\\limits_{i=1}^n \\mathbb{I}_{(-\\infty, x]}(x_i) \\] La función de distribución empírica está definida para un vector numérico \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) por: \\[ \\hat{F} (x) = \\frac{1}{n} \\sum\\limits_{i=1}^{n} \\mathbb{I}_{(-\\infty, x]}(x_i) \\] La función de distribución empírica es una función de distribución pues cumple las siguientes propiedades (demuéstralo): \\(\\lim_{x \\to -\\infty} \\hat{F}(x) = 0\\) \\(\\lim_{x \\to \\infty} \\hat{F}(x) = 1\\) Si \\(x &lt; y\\) entonces \\(\\hat{F}(x) \\leq \\hat{F}(y)\\) (no decreciente) \\(\\hat{F}\\) es continua por la derecha con límites por la izquierda (càdlàg). Para demostrar 4. basta con demostrar que para \\(x_i\\) fija, la función \\(i(x) = \\mathbb{I}_{(-\\infty, x]}(x_i)\\) es continua por la derecha con límites por la izquierda pues \\(\\hat{F}(x)\\) es una suma de dichas funciones. En particular, podemos notar que la función de distribución empírica \\(\\hat{F} (x)\\) le atina a la función de distribución; es decir: \\[ \\mathbb{E}\\big[\\hat{F} (x) \\big] = F(x) \\] Para ello consideramos un vector de valores posibles \\(\\vec{X} = (X_1, X_2, \\dots, X_n)^T\\) donde las \\(X_i\\) tienen la misma distribución que \\(X\\). Y notamos que: \\[ \\mathbb{I}_{(-\\infty, x]}(X_i) \\sim \\textrm{Bernoulli}\\big(F(x)\\big) \\] pues \\(\\mathbb{I}_{(-\\infty, x]}(X_i) = 1\\) si \\(X_i \\leq x\\) y \\(\\mathbb{I}_{(-\\infty, x]}(X_i) = 0\\) si \\(X_i &gt; x\\). Luego: \\[ \\mathbb{P}\\Big( \\mathbb{I}_{(-\\infty, x]}(X_i) = 1 \\Big) = \\mathbb{P}(X_i \\leq x) = \\mathbb{P}(X\\leq x) = F(x) \\] donde la igualdad del medio se sigue de que \\(X_i\\) y \\(X\\) tienen la misma distribución. Entonces: \\[ \\mathbb{E}\\big[ \\hat{F}(x) \\big] = \\mathbb{E}\\Big[ \\dfrac{1}{n} \\sum\\limits_{i=1}^{n} \\mathbb{I}_{(-\\infty, x]}(X_i) \\Big] = \\dfrac{1}{n}\\sum\\limits_{i=1}^{n} \\mathbb{E}\\big[ \\mathbb{I}_{(-\\infty, x]}(X_i)\\big] = \\dfrac{1}{n} n \\cdot F(x) = F(x) \\] En R podemos calcular la función de distribución empírica con el comando ecdf el cual cuenta la cantidad de observaciones y regresa una función. Así, para la base de datos conteo_delitos podemos calcular la función de distribución empírica ecdf asociada a la cantidad de delitos que se cometen en un día mediante: Fgorro &lt;- ecdf(conteo_delitos$n) De esta forma podemos calcular la probabilidad de que en un día se cometan menos de 500 delitos: Fgorro(500) ## [1] 0.0967742 O bien podemos graficar la función: x &lt;- seq(300, 1000, length.out = 100) y &lt;- Fgorro(x) ggplot() + geom_step(aes(x = x, y = y), color = &quot;red&quot;) + labs( x = &quot;Número de carpetas de investigación (x)&quot;, y = &quot;Probabilidad de que en un día\\nse abran menos de x carpetas&quot;, title = &quot;Distribución acumulada de carpetas de investigación en CDMX&quot; ) + theme_minimal() Mediante simulaciones, podemos observar que \\(\\hat{F}\\) realmente le atina a \\(F\\) como sigue: #Cantidad de simulaciones nsim &lt;- 100 #Tamaño de la muestra en cada simulacion n_muestra &lt;- 100 #Valores a evaluar la función x &lt;- seq(-5, 5, length.out = 200) #Base de datos para guardar resultados de simulaciones F_simulado &lt;- data.frame(matrix(NA, ncol = nsim, nrow = length(x))) for (i in 1:nsim){ valores_simulados &lt;- rnorm(n_muestra) F_empirica &lt;- ecdf(valores_simulados) F_simulado[,i] &lt;- F_empirica(x) } F_simulado$Valor_x &lt;- x #Cambiamos el formato de la base para graficar F_simulado &lt;- F_simulado %&gt;% pivot_longer(cols = -Valor_x) ggplot(F_simulado) + geom_step(aes(x = Valor_x, y = value, color = name), alpha = 0.1) + geom_line(aes(x = Valor_x, y = pnorm(Valor_x)), color = &quot;black&quot;) + theme_classic() + theme(legend.position = &quot;none&quot;) + labs( x = &quot;x&quot;, y = &quot;F(x)&quot;, title = &quot;Simulaciones de funciones de distribuciones acumuladas empíricas&quot;, subtitle = &quot;Para X ~ Normal(0,1)&quot; ) 2. Histograma Para una variable aleatoria continua, la aproximación \\(\\hat{p}\\) que hicimos no funciona (la masa siempre es \\(0\\)). Por lo que es necesario analizar alternativas para estudiar la densidad si suponemos que los datos pueden modelarse mediante algo continuo. Para construir un histograma consideremos \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) y una constante \\(h &gt; 0\\) llamada el ancho de banda (binwidth). Sea \\(\\{ I_j \\}\\) una colección de intervalos no vacíos de \\(\\mathbb{R}\\) tal que \\(\\cup_{j=1} I_j = \\mathbb{R}\\) e \\(I_j \\cap I_k = \\emptyset\\) (i.e. los \\(\\{ I_j \\}\\) forman una partición de \\(\\mathbb{R}\\)). Supongamos, además, los \\(I_j\\) son de la forma: \\[ I_j = \\Big[\\kappa + (j-1) h, \\kappa + jh \\Big) \\] para algún \\(\\kappa \\in \\mathbb{R}\\) fijo. Sea \\[ n_j(\\vec{x}) = \\sum\\limits_{i=1}^n \\mathbb{I}_{I_j}(x_i) \\] la cantidad de \\(x_i\\) en el intervalo \\(I_j\\). Un histograma es la gráfica de la función (ver Panaretos (2016)): \\[ \\text{hist}_{\\vec{x}}(x) = \\frac{1}{n \\cdot h} \\sum\\limits_{j} n_j(\\vec{x}) \\cdot \\mathbb{I}_{I_j}(x) \\] Una propiedad interesante de un histograma es que éste aproxima correctamente las probabilidades \\(\\mathbb{P}(X \\in I_j)\\). Para ver esto, consideramos un vector de valores posibles \\(\\vec{X} = (X_1, X_2, \\dots, X_n)^T\\) y que \\(x\\in I_j\\), luego: \\[ \\mathbb{E}\\Big[ \\int_{I_j} \\text{hist}_{\\vec{X}}(x) dx \\Big] = \\mathbb{E}\\bigg[ \\frac{1}{n \\cdot h} \\sum\\limits_{j} n_j(\\vec{X}) \\cdot \\int_{I_j} \\mathbb{I}_{I_j}(x) dx \\bigg] = \\frac{1}{n \\cdot h} \\sum\\limits_{j} \\mathbb{E}\\big[ n_j(\\vec{X}) \\big] \\cdot h = \\dfrac{1}{n}\\sum\\limits_{j} \\mathbb{E}\\big[ n_j(\\vec{X}) \\big] \\] donde las \\(n_j(\\vec{X})\\) son variables aleatorias en este caso y: \\[ \\mathbb{E}\\big[ n_j(\\vec{X})\\big] = \\sum\\limits_{i=1}^n \\mathbb{E}\\big[\\mathbb{I}_{I_j}(X_i)\\big] = \\sum\\limits_{i=1}^n\\mathbb{P}(X_i \\in I_j) = n \\mathbb{P}(X \\in I_j) \\] donde la última igualdad se da pues las \\(X_i\\) tienen la misma distribución que \\(X\\). Luego: \\[ \\mathbb{E}\\Big[ \\int_{I_j} \\text{hist}_{\\vec{X}}(x) dx \\Big] = \\mathbb{P}(X \\in I_j) \\] Es decir, el valor esperado del área bajo un histograma en un intervalo \\(I_j\\) coincide con la probabilidad de que \\(X\\) pertenezca a dicho intervalo. Gráficamente: En R podemos hacer un histograma a través de geom_histogram. En este caso lo haremos de la latitud: #En este caso binwidth = h y kappa = boundary ggplot(datos) + geom_histogram(aes(x = latitud, y = ..density..), binwidth = 0.02, boundary = -99, color = &quot;white&quot;, fill = &quot;purple&quot;) + theme_light() 3.11.1 Ejercicio Considera la siguiente base de datos (obtenida de Cross Validated): mis.datos &lt;- data.frame( A = c(3.15, 5.46, 3.28, 4.20, 1.98, 2.28, 3.12, 4.10, 3.42, 3.91, 2.06, 5.53, 5.19, 2.39, 1.88, 3.43, 5.51, 2.54, 3.64, 4.33, 4.85, 5.56, 1.89, 4.84, 5.74, 3.22, 5.52, 1.84, 4.31, 2.01, 4.01, 5.31, 2.56, 5.11, 2.58, 4.43, 4.96, 1.90, 5.60, 1.92), B = c(2.90, 5.21, 3.03, 3.95, 1.73, 2.03, 2.87, 3.85, 3.17, 3.66, 1.81, 5.28, 4.94, 2.14, 1.63, 3.18, 5.26, 2.29, 3.39, 4.08, 4.60, 5.31, 1.64, 4.59, 5.49, 2.97, 5.27, 1.59, 4.06, 1.76, 3.76, 5.06, 2.31, 4.86, 2.33, 4.18, 4.71, 1.65, 5.35, 1.67), C = c(2.65, 4.96, 2.78, 3.70, 1.48, 1.78, 2.62, 3.60, 2.92, 3.41, 1.56, 5.03, 4.69, 1.89, 1.38, 2.93, 5.01, 2.04, 3.14, 3.83, 4.35, 5.06, 1.39, 4.34, 5.24, 2.72, 5.02, 1.34, 3.81, 1.51, 3.51, 4.81, 2.06, 4.61, 2.08, 3.93, 4.46, 1.4, 5.1, 1.42), D = c(2.40, 4.71, 2.53, 3.45, 1.23, 1.53, 2.37, 3.35, 2.67, 3.16, 1.31, 4.78, 4.44, 1.64, 1.13, 2.68, 4.76, 1.79, 2.89, 3.58, 4.10, 4.81, 1.14, 4.09, 4.99, 2.47, 4.77, 1.09, 3.56, 1.26, 3.26, 4.56, 1.81, 4.36, 1.83, 3.68, 4.21, 1.15, 4.85, 1.17) ) Grafica un histograma de las variables A, B, C y D de dicha base con un ancho de banda (binwidth) igual a 1. ¿Podemos concluir la forma de la distribución a partir del histograma? Es decir ¿hay distribuciones sesgadas a la izquierda, a la derecha, uniformes, centradas o con colas pesadas? Realiza el mismo histograma pero ahora con un ancho de banda de 0.25 ¿por qué hubo cambios? Analiza la base de datos (los valores en función de la columna A) y concluye. 3. Densidad kernel Un histograma tiene muchos bemoles: en particular, es necesario decidir quién es \\(h\\) y quién \\(\\kappa\\) y no hay una regla clara de cómo hacerlo. La densidad kernel es un intento de mejorar esta situación. Para ello recordamos que si \\(X\\) es una variable aleatoria continua con densidad \\(F\\) entonces: \\[ f(x) = F&#39;(x) = \\lim_{h \\to 0} \\dfrac{F(x + h) - F(x - h)}{2h} \\] Por lo que para un \\(h\\) positiva con \\(h \\approx 0\\) tenemos que: \\[ f(x) \\approx \\dfrac{F(x + h) - F(x - h)}{2h} \\] En el caso de un vector de observaciones \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) recordamos que podemos asociar una función de distribución empírica \\(\\hat{F}\\) y por tanto obtener el estimador de Rosenblatt de la densidad \\(f\\) mediante: \\[ \\hat{f}(x) = \\dfrac{\\hat{F}(x + h) -\\hat{F}(x - h)}{2h} \\] Podemos reescribir esto como: \\[ \\hat{f}(x) = \\dfrac{1}{2nh} \\sum\\limits_{i = 1}^n \\mathbb{I}_{(x-h, x+h]}(x_i) = \\dfrac{1}{nh}\\sum\\limits_{i = 1}^n K\\Big( \\frac{x_i - x}{h}\\Big) \\] donde: \\[ K(u) = \\frac{1}{2} \\mathbb{I}_{(-1, 1]}(u) \\] se conoce como el kernel rectangular. Una vez que llegamos hasta este punto notamos que para cualquier \\(K\\) que cumple: \\(\\int\\limits_{-\\infty}^{\\infty} K(u) du = 1\\) \\(K(u) \\geq 0\\) la función \\(\\hat{f}\\) es una función de densidad. La función \\(\\hat{f}\\) se conoce como el estimador de densidad del kernel K. Algunos ejemplos de kernels \\(K\\) son: Rectangular: \\(K(u) = \\frac{1}{2} \\mathbb{I}_{(-1, 1]}(u)\\) Triangular: \\(K(u) = (1 - |u|) \\mathbb{I}_{(-1, 1]}(u)\\) Epanechnikov: \\(K(u) = \\frac{3}{4}(1 - u^2) \\mathbb{I}_{(-1, 1]}(u)\\) Gaussiano: \\(K(u) = \\frac{1}{\\sqrt{2\\pi}}\\text{exp}(-u^2/2)\\) OJO No confundir el Kernel \\(K\\) (que es una función que integra a \\(1\\)) con función de densidad kernel que es una función de los datos que utiliza un kernel y es una densidad por sí misma. En R podemos calcular la densidad kernel en n puntos con relativa facilidad mediante density: densidad_kernel &lt;- density(datos$latitud, kernel = &quot;gaussian&quot;, n = 700, na.rm = TRUE) Nota que R en automático preselecciona los valores de h mediante un criterio preprogramado de optimización. Podemos ver dicha densidad gráficamente (y compararla con un histograma): ggplot(datos) + geom_histogram(aes(x = latitud, y = ..density..), binwidth = 0.01, boundary = 19, fill = &quot;purple&quot;, color = &quot;white&quot;) + geom_density(aes(x = latitud), kernel = &quot;gaussian&quot;, size = 1.5) + theme_bw() Esto no se queda ahí, podemos generalizar el concepto de kernel a dos dimensiones para aproximar una función de densidad \\(f(x,y)\\) de dos variables aleatorias sí tenemos dos vectores \\(\\vec{x}\\) y \\(\\vec{y}\\) y calculamos: \\[ \\hat{f}(x,y) =\\dfrac{1}{nh^2} \\sum\\limits_{i = 1}^n K \\Big( \\frac{x_i - x}{h}\\Big) K \\Big( \\frac{y_i - y}{h}\\Big) \\] En particular esto nos permite generar una densidad en R para saber en qué coordenadas de latitud y longitud ocurren más los delitos: ggplot(datos) + geom_point(aes(x = longitud, y = latitud), alpha = 0.025) + geom_density_2d_filled(aes(x = longitud, y = latitud), alpha = 0.75) + geom_density2d(aes(x = longitud, y = latitud)) + theme_void() 3.11.2 Ejercicio sugerido Este ejercicio es para que tengas la seguridad de que comprendiste los conceptos previos y sabes calcularlos. Es tedioso pero bueno para aclarar dudas. Considera la siguiente base de datos: x y z w 1 -100 Rojo Bueno 2 -2 Azul Malo 3 2 Azul Regular 2 3 Rojo Bueno 1 1 Verde Bueno 3 4 Amarillo Malo Calcula a mano (es decir puedes usar calculadora pero no lo calcules en R) y luego verifica tus cálculos haciéndolo en R: El total de \\(\\vec{x}\\) La media y varianza de \\(\\vec{y}\\) La curtosis y la asimetría de \\(\\vec{x}\\) (su media es 2 y su varianza 0.8). Determina si tiene un sesgo a la derecha, a la izquierda o ninguno. Determina mediante la curtosis si \\(\\vec{x}\\) tiene colas más pesadas que \\(\\vec{y}\\). Calcula el cuantil \\(0.25\\) y el \\(0.75\\) de \\(\\vec{y}\\) así como su rango intercuartílico (IQR). ¿Hay valores atípicos (outliers) en \\(\\vec{y}\\)? En caso afirmativo, determina cuáles son. ¿Cuál es el rango de \\(\\vec{y}\\)? (no confundir con el IQR). Determina la moda de \\(\\vec{z}\\) Determina la mediana de \\(\\vec{x}\\). Determina la MAD de \\(\\vec{x}\\) Realiza el conteo de cuáles \\(\\vec{z}\\) pertenecen al conjunto \\(A = \\{ \\text{Rojo}, \\text{Amarillo} \\}\\) Realiza una tabla de contingencia de \\(\\vec{w}\\) y \\(\\vec{z}\\). Determina la distribución frecuencial (observada) marginal de \\(\\vec{w}\\). Realiza una tabla de frecuencias de \\(\\vec{w}\\) y \\(\\vec{z}\\). Calcula el riesgo relativo de estar en un choque dado que manejas en CDMX a partir de los datos en la tabla: CDMX MTY Total Chocó 1100 4000 5100 No Chocó 120 5080 5200 Total 1220 9080 10300 Interpreta tu resultado. De la tabla anterior calcula la razón de momios asociada a chocar dado que manejas en CDMX. Interprétala. Calcula la correlación de Bravais Pearson de \\(\\vec{x}\\) y \\(\\vec{y}\\). Interpreta. Obtén la correlación de Spearman de \\(\\vec{x}\\) y \\(\\vec{y}\\) Para \\(\\vec{w}\\) y \\(\\vec{x}\\) obtén la \\(\\tau\\) de Kendall (son 15 comparaciones para generarla) Descartando el outlier de \\(\\vec{y}\\) (y su \\(\\vec{x}\\) asociada), ajusta un modelo lineal \\(\\hat{y} = \\hat{\\beta}_1 x + \\hat{\\beta}_0\\) y grafícalo para ver qué tan buen modelo es. Realiza una gráfica de caja (boxplot) para \\(\\vec{y}\\) Realiza un scatterplot para la submatriz \\(Z_{(x,y)}\\). Realiza una gráfica de líneas para \\(Z_{(x,y)}\\) identificando la función de interpolación lineal \\(f(x)\\) asociada. Realiza una gráfica de barras de \\(\\vec{w}\\) especificando quiénes son los \\(a_i\\) y los \\(n_j\\). Estima mediante \\(\\hat{p}\\) la función de probabilidad de \\(\\vec{w}\\). Identifica la función de distribución empírica para \\(\\vec{x}\\), \\(\\hat{F}\\) y grafícala. Realiza un histograma con \\(h = 2\\) para \\(x\\). Toma \\(\\kappa = 4\\). Ajusta una densidad kernel a \\(\\vec{x}\\) con \\(h = 1\\) y usando un kernel \\(K\\) triangular. Calcula \\(\\hat{f}(x)\\) para \\(x = 0,1,2,3,4\\). Referencias "],["referencias.html", "Referencias", " Referencias "]]
